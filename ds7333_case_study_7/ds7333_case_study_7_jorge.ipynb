{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import re\n",
    "import os\n",
    "from IPython.display import Image\n",
    "from abc import ABC, abstractmethod\n",
    "import time\n",
    "#import sklearn\n",
    "#import time\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tabulate import tabulate\n",
    "from IPython.display import clear_output\n",
    "import xgboost\n",
    "\n",
    "# data pre-processing\n",
    "from scipy.io import arff\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.impute._base import _BaseImputer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection._split import BaseShuffleSplit\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# prediction models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm._base import BaseSVC \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "\n",
    "# import warnings filter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from warnings import simplefilter \n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilePathManager:\n",
    "    def __init__(self, local_dir: str):\n",
    "        self.local_dir = local_dir\n",
    "    \n",
    "    def retrieve_full_path(self):\n",
    "        return os.getcwd()+'/'+self.local_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    def load_data(self, file_name):\n",
    "        pass\n",
    "    \n",
    "    def get_df(self):\n",
    "        pass\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    " \n",
    "class CSVLoader(Loader):\n",
    "    def __init__(self, file_path_manager: FilePathManager):\n",
    "        self.file_path_manager = file_path_manager\n",
    "        \n",
    "    def load_data(self, _prepare_data: Callable[[pd.DataFrame], pd.DataFrame] = None):\n",
    "        self.df = pd.read_csv(self.file_path_manager.retrieve_full_path())\n",
    "        if _prepare_data:\n",
    "            self.df = _prepare_data(self.df)\n",
    "    \n",
    "    def get_df(self):\n",
    "        return self.df;\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df['y'] = df['y'].astype(int)\n",
    "    df['x32'] = df['x32'].str.replace('%','').astype(float)\n",
    "    df['x37'] = df['x37'].str.replace('$','').astype(float)\n",
    "#     cont_vars = df.describe().columns\n",
    "#     cat_vars = set(df.columns) - set(cont_vars)\n",
    "#     for column in [*cat_vars]:\n",
    "#         df[column] = labelencoder.fit_transform(df[column].astype(str))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x32</th>\n",
       "      <th>x29</th>\n",
       "      <th>x30</th>\n",
       "      <th>x37</th>\n",
       "      <th>x24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0%</td>\n",
       "      <td>July</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>$1313.96</td>\n",
       "      <td>euorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.02%</td>\n",
       "      <td>Aug</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>$1962.78</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.01%</td>\n",
       "      <td>July</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>$430.47</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01%</td>\n",
       "      <td>July</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>$-2366.29</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01%</td>\n",
       "      <td>July</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>$-620.66</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159995</th>\n",
       "      <td>0.0%</td>\n",
       "      <td>Aug</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>$-891.96</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159996</th>\n",
       "      <td>-0.01%</td>\n",
       "      <td>May</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>$1588.65</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159997</th>\n",
       "      <td>-0.0%</td>\n",
       "      <td>Jun</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>$687.46</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159998</th>\n",
       "      <td>-0.02%</td>\n",
       "      <td>May</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>$439.21</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159999</th>\n",
       "      <td>0.02%</td>\n",
       "      <td>Aug</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>$-1229.34</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x32   x29        x30        x37     x24\n",
       "0         0.0%  July    tuesday   $1313.96  euorpe\n",
       "1       -0.02%   Aug  wednesday   $1962.78    asia\n",
       "2       -0.01%  July  wednesday    $430.47    asia\n",
       "3        0.01%  July  wednesday  $-2366.29    asia\n",
       "4        0.01%  July    tuesday   $-620.66    asia\n",
       "...        ...   ...        ...        ...     ...\n",
       "159995    0.0%   Aug  wednesday   $-891.96    asia\n",
       "159996  -0.01%   May  wednesday   $1588.65    asia\n",
       "159997   -0.0%   Jun  wednesday    $687.46    asia\n",
       "159998  -0.02%   May  wednesday    $439.21    asia\n",
       "159999   0.02%   Aug    tuesday  $-1229.34    asia\n",
       "\n",
       "[160000 rows x 5 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cat_vars=[*cat_vars]\n",
    "df[my_cat_vars].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2\n",
       "1         1\n",
       "2         1\n",
       "3         1\n",
       "4         1\n",
       "         ..\n",
       "159995    1\n",
       "159996    1\n",
       "159997    1\n",
       "159998    1\n",
       "159999    1\n",
       "Name: x24, Length: 160000, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['x24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x32</th>\n",
       "      <th>x29</th>\n",
       "      <th>x30</th>\n",
       "      <th>x37</th>\n",
       "      <th>x24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0%</td>\n",
       "      <td>July</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>$1313.96</td>\n",
       "      <td>euorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.02%</td>\n",
       "      <td>Aug</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>$1962.78</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.01%</td>\n",
       "      <td>July</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>$430.47</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01%</td>\n",
       "      <td>July</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>$-2366.29</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01%</td>\n",
       "      <td>July</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>$-620.66</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159995</th>\n",
       "      <td>0.0%</td>\n",
       "      <td>Aug</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>$-891.96</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159996</th>\n",
       "      <td>-0.01%</td>\n",
       "      <td>May</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>$1588.65</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159997</th>\n",
       "      <td>-0.0%</td>\n",
       "      <td>Jun</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>$687.46</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159998</th>\n",
       "      <td>-0.02%</td>\n",
       "      <td>May</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>$439.21</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159999</th>\n",
       "      <td>0.02%</td>\n",
       "      <td>Aug</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>$-1229.34</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x32   x29        x30        x37     x24\n",
       "0         0.0%  July    tuesday   $1313.96  euorpe\n",
       "1       -0.02%   Aug  wednesday   $1962.78    asia\n",
       "2       -0.01%  July  wednesday    $430.47    asia\n",
       "3        0.01%  July  wednesday  $-2366.29    asia\n",
       "4        0.01%  July    tuesday   $-620.66    asia\n",
       "...        ...   ...        ...        ...     ...\n",
       "159995    0.0%   Aug  wednesday   $-891.96    asia\n",
       "159996  -0.01%   May  wednesday   $1588.65    asia\n",
       "159997   -0.0%   Jun  wednesday    $687.46    asia\n",
       "159998  -0.02%   May  wednesday    $439.21    asia\n",
       "159999   0.02%   Aug    tuesday  $-1229.34    asia\n",
       "\n",
       "[160000 rows x 5 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['x32', 'x29', 'x30', 'x37', 'x24']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(FilePathManager('final_project(5).csv'))\n",
    "loader.load_data(clean_data)\n",
    "df = loader.get_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseImputer:\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X):\n",
    "        pass\n",
    "\n",
    "class BaseModel:\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modeling:\n",
    "    _X_train_fitted = None\n",
    "    _X_test_fitted = None\n",
    "    _y_train = None\n",
    "    _y_test = None\n",
    "    _y_preds = None\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, \n",
    "                 target_name: str, \n",
    "                 shuffle_splitter: BaseShuffleSplit, \n",
    "                 imputer: BaseImputer, \n",
    "                 model: BaseModel, scaler = None, encoder = None):\n",
    "        self._data = data\n",
    "        self._target_name = target_name\n",
    "        self._shuffle_splitter = shuffle_splitter\n",
    "        self._imputer = imputer\n",
    "        self._model = model\n",
    "        self._encoder = encoder\n",
    "        self._X, self._y = self._split_data()\n",
    "        self._scaler = scaler\n",
    "        \n",
    "    @property\n",
    "    def X(self):\n",
    "        return self._X\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._y\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "    \n",
    "    @model.setter\n",
    "    def model(self, model):\n",
    "        self._model = model\n",
    "     \n",
    "    @property\n",
    "    def X_train(self):\n",
    "        return self._X_train_fitted\n",
    "    \n",
    "    @property\n",
    "    def X_test(self):\n",
    "        return self._X_test_fitted\n",
    "    \n",
    "    @property\n",
    "    def y_train(self):\n",
    "        return self._y_train\n",
    "    \n",
    "    @property\n",
    "    def y_test(self):\n",
    "        return self._y_test\n",
    "    \n",
    "    @property\n",
    "    def y_preds(self):\n",
    "        return self._y_preds\n",
    "    \n",
    "    def _split_data(self):\n",
    "        X = self._data.copy()\n",
    "        return X.drop([self._target_name], axis=1) , X[self._target_name]\n",
    "    \n",
    "    def _shuffle_split(self):\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        for train_index, test_index in self._shuffle_splitter.split(X,y):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def _fit_imputer(self, train):\n",
    "        if self._imputer is not None:\n",
    "            self._imputer.fit(train)\n",
    "    \n",
    "    def _fit_scaler(self, train, cont_vars = None):\n",
    "        transform_cols = None\n",
    "        if cont_vars is None:\n",
    "            transform_cols = self.X.columns\n",
    "        else:\n",
    "            transform_cols = cont_vars\n",
    "            \n",
    "        if self._scaler is not None:\n",
    "            self._scaler.fit(train[transform_cols])\n",
    "    \n",
    "    def _impute_data(self, X: pd.DataFrame):\n",
    "        if self._imputer is not None:\n",
    "            return pd.DataFrame(self._imputer.transform(X), columns = self.X.columns, index = X.index)\n",
    "        return X\n",
    "    \n",
    "    def _scale_data(self, X: pd.DataFrame, cont_vars = None):\n",
    "        transform_cols = None\n",
    "        if cont_vars is None:\n",
    "            transform_cols = X.columns\n",
    "        else:\n",
    "            transform_cols = cont_vars\n",
    "        scaled_data = X[transform_cols]\n",
    "        if self._scaler is not None:\n",
    "            scaled_data = pd.DataFrame(self._scaler.transform(X[transform_cols]), columns = transform_cols)\n",
    "        X[transform_cols] = scaled_data\n",
    "        return X\n",
    "    \n",
    "    def _encode_data(self):\n",
    "        df = self.X.copy()\n",
    "        cont_vars = df.describe().columns\n",
    "        cat_vars = set(df.columns) - set(cont_vars)\n",
    "        for column in [*cat_vars]:\n",
    "            df[column] = self._encoder.fit_transform(df[column].astype(str))\n",
    "        self._X = df\n",
    "        return cont_vars, cat_vars\n",
    "        \n",
    "    \n",
    "    def prepare(self):\n",
    "        cont_vars = None\n",
    "        if self._encoder is not None: \n",
    "            cont_vars, _ = self._encode_data()\n",
    "        X_train, X_test, y_train, y_test = self._shuffle_split()   \n",
    "        self._fit_imputer(X_train)\n",
    "        X_train = self._impute_data(X_train)\n",
    "        X_test = self._impute_data(X_test)\n",
    "        self._fit_scaler(X_train, cont_vars)\n",
    "        self._X_train_fitted = self._scale_data(X_train, cont_vars)\n",
    "        self._X_test_fitted = self._scale_data(X_test, cont_vars)\n",
    "        self._y_train = y_train\n",
    "        self._y_test = y_test\n",
    "        \n",
    "    def prepare_and_train(self):\n",
    "        self.prepare()\n",
    "        return self.train()\n",
    "        \n",
    "    def train(self):\n",
    "        self._model.fit(self.X_train, self.y_train)\n",
    "        self._y_preds = self._model.predict(self.X_train)        \n",
    "        \n",
    "        return self.metrics(self.y_train, self.y_preds)\n",
    "        \n",
    "    def test(self):\n",
    "        return self.metrics(self.y_test, self._model.predict(self.X_test))\n",
    "       \n",
    "    @abstractmethod\n",
    "    def metrics(self, y_true = None, y_pred = None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10',\n",
       "       'x11', 'x12', 'x13', 'x14', 'x15', 'x16', 'x17', 'x18', 'x19', 'x20',\n",
       "       'x21', 'x22', 'x23', 'x24', 'x25', 'x26', 'x27', 'x28', 'x29', 'x30',\n",
       "       'x31', 'x32', 'x33', 'x34', 'x35', 'x36', 'x37', 'x38', 'x39', 'x40',\n",
       "       'x41', 'x42', 'x43', 'x44', 'x45', 'x46', 'x47', 'x48', 'x49', 'y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBModel:\n",
    "    _model = None\n",
    "    \n",
    "    def __init__(self, params, num_round: int = 100):\n",
    "        self._params = params\n",
    "        self._num_round = num_round\n",
    "        \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        dtrain = xgb.DMatrix(X, label = y)\n",
    "        self._model = xgb.train(self._params, dtrain)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        dtest = xgb.DMatrix(X)\n",
    "        return self._model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModeling(Modeling):\n",
    "    def __init__(self, \n",
    "                 data: pd.DataFrame, \n",
    "                 target_name: str, \n",
    "                 shuffle_splitter: BaseShuffleSplit, \n",
    "                 imputer: BaseImputer, \n",
    "                 model: BaseModel, \n",
    "                 scaler = None,\n",
    "                 encoder = None,\n",
    "                 beta: int = 1, \n",
    "                 classification: str = 'binary'):\n",
    "        super().__init__(data, target_name, shuffle_splitter, imputer, model, scaler, encoder)\n",
    "        self.beta = beta\n",
    "        self.classification = classification\n",
    "        \n",
    "    @abstractmethod\n",
    "    def metrics(self, y_true = None, y_pred = None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, TypeVar\n",
    "\n",
    "class XGBClassificationModeling(ClassificationModeling):\n",
    "    TXGB = TypeVar(\"TXGB\", bound=XGBClassifier)\n",
    "    all_models = [];\n",
    "    \n",
    "    def __init__(self, \n",
    "             data: pd.DataFrame, \n",
    "             target_name: str, \n",
    "             shuffle_splitter: BaseShuffleSplit, \n",
    "             imputer: BaseImputer, \n",
    "             model: BaseModel, \n",
    "             scaler = None,\n",
    "             encoder = None,\n",
    "             beta: int = 1, \n",
    "             classification: str = 'binary'):\n",
    "         super().__init__(data, target_name, shuffle_splitter, imputer, model, scaler, encoder, beta, classification)\n",
    "        \n",
    "            \n",
    "    def parameter_tuning(self, params, class_to_instantiate: Type[TXGB]):\n",
    "        list_of_models = []\n",
    "        combination = []\n",
    "        params_base = {}\n",
    "        output = []\n",
    "        for key, value in params.items():\n",
    "            if isinstance(value, list):\n",
    "                combination.append((key,value))\n",
    "            else:\n",
    "                params_base[key]=value\n",
    "              \n",
    "        result = XGBClassificationModeling.get_combinations(combination)\n",
    "\n",
    "        for r in result:\n",
    "            list_of_models.append(class_to_instantiate(**{**params_base, **r}))\n",
    "            \n",
    "        for a_model in list_of_models:\n",
    "            self.model = a_model\n",
    "            startTrain = time.time()\n",
    "            train_metrics = self.train()\n",
    "            endTrain = time.time()\n",
    "            test_metrics = self.test()\n",
    "            endTest = time.time()\n",
    "            train_time = endTrain - startTrain\n",
    "            test_time = endTest - endTrain\n",
    "            output.append({'model': a_model, 'train_metrics': {**train_metrics,**{'elapsed_time':train_time}}, 'test_metrics': {**test_metrics,**{'elapsed_time':test_time}}})\n",
    "        self.all_models = output\n",
    "        return output\n",
    "        \n",
    "    def find_best_model(self):\n",
    "        max_accuracy = self.all_models[0]['test_metrics']['accuracy']\n",
    "        location = 0\n",
    "        for indx, output_metrics in enumerate(self.all_models):\n",
    "            if max_accuracy < output_metrics['test_metrics']['accuracy']:\n",
    "                max_accuracy = output_metrics['test_metrics']['accuracy']\n",
    "                location = indx\n",
    "            elif max_accuracy == output_metrics['test_metrics']['accuracy']:\n",
    "                if output_metrics['test_metrics']['elapsed_time'] < self.all_models[location]['test_metrics']['elapsed_time']:\n",
    "                    location = indx\n",
    "                \n",
    "        return self.all_models[location]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_combinations(tuples):\n",
    "        length = len(tuples)\n",
    "        if length > 1:\n",
    "            total_params = []\n",
    "            tuple_copy = tuples.copy()\n",
    "            a_tuple = tuple_copy.pop(0)\n",
    "            params_list = XGBClassificationModeling.get_combinations(tuple_copy)\n",
    "            for value in a_tuple[1]:\n",
    "                for a_params in params_list:\n",
    "                    temp = { a_tuple[0]: value}\n",
    "                    total_params.append({**temp, **a_params})\n",
    "            return total_params\n",
    "        else:\n",
    "            params_list = []\n",
    "            a_tuple =  tuples[0]\n",
    "            for value in a_tuple[1]:\n",
    "                temp = {}\n",
    "                temp[a_tuple[0]] = value\n",
    "                params_list.append(temp)\n",
    "            return params_list\n",
    "            \n",
    "    \n",
    "    def metrics(self, y_true = None, y_pred = None):\n",
    "        if y_true is None and y_pred is None:\n",
    "            y_true = self.y_train\n",
    "            y_pred = self.y_preds       \n",
    "        return {'matrix': confusion_matrix(y_true, y_pred), \n",
    "                'accuracy': round(accuracy_score(y_true, y_pred), 5), \n",
    "                'precision': precision_score(y_true, y_pred, average=self.classification), \n",
    "                'recall': recall_score(y_true, y_pred, average=self.classification),\n",
    "                'f1': f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = XGBClassificationModeling(loader.get_df(),'y',\n",
    "                                           StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=12343),\n",
    "                                           None, XGBClassifier, None, LabelEncoder(), beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2\n",
       "1         1\n",
       "2         1\n",
       "3         1\n",
       "4         1\n",
       "         ..\n",
       "159995    1\n",
       "159996    1\n",
       "159997    1\n",
       "159998    1\n",
       "159999    1\n",
       "Name: x24, Length: 160000, dtype: int64"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_classifier.X['x24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:46:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb_results = xgb_classifier.parameter_tuning( { \n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [100],\n",
    "    'colsample_bytree': [0.3],\n",
    " }, XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                colsample_bynode=1, colsample_bytree=0.3,\n",
       "                enable_categorical=False, gamma=0, gpu_id=-1,\n",
       "                importance_type=None, interaction_constraints='',\n",
       "                learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "                min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "                n_estimators=100, n_jobs=8, num_parallel_tree=1, predictor='auto',\n",
       "                random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                subsample=1, tree_method='exact', validate_parameters=1,\n",
       "                verbosity=None),\n",
       "  'train_metrics': {'matrix': array([[60663,  6399],\n",
       "          [12781, 32157]]),\n",
       "   'accuracy': 0.82875,\n",
       "   'precision': 0.8340336134453782,\n",
       "   'recall': 0.7155859183764297,\n",
       "   'f1': 0.7702828945792514,\n",
       "   'elapsed_time': 24.367619037628174},\n",
       "  'test_metrics': {'matrix': array([[25890,  2851],\n",
       "          [ 5630, 13629]]),\n",
       "   'accuracy': 0.82331,\n",
       "   'precision': 0.827002427184466,\n",
       "   'recall': 0.7076691416999844,\n",
       "   'f1': 0.76269621421976,\n",
       "   'elapsed_time': 0.2505919933319092}}]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:50:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:50:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:51:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:52:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:55:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:57:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:03:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:03:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:04:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:05:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:08:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:11:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:21:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:26:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:30:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:38:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:45:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:58:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:59:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:00:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:03:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:09:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:14:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:13:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:14:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:00:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:01:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:23:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:32:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:51:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:52:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:54:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:58:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:18:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:26:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb_results = xgb_classifier.parameter_tuning( { \n",
    "    'max_depth': [3,6,10],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    " }, XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=0.7,\n",
       "               enable_categorical=False, gamma=0, gpu_id=-1,\n",
       "               importance_type=None, interaction_constraints='',\n",
       "               learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "               n_estimators=1000, n_jobs=8, num_parallel_tree=1,\n",
       "               predictor='auto', random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "               scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "               validate_parameters=1, verbosity=None),\n",
       " 'train_metrics': {'matrix': array([[67062,     0],\n",
       "         [    0, 44938]]),\n",
       "  'accuracy': 1.0,\n",
       "  'precision': 1.0,\n",
       "  'recall': 1.0,\n",
       "  'f1': 1.0,\n",
       "  'elapsed_time': 101146.86843967438},\n",
       " 'test_metrics': {'matrix': array([[27657,  1084],\n",
       "         [ 1452, 17807]]),\n",
       "  'accuracy': 0.94717,\n",
       "  'precision': 0.9426181779683447,\n",
       "  'recall': 0.9246066773975804,\n",
       "  'f1': 0.9335255570117955,\n",
       "  'elapsed_time': 0.7830641269683838}}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_classifier.find_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, TypeVar\n",
    "\n",
    "class TuningClassificationModeling(ClassificationModeling):\n",
    "    TClass = None\n",
    "    all_models = [];\n",
    "    \n",
    "    def __init__(self, \n",
    "             data: pd.DataFrame, \n",
    "             target_name: str, \n",
    "             shuffle_splitter: BaseShuffleSplit, \n",
    "             imputer: BaseImputer, \n",
    "             model: BaseModel, \n",
    "             scaler = None,\n",
    "             encoder = None,\n",
    "             beta: int = 1, \n",
    "             classification: str = 'binary',\n",
    "                 classification_type: str = 'logistic'):\n",
    "        super().__init__(data, target_name, shuffle_splitter, imputer, model, scaler, encoder, beta, classification)\n",
    "        if classification_type == 'logistic':\n",
    "            TClass = TypeVar(\"TClass\", bound=LogisticRegression)\n",
    "\n",
    "    def parameter_tuning(self, params, class_to_instantiate: Type[TClass]):\n",
    "        list_of_models = []\n",
    "        combination = []\n",
    "        params_base = {}\n",
    "        output = []\n",
    "        for key, value in params.items():\n",
    "            if isinstance(value, list):\n",
    "                combination.append((key,value))\n",
    "            else:\n",
    "                params_base[key]=value\n",
    "        result = {}\n",
    "        if len(combination) > 0:       \n",
    "            result = TuningClassificationModeling.get_combinations(combination)\n",
    "        print(params_base)\n",
    "        for r in result:\n",
    "            list_of_models.append(class_to_instantiate(**{**params_base, **r}))\n",
    "            \n",
    "        for a_model in list_of_models:\n",
    "            print(a_model)\n",
    "            self.model = a_model\n",
    "            startTrain = time.time()\n",
    "            train_metrics = self.train()\n",
    "            endTrain = time.time()\n",
    "            test_metrics = self.test()\n",
    "            endTest = time.time()\n",
    "            train_time = endTrain - startTrain\n",
    "            test_time = endTest - endTrain\n",
    "            output.append({'model': a_model, 'train_metrics': {**train_metrics,**{'elapsed_time':train_time}}, 'test_metrics': {**test_metrics,**{'elapsed_time':test_time}}})\n",
    "        self.all_models = output\n",
    "        return output\n",
    "        \n",
    "    def find_best_model(self):\n",
    "        max_accuracy = self.all_models[0]['test_metrics']['accuracy']\n",
    "        location = 0\n",
    "        for indx, output_metrics in enumerate(self.all_models):\n",
    "            if max_accuracy < output_metrics['test_metrics']['accuracy']:\n",
    "                max_accuracy = output_metrics['test_metrics']['accuracy']\n",
    "                location = indx\n",
    "            elif max_accuracy == output_metrics['test_metrics']['accuracy']:\n",
    "                if output_metrics['test_metrics']['elapsed_time'] < self.all_models[location]['test_metrics']['elapsed_time']:\n",
    "                    location = indx\n",
    "                \n",
    "        return self.all_models[location]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_combinations(tuples):\n",
    "        length = len(tuples)\n",
    "        if length > 1:\n",
    "            total_params = []\n",
    "            tuple_copy = tuples.copy()\n",
    "            a_tuple = tuple_copy.pop(0)\n",
    "            params_list = TuningClassificationModeling.get_combinations(tuple_copy)\n",
    "            for value in a_tuple[1]:\n",
    "                for a_params in params_list:\n",
    "                    temp = { a_tuple[0]: value}\n",
    "                    total_params.append({**temp, **a_params})\n",
    "            return total_params\n",
    "        else:\n",
    "            params_list = []\n",
    "            a_tuple =  tuples[0]\n",
    "            for value in a_tuple[1]:\n",
    "                temp = {}\n",
    "                temp[a_tuple[0]] = value\n",
    "                params_list.append(temp)\n",
    "            return params_list\n",
    "            \n",
    "    \n",
    "    def metrics(self, y_true = None, y_pred = None, y_pred_proba=None):\n",
    "        if y_true is None and y_pred is None:\n",
    "            y_true = self.y_train\n",
    "            y_pred = self.y_preds\n",
    "        conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        return {'matrix': conf_matrix,\n",
    "                'cost': TuningClassificationModeling.cost_calc(conf_matrix),\n",
    "                'accuracy': round(accuracy_score(y_true, y_pred), 5), \n",
    "                'precision': precision_score(y_true, y_pred, average=self.classification), \n",
    "                'recall': recall_score(y_true, y_pred, average=self.classification),\n",
    "                'f1': f1_score(y_true, y_pred),\n",
    "                'y_proba': y_pred_proba}\n",
    "    \n",
    "    @staticmethod\n",
    "    def cost_calc(conf_matrix):\n",
    "        cost_matrix = np.array([[0,-100],[-25,0]])\n",
    "        cost = np.sum(cost_matrix*conf_matrix)/np.sum(conf_matrix)\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_modeling = TuningClassificationModeling(loader.get_df(),'y',\n",
    "                                           StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=12343),\n",
    "                                           SimpleImputer(missing_values=np.nan, strategy='mean'), LogisticRegression, None, LabelEncoder(), beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_modeling.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l2', 'random_state': 1, 'solver': 'liblinear'}\n",
      "LogisticRegression(C=0.001, random_state=1, solver='liblinear')\n",
      "LogisticRegression(C=0.01, random_state=1, solver='liblinear')\n",
      "LogisticRegression(C=1, random_state=1, solver='liblinear')\n",
      "LogisticRegression(C=10, random_state=1, solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "linear_result = linear_modeling.parameter_tuning( { \n",
    "    'penalty':'l2',\n",
    "    'random_state':1,\n",
    "    'solver': 'liblinear',\n",
    "    'C':  [0.001, 0.01, 1, 10],\n",
    " }, LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': LogisticRegression(C=0.01, random_state=1, solver='liblinear'),\n",
       " 'train_metrics': {'matrix': array([[55483, 11579],\n",
       "         [21607, 23331]]),\n",
       "  'cost': -15.161383928571428,\n",
       "  'accuracy': 0.7037,\n",
       "  'precision': 0.6683185333715268,\n",
       "  'recall': 0.5191819840669367,\n",
       "  'f1': 0.584385332131049,\n",
       "  'elapsed_time': 12.044059038162231},\n",
       " 'test_metrics': {'matrix': array([[23690,  5051],\n",
       "         [ 9282,  9977]]),\n",
       "  'cost': -15.357291666666667,\n",
       "  'accuracy': 0.7014,\n",
       "  'precision': 0.6638940644130955,\n",
       "  'recall': 0.5180435121242016,\n",
       "  'f1': 0.5819698427975618,\n",
       "  'elapsed_time': 0.12191271781921387}}"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_modeling.find_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNTuningClassificationModeling(TuningClassificationModeling):\n",
    "    TClass = None\n",
    "    all_models = [];\n",
    "    \n",
    "    def __init__(self, \n",
    "             data: pd.DataFrame, \n",
    "             target_name: str, \n",
    "             shuffle_splitter: BaseShuffleSplit, \n",
    "             imputer: BaseImputer, \n",
    "             model: BaseModel, \n",
    "             scaler = None,\n",
    "             encoder = None,\n",
    "             beta: int = 1, \n",
    "             classification: str = 'binary',\n",
    "                 classification_type: str = 'logistic'):\n",
    "        super().__init__(data, target_name, shuffle_splitter, imputer, model, scaler, encoder, beta, classification, classification_type)\n",
    "        if classification_type == 'neural':\n",
    "            TClass = TypeVar(\"TClass\", bound=NNModel)\n",
    "                \n",
    "#     def train(self, epoch, batch):\n",
    "#         logDir = \"logs/{epoch}-{batchsize}-{time}\".format(epoch=epoch, batchsize=batch, time=time.time())\n",
    "#         self.tb_callback.log_dir = logDir\n",
    "#         self._model.fit(self.X_train, self.y_train, batch_size=batch, epochs=epoch, validation_data=(self.X_test, self.y_test), callbacks=[self.tb_callback])\n",
    "#         self._y_preds = self._model.predict(self.X_train)\n",
    "#         return self.metrics(self.y_train, self.y_preds)\n",
    "    \n",
    "    def metrics(self, y_true = None, y_pred = None):\n",
    "        if y_true is None and y_pred is None:\n",
    "            y_true = self.y_train\n",
    "            y_pred = self.y_preds\n",
    "            \n",
    "        y_pred_proba= pd.Series(y_pred.reshape((y_pred.shape[1], y_pred.shape[0]))[0], index=y_true.index)\n",
    "        y_pred = pd.Series( (y_pred_proba>0.5).astype(int), index=y_true.index)\n",
    "        return super().metrics(y_true,y_pred, y_pred_proba)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel:\n",
    "    model = None\n",
    "    epoch = 50\n",
    "    batch_size = 32\n",
    "    loss = 'BinaryCrossentropy',\n",
    "    metric = 'accuracy'\n",
    "    optimizer = 'adam'\n",
    "    \n",
    "    def __init__(self,**inputs):\n",
    "        self.model = tf.keras.Sequential()\n",
    "        for arg, content in inputs.items():\n",
    "            if arg.startswith('input'):\n",
    "                self.model.add( tf.keras.layers.Input( shape=(content,) ) )\n",
    "            if arg.startswith('layer'):\n",
    "                self.model.add( tf.keras.layers.Dense(content['s'], activation = content['activation']) )\n",
    "            if arg == 'epoch':\n",
    "                self.epoch = content\n",
    "            if arg == 'bs':\n",
    "                self.batch_size = content\n",
    "            if arg == 'optimizer':\n",
    "                self.optimizer = content\n",
    "            if arg == 'loss':\n",
    "                self.loss = content\n",
    "            if arg == 'metric':\n",
    "                self.metric = content\n",
    "        self.model.compile(optimizer=self.optimizer, loss=self.loss, metrics=[self.metric])\n",
    "        print(self.model)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        #validation_data=(self.X_test, self.y_test), callbacks=[self.tb_callback]\n",
    "        self.model.fit(X, y, batch_size=self.batch_size, epochs=self.epoch)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x7ff6c56de490>\n"
     ]
    }
   ],
   "source": [
    "mynn= NNModel(input=67,\n",
    "        layer1={'s':300, 'activation': 'relu'}, \n",
    "        layer2={'s':200, 'activation': 'relu'}, \n",
    "        layer3={'s':100, 'activation': 'relu'},\n",
    "        layer4={'s':1, 'activation':'sigmoid'},\n",
    "        loss='BinaryCrossentropy',\n",
    "        metric='accuracy',\n",
    "        epoch=30,\n",
    "        bs=100, \n",
    "        optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 300)               20400     \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 100,801\n",
      "Trainable params: 100,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mynn.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_modeling = NNTuningClassificationModeling(loader.get_df(),'y',\n",
    "                                           StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=12343),\n",
    "                                           SimpleImputer(missing_values=np.nan, strategy='mean'), NNModel, None, LabelEncoder(), beta=1,classification_type='neural' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_modeling.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 50, 'layer1': {'s': 300, 'activation': 'relu'}, 'layer2': {'s': 200, 'activation': 'relu'}, 'layer3': {'s': 100, 'activation': 'relu'}, 'layer4': {'s': 1, 'activation': 'sigmoid'}, 'loss': 'BinaryCrossentropy', 'metric': 'accuracy', 'epoch': 100, 'optimizer': 'adam'}\n",
      "<keras.engine.sequential.Sequential object at 0x7ff6c97236a0>\n",
      "<keras.engine.sequential.Sequential object at 0x7ff6c9726fa0>\n",
      "<__main__.NNModel object at 0x7ff6c9723160>\n",
      "Epoch 1/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.6810 - accuracy: 0.7874\n",
      "Epoch 2/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.3138 - accuracy: 0.8700\n",
      "Epoch 3/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.2575 - accuracy: 0.8953\n",
      "Epoch 4/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.2301 - accuracy: 0.9083\n",
      "Epoch 5/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.2032 - accuracy: 0.9209\n",
      "Epoch 6/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1816 - accuracy: 0.9316\n",
      "Epoch 7/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1646 - accuracy: 0.9390\n",
      "Epoch 8/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1506 - accuracy: 0.9449\n",
      "Epoch 9/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1374 - accuracy: 0.9513\n",
      "Epoch 10/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1280 - accuracy: 0.9552\n",
      "Epoch 11/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1213 - accuracy: 0.9577\n",
      "Epoch 12/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1145 - accuracy: 0.9605\n",
      "Epoch 13/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1100 - accuracy: 0.9630\n",
      "Epoch 14/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1050 - accuracy: 0.9643\n",
      "Epoch 15/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1004 - accuracy: 0.9668\n",
      "Epoch 16/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0970 - accuracy: 0.9685\n",
      "Epoch 17/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0928 - accuracy: 0.9692\n",
      "Epoch 18/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0893 - accuracy: 0.9701\n",
      "Epoch 19/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0869 - accuracy: 0.9717\n",
      "Epoch 20/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0833 - accuracy: 0.9725\n",
      "Epoch 21/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0797 - accuracy: 0.9737\n",
      "Epoch 22/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0772 - accuracy: 0.9746\n",
      "Epoch 23/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0756 - accuracy: 0.9749\n",
      "Epoch 24/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0725 - accuracy: 0.9758\n",
      "Epoch 25/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0698 - accuracy: 0.9769\n",
      "Epoch 26/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0660 - accuracy: 0.9778\n",
      "Epoch 27/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0662 - accuracy: 0.9780\n",
      "Epoch 28/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0627 - accuracy: 0.9794\n",
      "Epoch 29/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0610 - accuracy: 0.9801\n",
      "Epoch 30/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0576 - accuracy: 0.9809\n",
      "Epoch 31/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0577 - accuracy: 0.9805\n",
      "Epoch 32/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0539 - accuracy: 0.9820\n",
      "Epoch 33/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0529 - accuracy: 0.9820\n",
      "Epoch 34/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0530 - accuracy: 0.9822\n",
      "Epoch 35/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0502 - accuracy: 0.9831\n",
      "Epoch 36/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0500 - accuracy: 0.9829\n",
      "Epoch 37/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0488 - accuracy: 0.9837\n",
      "Epoch 38/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0471 - accuracy: 0.9837\n",
      "Epoch 39/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0465 - accuracy: 0.9843\n",
      "Epoch 40/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0445 - accuracy: 0.9847\n",
      "Epoch 41/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0439 - accuracy: 0.9851\n",
      "Epoch 42/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0420 - accuracy: 0.9852\n",
      "Epoch 43/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0419 - accuracy: 0.9860\n",
      "Epoch 44/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0406 - accuracy: 0.9860\n",
      "Epoch 45/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0399 - accuracy: 0.9865\n",
      "Epoch 46/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0400 - accuracy: 0.9863\n",
      "Epoch 47/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0386 - accuracy: 0.9871\n",
      "Epoch 48/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0372 - accuracy: 0.9872\n",
      "Epoch 49/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0390 - accuracy: 0.9867\n",
      "Epoch 50/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0374 - accuracy: 0.9874\n",
      "Epoch 51/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0359 - accuracy: 0.9880\n",
      "Epoch 52/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0360 - accuracy: 0.9881\n",
      "Epoch 53/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0345 - accuracy: 0.9880\n",
      "Epoch 54/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0357 - accuracy: 0.9882\n",
      "Epoch 55/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0337 - accuracy: 0.9886\n",
      "Epoch 56/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0336 - accuracy: 0.9889\n",
      "Epoch 57/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0312 - accuracy: 0.9895\n",
      "Epoch 58/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0321 - accuracy: 0.9890\n",
      "Epoch 59/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0324 - accuracy: 0.9894\n",
      "Epoch 60/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0321 - accuracy: 0.9892\n",
      "Epoch 61/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0305 - accuracy: 0.9895\n",
      "Epoch 62/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0309 - accuracy: 0.9897\n",
      "Epoch 63/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0310 - accuracy: 0.9897\n",
      "Epoch 64/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0269 - accuracy: 0.9905\n",
      "Epoch 65/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0304 - accuracy: 0.9896\n",
      "Epoch 66/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0291 - accuracy: 0.9903\n",
      "Epoch 67/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0297 - accuracy: 0.9895\n",
      "Epoch 68/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0275 - accuracy: 0.9909\n",
      "Epoch 69/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0262 - accuracy: 0.9911\n",
      "Epoch 70/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0276 - accuracy: 0.9909\n",
      "Epoch 71/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0295 - accuracy: 0.9902\n",
      "Epoch 72/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0277 - accuracy: 0.9906\n",
      "Epoch 73/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0263 - accuracy: 0.9914\n",
      "Epoch 74/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0275 - accuracy: 0.9908\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0250 - accuracy: 0.9917\n",
      "Epoch 76/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0247 - accuracy: 0.9916\n",
      "Epoch 77/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0257 - accuracy: 0.9914\n",
      "Epoch 78/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0255 - accuracy: 0.9918\n",
      "Epoch 79/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0235 - accuracy: 0.9922\n",
      "Epoch 80/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0276 - accuracy: 0.9912\n",
      "Epoch 81/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0250 - accuracy: 0.9919\n",
      "Epoch 82/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0242 - accuracy: 0.9923\n",
      "Epoch 83/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0237 - accuracy: 0.9920\n",
      "Epoch 84/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0234 - accuracy: 0.9923\n",
      "Epoch 85/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0253 - accuracy: 0.9916\n",
      "Epoch 86/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0227 - accuracy: 0.9924\n",
      "Epoch 87/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0242 - accuracy: 0.9920\n",
      "Epoch 88/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0227 - accuracy: 0.9924\n",
      "Epoch 89/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0240 - accuracy: 0.9921\n",
      "Epoch 90/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0218 - accuracy: 0.9926\n",
      "Epoch 91/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0236 - accuracy: 0.9923\n",
      "Epoch 92/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0225 - accuracy: 0.9927\n",
      "Epoch 93/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0223 - accuracy: 0.9927\n",
      "Epoch 94/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0208 - accuracy: 0.9930\n",
      "Epoch 95/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0232 - accuracy: 0.9927\n",
      "Epoch 96/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0219 - accuracy: 0.9930\n",
      "Epoch 97/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0232 - accuracy: 0.9926\n",
      "Epoch 98/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0207 - accuracy: 0.9930\n",
      "Epoch 99/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0218 - accuracy: 0.9929\n",
      "Epoch 100/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0195 - accuracy: 0.9934\n",
      "<__main__.NNModel object at 0x7ff6c9723550>\n",
      "Epoch 1/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 1.4778 - accuracy: 0.7046\n",
      "Epoch 2/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.4687 - accuracy: 0.7983\n",
      "Epoch 3/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.4125 - accuracy: 0.8292\n",
      "Epoch 4/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.3488 - accuracy: 0.8559\n",
      "Epoch 5/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.3065 - accuracy: 0.8726\n",
      "Epoch 6/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2798 - accuracy: 0.8856\n",
      "Epoch 7/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2607 - accuracy: 0.8943\n",
      "Epoch 8/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2461 - accuracy: 0.9013\n",
      "Epoch 9/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2487 - accuracy: 0.9010\n",
      "Epoch 10/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.2366 - accuracy: 0.9067\n",
      "Epoch 11/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.2355 - accuracy: 0.9087\n",
      "Epoch 12/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.2020 - accuracy: 0.9224\n",
      "Epoch 13/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2086 - accuracy: 0.9201\n",
      "Epoch 14/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1923 - accuracy: 0.9262\n",
      "Epoch 15/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1812 - accuracy: 0.9312\n",
      "Epoch 16/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1817 - accuracy: 0.9309\n",
      "Epoch 17/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1911 - accuracy: 0.9281\n",
      "Epoch 18/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1778 - accuracy: 0.9331\n",
      "Epoch 19/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1670 - accuracy: 0.9373\n",
      "Epoch 20/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1796 - accuracy: 0.9326\n",
      "Epoch 21/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1642 - accuracy: 0.9383\n",
      "Epoch 22/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1565 - accuracy: 0.9414\n",
      "Epoch 23/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1538 - accuracy: 0.9431\n",
      "Epoch 24/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1553 - accuracy: 0.9426\n",
      "Epoch 25/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1544 - accuracy: 0.9436\n",
      "Epoch 26/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1419 - accuracy: 0.9471\n",
      "Epoch 27/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1426 - accuracy: 0.9481\n",
      "Epoch 28/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1383 - accuracy: 0.9492\n",
      "Epoch 29/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1336 - accuracy: 0.9512\n",
      "Epoch 30/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1302 - accuracy: 0.9527\n",
      "Epoch 31/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1335 - accuracy: 0.9515\n",
      "Epoch 32/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1302 - accuracy: 0.9535\n",
      "Epoch 33/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1238 - accuracy: 0.9554\n",
      "Epoch 34/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1200 - accuracy: 0.9570\n",
      "Epoch 35/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1241 - accuracy: 0.9551\n",
      "Epoch 36/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1154 - accuracy: 0.9586\n",
      "Epoch 37/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1155 - accuracy: 0.9590\n",
      "Epoch 38/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1157 - accuracy: 0.9588\n",
      "Epoch 39/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1104 - accuracy: 0.9609\n",
      "Epoch 40/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1078 - accuracy: 0.9617\n",
      "Epoch 41/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1064 - accuracy: 0.9618\n",
      "Epoch 42/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1044 - accuracy: 0.9630\n",
      "Epoch 43/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1012 - accuracy: 0.9640\n",
      "Epoch 44/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1017 - accuracy: 0.9637\n",
      "Epoch 45/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1041 - accuracy: 0.9629\n",
      "Epoch 46/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0998 - accuracy: 0.9644\n",
      "Epoch 47/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1011 - accuracy: 0.9646\n",
      "Epoch 48/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0923 - accuracy: 0.9674\n",
      "Epoch 49/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0961 - accuracy: 0.9657\n",
      "Epoch 50/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0914 - accuracy: 0.9677\n",
      "Epoch 51/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0936 - accuracy: 0.9673\n",
      "Epoch 52/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0886 - accuracy: 0.9688\n",
      "Epoch 53/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0903 - accuracy: 0.9681\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0870 - accuracy: 0.9697\n",
      "Epoch 55/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0864 - accuracy: 0.9696\n",
      "Epoch 56/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0892 - accuracy: 0.9685\n",
      "Epoch 57/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0819 - accuracy: 0.9713\n",
      "Epoch 58/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0825 - accuracy: 0.9715\n",
      "Epoch 59/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0802 - accuracy: 0.9722\n",
      "Epoch 60/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0762 - accuracy: 0.9735\n",
      "Epoch 61/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0771 - accuracy: 0.9729\n",
      "Epoch 62/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0764 - accuracy: 0.9739\n",
      "Epoch 63/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0759 - accuracy: 0.9737\n",
      "Epoch 64/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0764 - accuracy: 0.9731\n",
      "Epoch 65/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0767 - accuracy: 0.9733\n",
      "Epoch 66/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0695 - accuracy: 0.9762\n",
      "Epoch 67/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0698 - accuracy: 0.9756\n",
      "Epoch 68/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0668 - accuracy: 0.9764\n",
      "Epoch 69/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0685 - accuracy: 0.9762\n",
      "Epoch 70/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0650 - accuracy: 0.9775\n",
      "Epoch 71/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0661 - accuracy: 0.9772\n",
      "Epoch 72/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0647 - accuracy: 0.9780\n",
      "Epoch 73/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0642 - accuracy: 0.9780\n",
      "Epoch 74/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0650 - accuracy: 0.9772\n",
      "Epoch 75/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0606 - accuracy: 0.9790\n",
      "Epoch 76/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0619 - accuracy: 0.9790\n",
      "Epoch 77/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0576 - accuracy: 0.9804\n",
      "Epoch 78/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0572 - accuracy: 0.9801\n",
      "Epoch 79/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0524 - accuracy: 0.9822\n",
      "Epoch 80/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0612 - accuracy: 0.9788\n",
      "Epoch 81/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0576 - accuracy: 0.9803\n",
      "Epoch 82/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0536 - accuracy: 0.9819\n",
      "Epoch 83/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0539 - accuracy: 0.9815\n",
      "Epoch 84/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0496 - accuracy: 0.9833\n",
      "Epoch 85/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0512 - accuracy: 0.9827\n",
      "Epoch 86/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0511 - accuracy: 0.9827\n",
      "Epoch 87/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0531 - accuracy: 0.9819\n",
      "Epoch 88/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0478 - accuracy: 0.9837\n",
      "Epoch 89/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0452 - accuracy: 0.9847\n",
      "Epoch 90/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0495 - accuracy: 0.9829\n",
      "Epoch 91/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0491 - accuracy: 0.9829\n",
      "Epoch 92/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0456 - accuracy: 0.9845\n",
      "Epoch 93/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0429 - accuracy: 0.9856\n",
      "Epoch 94/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0416 - accuracy: 0.9857\n",
      "Epoch 95/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0437 - accuracy: 0.9854\n",
      "Epoch 96/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0468 - accuracy: 0.9842\n",
      "Epoch 97/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0463 - accuracy: 0.9840\n",
      "Epoch 98/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0435 - accuracy: 0.9855\n",
      "Epoch 99/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0403 - accuracy: 0.9862\n",
      "Epoch 100/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0422 - accuracy: 0.9852\n"
     ]
    }
   ],
   "source": [
    "nn_model_tunning = nn_modeling.parameter_tuning( { \n",
    "        'input':50,\n",
    "        'layer1':{'s':300, 'activation': 'relu'}, \n",
    "        'layer2':{'s':200, 'activation': 'relu'}, \n",
    "        'layer3':{'s':100, 'activation': 'relu'},\n",
    "        'layer4':{'s':1, 'activation':'sigmoid'},\n",
    "        'loss':'BinaryCrossentropy',\n",
    "        'metric':'accuracy',\n",
    "        'epoch':100,\n",
    "        'bs':[100,1000], \n",
    "        'optimizer':'adam'\n",
    " }, NNModel)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model_tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_modeling.find_best_model()['model'].batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': <__main__.NNModel at 0x7ff6c9723160>,\n",
       " 'train_metrics': {'matrix': array([[66850,   212],\n",
       "         [  517, 44421]]),\n",
       "  'accuracy': 0.99349,\n",
       "  'precision': 0.9952501512333923,\n",
       "  'recall': 0.9884952601361876,\n",
       "  'f1': 0.9918612050775362,\n",
       "  'elapsed_time': 259.07529282569885},\n",
       " 'test_metrics': {'matrix': array([[27908,   833],\n",
       "         [ 1143, 18116]]),\n",
       "  'accuracy': 0.95883,\n",
       "  'precision': 0.9560398965644625,\n",
       "  'recall': 0.9406511241497482,\n",
       "  'f1': 0.9482830820770519,\n",
       "  'elapsed_time': 1.0936682224273682}}"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_modeling.find_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add input\n",
      "add layer\n",
      "add layer\n",
      "add layer\n",
      "set epoch\n",
      "set bs\n"
     ]
    }
   ],
   "source": [
    "sample_multiargument(input=67,layer1=300, layer2=200, layer3=100, epoch=30, bs=100, optimizer='adam')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
