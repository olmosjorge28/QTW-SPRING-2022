{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoCXzNvN8g-8"
   },
   "source": [
    "# Case Study 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBy24RcB8g-9"
   },
   "source": [
    "__Team Members:__ Amber Clark, Andrew Leppla, Jorge Olmos, Paritosh Rai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4O0up-U8g-9"
   },
   "source": [
    "# Content\n",
    "* [Business Understanding](#business-understanding)\n",
    "    - [Introduction](#introduction)\n",
    "    - [Methods](#methods)\n",
    "    - [Results](#results)\n",
    "* [Data Evaluation](#data-evaluation)\n",
    "    - [Loading Data](#loading-data) \n",
    "    - [Data Summary](#data-summary)\n",
    "    - [Missing Values](#missing-values)\n",
    "    - [Exploratory Data Analysis (EDA)](#eda)\n",
    "* [Model Preparations](#model-preparations)\n",
    "    - [Sampling & Scaling Data](#sampling-scaling-data)\n",
    "    - [Proposed Method](#proposed-metrics)\n",
    "    - [Evaluation Metrics](#evaluation-metrics)\n",
    "    - [Feature Selection](#feature-selection)\n",
    "* [Model Building & Evaluations](#model-building)\n",
    "    - [Performance Analysis](#performance-analysis)\n",
    "* [Model Interpretability & Explainability](#model-explanation)\n",
    "    - [Examining Feature Importance](#examining-feature-importance)\n",
    "* [Conclusion](#conclusion)\n",
    "    - [Final Model Proposal](#final-model-proposal)\n",
    "    - [Future Considerations, Model Enhancements and Alternative Modeling Approaches](#model-enhancements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRedT-FB8g_A"
   },
   "source": [
    "# Business Understanding & Executive Summary <a id='business-understanding'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-4BiuuQOEh4"
   },
   "source": [
    "## Objective:\n",
    "\n",
    "The objective of this case study is to classify a binary target in an anonymous dataset with the goal of reducing monetary losses as much as possible for the customer.\n",
    "\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Modeling:\n",
    "\n",
    "### Training and Test Split\n",
    "\n",
    "\n",
    "### Key Metrics\n",
    "       \n",
    "\n",
    "### Model Building\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "\n",
    "\n",
    "## Future Considerations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVtcYu5j8g_B"
   },
   "source": [
    "# Data Evaluation <a id='data-evaluation'>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Owfb6XnGfPKI"
   },
   "source": [
    "Summarize the data being used in the case using appropriate mediums (charts, graphs, tables); address questions such as: Are there missing values? Which variables are needed (which ones are not)? What assumptions or conclusions are you drawing that need to be relayed to your audience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WcvKI3y8g_C"
   },
   "source": [
    "## Loading Data <a id='loading-data'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import re\n",
    "import os\n",
    "from IPython.display import Image\n",
    "from abc import ABC, abstractmethod\n",
    "import time\n",
    "#import sklearn\n",
    "#import time\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tabulate import tabulate\n",
    "from IPython.display import clear_output\n",
    "import xgboost\n",
    "\n",
    "# data pre-processing\n",
    "from scipy.io import arff\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.impute._base import _BaseImputer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection._split import BaseShuffleSplit\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# prediction models\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm._base import BaseSVC \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.metrics import AUC\n",
    "# import warnings filter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from warnings import simplefilter \n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilePathManager:\n",
    "    def __init__(self, local_dir: str):\n",
    "        self.local_dir = local_dir\n",
    "    \n",
    "    def retrieve_full_path(self):\n",
    "        return os.getcwd()+'/'+self.local_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    def load_data(self, file_name):\n",
    "        pass\n",
    "    \n",
    "    def get_df(self):\n",
    "        pass\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    " \n",
    "class CSVLoader(Loader):\n",
    "    def __init__(self, file_path_manager: FilePathManager):\n",
    "        self.file_path_manager = file_path_manager\n",
    "        \n",
    "    def load_data(self, _prepare_data: Callable[[pd.DataFrame], pd.DataFrame] = None):\n",
    "        self.df = pd.read_csv(self.file_path_manager.retrieve_full_path())\n",
    "        if _prepare_data:\n",
    "            self.df = _prepare_data(self.df)\n",
    "    \n",
    "    def get_df(self):\n",
    "        return self.df;\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df['y'] = df['y'].astype(int)\n",
    "    df['x32'] = df['x32'].str.replace('%','').astype(float)\n",
    "    df['x37'] = df['x37'].str.replace('$','').astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(FilePathManager('final_project(5).csv'))\n",
    "loader.load_data(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ul_6nw48N5Dy"
   },
   "source": [
    "## Data Summary <a id='data-summary'>\n",
    "    \n",
    "The dataset consists of fifty (50) features and a binary target class. There is no metadata or other descriptive information for the dataset, and the fifty feature labels are numbered from \"x0\" to \"x49\". There are 160,000 observations in the dataset; less than 0.03% of the features were missing data, and the imputation of these missing values is described below in the Missing Data section. Most of the features provided are numeric, but five were initially imported as text features.\n",
    "    \n",
    "Three of the five text features were identified as continents, months of the year, and days of the week. The values were cleaned up for spelling correction and consistency. The other two text object columns were numeric columns with a special character introduced in the data; column x32 had a trailing \"%\" and column x37 had a leading \"$\". These characters were removed so that these columns would be treated as numeric.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aws5HAx98g_E"
   },
   "source": [
    "## Missing Values <a id='missing-values'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the variables, except the target class, had missing values. The chart below depicts the number of observations missing values for each feature. Note: Even though the plot doesn't show missing values for categorical features, they do have missing values which are represented as nan's and so are missing from the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/olmosjorge28/QTW-SPRING-2022/main/ds7333_case_study_7/visuals/missing_values.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of missing values was consistently around 20-40 missing observations for each column (less than 0.03% of 160,000 observations). For the logistic regression and neural network models, the mean of each column was used to impute the missing values for the numeric data, and the mode of each column was used for the missing categorical features.  For the XGBoost model, the algorithm can automatically handle missing values and find their optimal split for modeling, so no imputation was done prior to modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbAmkozvN5Dz"
   },
   "source": [
    "## Exploratory Data Analysis (EDA) <a id='eda'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numeric data was examined to view the scales of the variables, and the data needs normalization to be effectively used in most types of models without issues. \n",
    "\n",
    "For two model types, logistic regression and neural network, the categorical data for the three text columns were one-hot encoded to produce binary features for each of the values within those variables. In this data, there were three continents, twelve months, and five days of the week, so the one-hot encoding process did not contribute to creating an excess of sparsity in the dataframe that would be used for modeling. After one-hot encoding, the total number of explanatory features has increased to 67.\n",
    "For the third model type, XGBoost, the categorical data were not one-hot encoded but rather label-encoded so the tree-based algorithm could split the data effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance of Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target classes are considered balanced in the dataset, with roughly 40:60 split between the positive and negative classes, as depicted below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/olmosjorge28/QTW-SPRING-2022/main/ds7333_case_study_7/visuals/y_dist.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmuI_mep8g_b"
   },
   "source": [
    "# Model Preparations <a id='model-preparations'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseImputer:\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X):\n",
    "        pass\n",
    "\n",
    "class BaseModel:\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modeling:\n",
    "    _X_train_fitted = None\n",
    "    _X_test_fitted = None\n",
    "    _y_train = None\n",
    "    _y_test = None\n",
    "    _y_preds = None\n",
    "    _y_preds_proba = None\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, \n",
    "                 target_name: str, \n",
    "                 shuffle_splitter: BaseShuffleSplit, \n",
    "                 imputer: BaseImputer, \n",
    "                 model: BaseModel, scaler = None, encoder = None):\n",
    "        self._data = data\n",
    "        self._target_name = target_name\n",
    "        self._shuffle_splitter = shuffle_splitter\n",
    "        self._imputer = imputer\n",
    "        self._model = model\n",
    "        self._encoder = encoder\n",
    "        self._X, self._y = self._split_data()\n",
    "        self._scaler = scaler\n",
    "        \n",
    "    @property\n",
    "    def X(self):\n",
    "        return self._X\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._y\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "    \n",
    "    @model.setter\n",
    "    def model(self, model):\n",
    "        self._model = model\n",
    "     \n",
    "    @property\n",
    "    def X_train(self):\n",
    "        return self._X_train_fitted\n",
    "    \n",
    "    @property\n",
    "    def X_test(self):\n",
    "        return self._X_test_fitted\n",
    "    \n",
    "    @property\n",
    "    def y_train(self):\n",
    "        return self._y_train\n",
    "    \n",
    "    @property\n",
    "    def y_test(self):\n",
    "        return self._y_test\n",
    "    \n",
    "    @property\n",
    "    def y_preds(self):\n",
    "        return self._y_preds\n",
    "    \n",
    "    def _split_data(self):\n",
    "        X = self._data.copy()\n",
    "        return X.drop([self._target_name], axis=1) , X[self._target_name]\n",
    "    \n",
    "    def _shuffle_split(self):\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        for train_index, test_index in self._shuffle_splitter.split(X,y):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def _fit_imputer(self, train):\n",
    "        if self._imputer is not None:\n",
    "            self._imputer.fit(train)\n",
    "    \n",
    "    def _fit_scaler(self, train, cont_vars = None):\n",
    "        transform_cols = None\n",
    "        if cont_vars is None:\n",
    "            transform_cols = self.X.columns\n",
    "        else:\n",
    "            transform_cols = cont_vars\n",
    "            \n",
    "        if self._scaler is not None:\n",
    "            self._scaler.fit(train[transform_cols])\n",
    "    \n",
    "    def _impute_data(self, X: pd.DataFrame):\n",
    "        if self._imputer is not None:\n",
    "            return pd.DataFrame(self._imputer.transform(X), columns = self.X.columns, index = X.index)\n",
    "        return X\n",
    "    \n",
    "    def _scale_data(self, X: pd.DataFrame, cont_vars = None):\n",
    "        transform_cols = None\n",
    "        if cont_vars is None:\n",
    "            transform_cols = X.columns\n",
    "        else:\n",
    "            transform_cols = cont_vars\n",
    "        scaled_data = X[transform_cols]\n",
    "        if self._scaler is not None:\n",
    "            scaled_data = pd.DataFrame(self._scaler.transform(X[transform_cols]), columns = transform_cols)\n",
    "        X[transform_cols] = scaled_data\n",
    "        return X\n",
    "    \n",
    "    def _encode_data(self):\n",
    "        df = self.X.copy()\n",
    "        cont_vars = df.describe().columns\n",
    "        cat_vars = set(df.columns) - set(cont_vars)\n",
    "        for column in [*cat_vars]:\n",
    "            df[column] = self._encoder.fit_transform(df[column].astype(str))\n",
    "        self._X = df\n",
    "        return cont_vars, cat_vars\n",
    "        \n",
    "    \n",
    "    def prepare(self):\n",
    "        cont_vars = None\n",
    "        if self._encoder is not None: \n",
    "            cont_vars, _ = self._encode_data()\n",
    "        X_train, X_test, y_train, y_test = self._shuffle_split()   \n",
    "        self._fit_imputer(X_train)\n",
    "        X_train = self._impute_data(X_train)\n",
    "        X_test = self._impute_data(X_test)\n",
    "        self._fit_scaler(X_train, cont_vars)\n",
    "        self._X_train_fitted = self._scale_data(X_train, cont_vars)\n",
    "        self._X_test_fitted = self._scale_data(X_test, cont_vars)\n",
    "        self._y_train = y_train\n",
    "        self._y_test = y_test\n",
    "        \n",
    "    def prepare_and_train(self):\n",
    "        self.prepare()\n",
    "        return self.train()\n",
    "        \n",
    "    def train(self):\n",
    "        self._model.fit(self.X_train, self.y_train)\n",
    "        self._y_preds = self._model.predict(self.X_train)\n",
    "        self._y_preds_proba = self._model.predict_proba(self.X_train)\n",
    "        \n",
    "        return self.metrics(self.y_train, self.y_preds, self._y_preds_proba)\n",
    "        \n",
    "    def test(self):\n",
    "        return self.metrics(self.y_test, self._model.predict(self.X_test), self._model.predict_proba(self.X_test))\n",
    "       \n",
    "    @abstractmethod\n",
    "    def metrics(self, y_true = None, y_pred = None, y_preds_proba = None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModeling(Modeling):\n",
    "    def __init__(self, \n",
    "                 data: pd.DataFrame, \n",
    "                 target_name: str, \n",
    "                 shuffle_splitter: BaseShuffleSplit, \n",
    "                 imputer: BaseImputer, \n",
    "                 model: BaseModel, \n",
    "                 scaler = None,\n",
    "                 encoder = None,\n",
    "                 beta: int = 1, \n",
    "                 classification: str = 'binary'):\n",
    "        super().__init__(data, target_name, shuffle_splitter, imputer, model, scaler, encoder)\n",
    "        self.beta = beta\n",
    "        self.classification = classification\n",
    "        \n",
    "    @abstractmethod\n",
    "    def metrics(self, y_true = None, y_pred = None, y_preds_proba=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, TypeVar\n",
    "\n",
    "class TuningClassificationModeling(ClassificationModeling):\n",
    "    TClass = None\n",
    "    all_models = [];\n",
    "    \n",
    "    def __init__(self, \n",
    "             data: pd.DataFrame, \n",
    "             target_name: str, \n",
    "             shuffle_splitter: BaseShuffleSplit, \n",
    "             imputer: BaseImputer, \n",
    "             model: BaseModel, \n",
    "             scaler = None,\n",
    "             encoder = None,\n",
    "             beta: int = 1, \n",
    "             classification: str = 'binary',\n",
    "                 classification_type: str = 'logistic'):\n",
    "        super().__init__(data, target_name, shuffle_splitter, imputer, model, scaler, encoder, beta, classification)\n",
    "        if classification_type == 'logistic':\n",
    "            TClass = TypeVar(\"TClass\", bound=LogisticRegression)\n",
    "        elif classification_type == 'xgb':\n",
    "            TClass = TypeVar(\"TClass\", bound=XGBClassifier)\n",
    "        elif classification_type == 'neural':\n",
    "            TClass = TypeVar(\"TClass\", bound=NNModel)\n",
    "            \n",
    "\n",
    "    def parameter_tuning(self, params, class_to_instantiate: Type[TClass]):\n",
    "        list_of_models = []\n",
    "        combination = []\n",
    "        params_base = {}\n",
    "        output = []\n",
    "        for key, value in params.items():\n",
    "            if isinstance(value, list):\n",
    "                combination.append((key,value))\n",
    "            else:\n",
    "                params_base[key]=value\n",
    "        result = {}\n",
    "        if len(combination) > 0:       \n",
    "            result = TuningClassificationModeling.get_combinations(combination)\n",
    "        print(params_base)\n",
    "        for r in result:\n",
    "            list_of_models.append(class_to_instantiate(**{**params_base, **r}))\n",
    "            \n",
    "        for a_model in list_of_models:\n",
    "            self.model = a_model\n",
    "            startTrain = time.time()\n",
    "            train_metrics = self.train()\n",
    "            endTrain = time.time()\n",
    "            test_metrics = self.test()\n",
    "            endTest = time.time()\n",
    "            train_time = endTrain - startTrain\n",
    "            test_time = endTest - endTrain\n",
    "            output.append({'model': a_model, 'train_metrics': {**train_metrics,**{'elapsed_time':train_time}}, 'test_metrics': {**test_metrics,**{'elapsed_time':test_time}}})\n",
    "        self.all_models = output\n",
    "        return output\n",
    "        \n",
    "    def find_best_model(self, metric):\n",
    "        max_accuracy = self.all_models[0]['test_metrics'][metric]\n",
    "        location = 0\n",
    "        for indx, output_metrics in enumerate(self.all_models):\n",
    "            if max_accuracy < output_metrics['test_metrics'][metric]:\n",
    "                max_accuracy = output_metrics['test_metrics'][metric]\n",
    "                location = indx\n",
    "            elif max_accuracy == output_metrics['test_metrics'][metric]:\n",
    "                if output_metrics['test_metrics']['elapsed_time'] < self.all_models[location]['test_metrics']['elapsed_time']:\n",
    "                    location = indx\n",
    "                \n",
    "        return self.all_models[location]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_combinations(tuples):\n",
    "        length = len(tuples)\n",
    "        if length > 1:\n",
    "            total_params = []\n",
    "            tuple_copy = tuples.copy()\n",
    "            a_tuple = tuple_copy.pop(0)\n",
    "            params_list = TuningClassificationModeling.get_combinations(tuple_copy)\n",
    "            for value in a_tuple[1]:\n",
    "                for a_params in params_list:\n",
    "                    temp = { a_tuple[0]: value}\n",
    "                    total_params.append({**temp, **a_params})\n",
    "            return total_params\n",
    "        else:\n",
    "            params_list = []\n",
    "            a_tuple =  tuples[0]\n",
    "            for value in a_tuple[1]:\n",
    "                temp = {}\n",
    "                temp[a_tuple[0]] = value\n",
    "                params_list.append(temp)\n",
    "            return params_list\n",
    "            \n",
    "    \n",
    "    def metrics(self, y_true = None, y_pred = None, y_pred_proba=None):\n",
    "        if y_true is None and y_pred is None:\n",
    "            y_true = self.y_train\n",
    "            y_pred = self.y_preds\n",
    "        conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        return  {\n",
    "                'matrix': conf_matrix, \n",
    "                'auc': roc_auc_score(y_true, y_pred),\n",
    "                'accuracy': round(accuracy_score(y_true, y_pred), 5), \n",
    "                'precision': precision_score(y_true, y_pred, average=self.classification), \n",
    "                'recall': recall_score(y_true, y_pred, average=self.classification),\n",
    "                'f1': f1_score(y_true, y_pred),\n",
    "                'cost': TuningClassificationModeling.cost_calc(conf_matrix),\n",
    "                'y_pred': y_pred,\n",
    "                'y_pred_proba': y_pred_proba\n",
    "               }\n",
    "    \n",
    "    @staticmethod\n",
    "    def cost_calc(conf_matrix):\n",
    "        cost_matrix = np.array([[0,-100],[-25,0]])\n",
    "        cost = np.sum(cost_matrix*conf_matrix)/np.sum(conf_matrix)\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel:\n",
    "    model = None\n",
    "    epoch = 50\n",
    "    batch_size = 32\n",
    "    loss = 'BinaryCrossentropy',\n",
    "    metric = 'accuracy'\n",
    "    optimizer = 'adam'\n",
    "    \n",
    "    def __init__(self,**inputs):\n",
    "        self.model = tf.keras.Sequential()\n",
    "        for arg, content in inputs.items():\n",
    "            if arg.startswith('input'):\n",
    "                self.model.add( tf.keras.layers.Input( shape=(content,) ) )\n",
    "            if arg.startswith('layer'):\n",
    "                self.model.add( tf.keras.layers.Dense(content['s'], activation = content['activation']) )\n",
    "            if arg == 'epoch':\n",
    "                self.epoch = content\n",
    "            if arg == 'bs':\n",
    "                self.batch_size = content\n",
    "            if arg == 'optimizer':\n",
    "                self.optimizer = content\n",
    "            if arg == 'loss':\n",
    "                self.loss = content\n",
    "            if arg == 'metric':\n",
    "                self.metric = content\n",
    "        self.model.compile(optimizer=self.optimizer, loss=self.loss, metrics=[self.metric])\n",
    "        print(self.model)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y, batch_size=self.batch_size, epochs=self.epoch)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        return pd.Series( (y_pred_proba>0.5).astype(int))\n",
    "        \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        y_pred_proba = self.model.predict(X)\n",
    "        return pd.Series(y_pred_proba.reshape((y_pred_proba.shape[1], y_pred_proba.shape[0]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_cost_proba(train_proba, test_proba, y_train, y_test, conf_train, conf_test):\n",
    "    cost_results = pd.DataFrame()\n",
    "    thresh = 0\n",
    "    for i in range(11):\n",
    "        yhat_train = pd.Series(train_proba < thresh).astype(int)\n",
    "        yhat_test = pd.Series(test_proba < thresh).astype(int)\n",
    "        conf_train = confusion_matrix(y_train, yhat_train)\n",
    "        conf_test = confusion_matrix(y_test, yhat_test)\n",
    "        cost_results = cost_results.append({\"Threshold\": thresh,\n",
    "                                        \"Train Cost\": -TuningClassificationModeling.cost_calc(conf_train),\n",
    "                                        \"Test Cost\":  -TuningClassificationModeling.cost_calc(conf_test),\n",
    "                                        \"conf_train\": conf_train,\n",
    "                                        \"conf_test\": conf_test\n",
    "                                        },\n",
    "                                        ignore_index=True)\n",
    "        thresh = thresh + 0.05\n",
    "    return cost_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iDJqPa8fUNp"
   },
   "source": [
    "Which methods are you proposing to utilize to solve the problem?  Why is this method appropriate given the business objective? How will you determine if your approach is useful (or how will you differentiate which approach is more useful than another)?  More specifically, what evaluation metrics are most useful given that the problem is a binary-classification one (ex., Accuracy, F1-score, Precision, Recall, AUC, etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final metric used for model evaluation was Cost per Prediction.  This was calculated as follows:\n",
    "\n",
    "Cost per Prediction = (- \\\\$100×FP - \\\\$ 25×FN)/(Total # Predictions)  \n",
    "where FP = false positive, FN = false negative.\n",
    "\n",
    "The cost of a false positive (predicting 1 when it is actually 0) is \\\\$100, and the cost of a false negative (predicting 0 when it is actually 1) is \\\\$25.  These costs are normalized by the total number of predictions so the costs can be compared between training and test sets and fairly assessed for any number of future predictions. \n",
    "\n",
    "\n",
    "Before evaluating the model(s) for Cost per Prediction, the models were tuned to maximize ROC Area Under the Curve (AUC).  The ROC (Receiver Operator Characteristic) curve plots the True Positive (TP) rate vs. the False Positive (FP) rate.  The Area Under this Curve typically has a range of 0.5 to 1.0.  A 50:50 random guess for classification would give an AUC = 0.5 with a diagonal line going from the lower left to upper right.  A perfect (ideal) classifier would have an AUC = 1.0 with a line that goes straight up and then straight across. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/olmosjorge28/QTW-SPRING-2022/main/ds7333_case_study_7/visuals/ROC_AUC_curve.png' height=400 width=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC was chosen as a standard metric that was quickly and easily implemented during initial model building and assessment.  AUC was an appropriate metric given that the target classes are fairly balanced (40:60), and AUC is also independent of the prediction threshold which is discussed in the following paragraph.\n",
    "\n",
    "Once the models were assessed for AUC, they were further tuned to minimize Cost per Prediction.  This was done by adjusting the probability threshold for predicting a positive (1) vs. negative (0) class.  The default threshold is 0.5 such that a probability < 0.5 is predicted as a negative class and ≥ 0.5 is predicted as a positive class.  This threshold can be adjusted away from 0.5 such that more positive or negative classes are predicted.  In this way, the number of FPs vs. FNs can be adjusted to minimize the Cost per Prediction.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuRjMsjg8g_d"
   },
   "source": [
    "# Model Building & Evaluations <a id='model-building'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and test sets were created from the data using the stratified splitting method to maintain the ratio of the binary outcome, although the class is relatively balanced between the two outcomes. 30% of the data was withheld for the test set, and the explanatory features were normalized using StandardScaler while avoiding data leakage into the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, logistic regression was run as a baseline model with fast implementation and high interpretability.  This model did not necessarily satisfy the customer requirements of minimizing cost, but it served as a starting point to increase model complexity and improve the model performance.  L1 (Lasso) regularization was used for feature selection with the logistic regression model.         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_modeling = TuningClassificationModeling(loader.get_df(),'y',\n",
    "                                           StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=12343),\n",
    "                                           SimpleImputer(missing_values=np.nan, strategy='mean'), LogisticRegression, None, LabelEncoder(), beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_modeling.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1', 'random_state': 1, 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "logistic_result = logistic_modeling.parameter_tuning( { \n",
    "    'penalty':'l1',\n",
    "    'random_state':1,\n",
    "    'solver': 'liblinear',\n",
    "    'C':  [0.001, 0.01, 1, 10],\n",
    " }, LogisticRegression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Best Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, penalty='l1', random_state=1, solver='liblinear')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_logistic_model = logistic_modeling.find_best_model('auc')\n",
    "best_logistic_model['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 0.6743131085040095,\n",
       " 'cost': -15.369196428571428,\n",
       " 'matrix': array([[55175, 11887],\n",
       "        [21306, 23632]])}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ metric: best_logistic_model['train_metrics'][metric] for metric in ['auc', 'cost', 'matrix'] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 0.6717950589503955,\n",
       " 'cost': -15.60625,\n",
       " 'matrix': array([[23539,  5202],\n",
       "        [ 9156, 10103]])}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ metric: best_logistic_model['test_metrics'][metric] for metric in ['auc', 'cost', 'matrix'] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance \n",
    "\n",
    "###### todo add some writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_tuned = linear_modeling.find_best_model('auc')\n",
    "feat_coef = []\n",
    "feat = zip(linear_modeling.X_train.columns, lr_tuned['model'].coef_[0])\n",
    "[feat_coef.append([i,j]) for i,j in feat]\n",
    "feat_coef = pd.DataFrame(feat_coef, columns = ['feature','coef'])\n",
    "top_feat_lr = feat_coef.loc[abs(feat_coef['coef'])>0].sort_values(by='coef')\n",
    "\n",
    "feat_plot = sns.barplot(data=top_feat_lr, x='feature', y='coef', palette = \"ch:s=.25,rot=-.25\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('LR Feature Importance with L1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tunning Treshold to for Lowest Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best_model_metrics(model):\n",
    "    return (model.find_best_model('auc')['train_metrics']['y_pred_proba'], \n",
    "            model.find_best_model('auc')['test_metrics']['y_pred_proba'], \n",
    "            model.y_train,\n",
    "            model.y_test,\n",
    "            model.find_best_model('auc')['train_metrics']['matrix'], \n",
    "            model.find_best_model('auc')['test_metrics']['matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Train Cost</th>\n",
       "      <th>Test Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>10.030804</td>\n",
       "      <td>10.030729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>9.952902</td>\n",
       "      <td>9.942708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>9.811161</td>\n",
       "      <td>9.811458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.15</td>\n",
       "      <td>9.767411</td>\n",
       "      <td>9.736979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.20</td>\n",
       "      <td>9.866741</td>\n",
       "      <td>9.868750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.25</td>\n",
       "      <td>10.257143</td>\n",
       "      <td>10.231250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.30</td>\n",
       "      <td>10.836384</td>\n",
       "      <td>10.835417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.35</td>\n",
       "      <td>11.559375</td>\n",
       "      <td>11.723437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.40</td>\n",
       "      <td>12.572098</td>\n",
       "      <td>12.766146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.45</td>\n",
       "      <td>13.752455</td>\n",
       "      <td>14.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.50</td>\n",
       "      <td>15.369196</td>\n",
       "      <td>15.606250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Threshold  Train Cost  Test Cost\n",
       "0        0.00   10.030804  10.030729\n",
       "1        0.05    9.952902   9.942708\n",
       "2        0.10    9.811161   9.811458\n",
       "3        0.15    9.767411   9.736979\n",
       "4        0.20    9.866741   9.868750\n",
       "5        0.25   10.257143  10.231250\n",
       "6        0.30   10.836384  10.835417\n",
       "7        0.35   11.559375  11.723437\n",
       "8        0.40   12.572098  12.766146\n",
       "9        0.45   13.752455  14.007812\n",
       "10       0.50   15.369196  15.606250"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_proba, test_proba, y_train, y_test, conf_train, conf_test = extract_best_model_metrics(logistic_modeling)\n",
    "logistic_cost_results = tune_cost_proba(train_proba[:,0], test_proba[:,0], y_train, y_test, conf_train, conf_test)\n",
    "logistic_cost_results[['Threshold', 'Train Cost','Test Cost' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_tunning(cost_results, threshold):\n",
    "    sns.lineplot(data=cost_results, x='Threshold', y='Train Cost', color='blue')\n",
    "    sns.lineplot(data=cost_results, x='Threshold', y='Test Cost', color='red')\n",
    "    plt.title('Tuning Threshold for Logistic Regression with L1')\n",
    "    plt.legend(['Train', 'Test'])\n",
    "    plt.axvline(threshold, color='black', ls='--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5RUlEQVR4nO3dd3hUZfbA8e9JCL1DkC6oAbuoUdeyiqv+VIQVK7qiYsOyiqgIIoKoKKggWBYBFbEtZXEtIK6KUlSwgAKCCkE6BBJC7ynn98d7E4eQMklm5mYy5/M882Ry5869581Mzrzz3veeK6qKMcaY2BHndwDGGGMiyxK/McbEGEv8xhgTYyzxG2NMjLHEb4wxMcYSvzHGxBhL/CEmIi1FZJeIxEd4v+1FZF0E9tNKRFREKpXiuUXGKCLjRGRQEY8PEpHNIrKxpPuOFBH5VERuLsXz/ioiS8MRU3kmIktEpH153b+IzBSR2yMXUWTEdOL3EnTuLUdE9gb8fkNptqmqa1S1pqpmhzjWTwNiyxSRAwG/jwrlvsojEWkBPAQcq6qNQ7RNFZGjQrGtXKp6qaq+VdJ9q+rXqtq2pPsTkYHe+2GXiGwTkTkicmZJt+MXVT1OVWeWh/17f8t3S7utojo2InK+iMwQke0isqq0+wiVmE78XoKuqao1gTVAp4Bl7/kdXyAvoeTG+h7wXECsd5V0e5H+RhIChwMZqppW0ieW5ttJlJnovS8aAjOA/4R6B+LEdL4oo93AWOBhvwOBGE/8hcn/yZ9/eMP7+veUiHwrIjtF5HMRaVjSdb3HbxKR1SKSISL9RWSViFxYhtgfEpE0EUkVkVsClo8TkVdFZJqI7AbOF5GmIvK+iKSLyEoR6RGw/ukiMk9EdojIJhF5Id+ubhCRNd7QS7+A51URkREissG7jRCRKoXEerKI/OT9XSYCVQtZ70LgC6Cp17Md5y3/u/dVfZv3dz4m4DmrRKSPiCwCdpck+YtIHRF52/u7rBaRx3KTnojEi8gwr90rReTeAl7v2737R4nILK+Xt9lrIyIy29vVQq89XfL3FkWkhYj814shQ0ReKS5uVc3CdQqaiUhiQFve8N4P68UNl8WXoC1Pi8i3wB7gCBE5WkS+EJEtIrJURK4NiLmDiPzqvZ7rRaSXt7yhiEz1XqctIvJ1wN8z7/1e1Hsn9+9T2Ps73+t3voj8EvD7dBH5IeD3b0Skc+D+ReQS4FGgi/eaLAzY5OFSyP9vsFT1B1V9B1hR0ueGgyX+0vsHcAvQCKgM9CrpuiJyLDASuAFoAtQBmpUhpsYB27gN+JeI1MsXx9NALWAOMAVY6K1/AdBTRC721n0ReFFVawNHApPy7escoK33vAEBSbcf8BegHXAScDrwWP5ARaQy8CHwDlAf10u9qqBGqep04FJgg/cNp5uItAHGAz2BRGAaMMXbbq7rgcuAul5SDNbLuL/jEcB5wE241w/gDi+WdsApQOcitvMU8DlQD2jubRdVPdd7/CSvPRMDn+Ql5qnAaqAV7vWZUFzQXttvAjKArd7it4As4CjgZOD/gNwx62DaciPQHfeeScd9AP8b916+HhgpIsd5674B3KmqtYDjga+85Q8B63Cv02G4BFtQrZji3jvFvb9zzQWO8j5wKnmxNBeRWiJSDTgV+DrwCar6P+AZvG9PqnpSwMMl+V+PCpb4S+9NVV2mqntxSbFdKda9Gpiiqt+o6gFgAAX/QwQrE3hSVTNVdRqwC5ecc32kqt+qag5wApCoqk+q6gFVXQG8BlwXsK2jRKShqu5S1e/y7esJVd2rqgtxHx65/yg3eDGkqWo68AQueeT3FyABGOHFOxn4sQRt7QJ8oqpfqGomMBSoBpwVsM5LqrrW+7sHxUu6XYC+qrpTVVcBwwLacC3uA3Gdqm4FhhSxuUzcEFVTVd2nqt8EGcbpQFPgYVXdHcRzrxWRbcBeXDK/WlWzROQwXGLv6W0nDRjOn69xMG0Zp6pLvA/OS4BVqvqmqmap6k/A+7j3cW57jxWR2qq61Xs8d3kT4HDvtf5aCy4SVtx7p7j3NwCqug+YB5wLJAOLgG+As3HvuxRVzSji75lfSf7Xo4Il/tILnFmyB6hZinWbAmtzH1DVPbjeWmll5OvZ5o9rbcD9w3FDJ9tyb7ie2GHe47cBbYDfReRHEemYb19FtWl1wGOrvWX5NQXW50sAqwtYrzAH7cf7MFvLwd+Y1uZ/UhAa4np1+duQu92DXrNi9tEbEOAHcUNStwYZQwtgdQm+pUxS1bq4124xrkcL7jVOAFIDXuPRuJ4rBNeW/O+ZM/K9Z27A9cTBfWPrAKz2hrhyDzI/DywHPheRFSLySCHtKO69U9z7O9AsoD0u+c8CZuK+vZ3n/V4SJflfjwoV/aBXae0Gqgf8HpJZJAVIJaDH4n0NbRCmfcHB3ybWAitVNanAFVVTgOu9sdgrgckiEkxsG3AJYon3e0tvWX6puLFoCUj+LYE/gthH7n5OyP1FRASXMNcHNiPIbQXazJ899V8D4srdbipu2CZXi8I2pKobcT1wROQcYLqIzFbV5cXEsBZoKSKVSjJEpaqbReRO4EcR+be3nf1Aw0K2E0xb8r9nZqnqRYXs/0fgchFJAO7F9Y5bqOpO3HDPQ96w0AwR+VFVv8y3iWDfO8GYhfumtgb3TWYr7hvtfuBfhTwnZkoVW4+/YAuAc8XNya8D9A3TfiYDnUTkLG989glcDzESfgB2iDsAWs070He8iJwGICJdRSTR60lv854TzBTV8cBjIpLoHQQbABQ0RW4ubuy5h4hUEpErcUMcwZoEXCYiF3iJ5iHcP/WcEmwDoLKIVM29BWz7aW9M+HDgwYA2TALuF5FmIlIX6FPYhkXkGhHJTaxbcYkl92+4CXcMoSA/4JLyEBGp4cV2djCNUdXfgc+A3qqaijvGMExEaotInIgcKSLnlbQtnqlAGxG5UUQSvNtpInKMiFQWkRtEpI439LYjt60i0lHcgW4JWF7QeynY904w5uA6VacDP6jqErxvLMDsQp6zCWglZZy9FPh+8m7i/e2r4r6Bibe8cnHbChdL/AVQ1S+Aibixwfm4N3w49rMEuA934C4V2Amk4RJYWKk7z6ATbrxyJa6n+zru4Bm48dwlIrILd6D3Om/stDiDcOOri4BfgJ+8Zfn3fwD3TaIbLil2Af5bgviXAl1xB0w3e23p5G23JJbgxsZzb7fgXpPduBkY3+AOZo711n8Nl0wXAT/jDipnUXAiOw343vsbfgzcr6orvccGAm95QybXBj4p4LU5CtdjXYf7+wTreaC7iDTCHeytjPv2shXX2WhSirbg9dz/D3eMYANuCORZIHfW1o3AKhHZAdyFe30AkoDpuDH5ucBILXjuflDvnWCo6m7v+UsC3hNzcUNohU0Jzp0GmyEiPxWyTnGacfD7aS9ucsS53v1puG8ye3F/e19IwcdYjB9EpCaud50UkCBMOSYilwKjVPVwv2Mpq4rUFlM06/H7TEQ6iUh1EamBm5nyC7DK36hMYbxhsQ7e8FQz4HHgA7/jKo2K1BZTMpb4/Xc57mvzBtxX4usKmepmygfBHYvZihse+Q03Fh2NKlJbTAnYUI8xxsQY6/EbY0yMiYp5/A0bNtRWrVr5HYYpwtKlrqJw27YlLjBpjAmT+fPnb1bVxPzLoyLxt2rVinnz5vkdhilC+/btAZg5c6avcRhj/iQiBZ4Nb0M9xhgTY6Kix2/Kv8ceO6QApzGmnLLEb0LiwgtLfQkBY0yERW3iz8zMZN26dezbF0wVgehWtWpVmjdvTkJCgt+hFGrBggUAtGvXztc4jDHFi9rEv27dOmrVqkWrVq1wtZ8qJlUlIyODdevW0bp1a7/DKVTPnj0BO7hrTDSI2oO7+/bto0GDBhU66QOICA0aNIiJbzbGmMiI2sQPVPiknytW2mmMiYyoTvzGGFNh7d0L998P69aFfNOW+EspIyODdu3a0a5dOxo3bkyzZs3yfj9woOiS8PPmzaNHjx4RitQYE5WGD4eXXoLlxV2wreSi9uCu3xo0aJA3k2XgwIHUrFmTXr165T2elZVFpUoF/3mTk5NJTk6ORJgR88wzz/gdgjEVx8aNMHgwdO4M3lnxoWSJP4S6detG/fr1+fnnnznllFPo0qULPXv2ZO/evVSrVo0333yTtm3bMnPmTIYOHcrUqVMZOHAga9asYcWKFaxZs4aePXtG5beBs846y+8QjKk4+veH/fvhuefCsvkKkfh79gSv8x0y7drBiBElf96yZcuYPn068fHx7Nixg9mzZ1OpUiWmT5/Oo48+yvvvv3/Ic37//XdmzJjBzp07adu2LXfffXe5nrNfkDlz3KVu7QPAmDJauBDeeMMltqSksOyiQiT+8uSaa64hPj4egO3bt3PzzTeTkpKCiJCZmVngcy677DKqVKlClSpVaNSoEZs2baJ58+YFrltePfroo4DN4zemTFThwQehfn3X6w+TCpH4S9MzD5caNWrk3e/fvz/nn38+H3zwAatWrcqrYJlflSpV8u7Hx8eTlZUV7jCNMeXR1Knw1Vfw8stQr17YdmOzesJo+/btNGvWDIBx48b5G4wxpnw7cAB69YKjj4Y77wzrrizxh1Hv3r3p27cvZ599NtnZ2X6HY4wpz159FZYtg6FDISGBPXvgn/+E1QVW1C+bqLjmbnJysua/EMtvv/3GMccc41NEkVfe22sXYjGmDLZsgaOOguRk+OwzEGHQIDfMP3s2/PWvpdusiMxX1UPmjleIMX7jvxHl6UCLMdHmySdh+3Z44QUQYeNGGDIErryy9Em/KJb4TUhYOWZjSmnpUvjXv+COO+D44wEYMMAN+T/7bHh2aWP8JiSmT5/O9OnT/Q7DmOjz8MNQrRo88QQAixe7afz//Kcb/QkH6/GbkBg0aBBgV+IypkS+/BKmTHHjOocdBriJPXXqhHUavyV+Y4zxRXa2O1mrVStXhRN3XPezz9xQf/364du1JX5jjPHDm2/CokUwcSJUrUpWFjz0EBx5pBvmCSdL/KWUkZHBBRdcAMDGjRuJj48nMTERgB9++IHKlSsX+fyZM2dSuXJlq21jTCzauRMeewzOPhuuuQaAsWNhyRKYPBmKSR9lZom/lIory1ycmTNnUrNmTUv8xsSiwYNh0yY3vi/Czp1uTP+cc9wUznCzWT0hNH/+fM477zxOPfVULr74YlJTUwF46aWXOPbYYznxxBO57rrrWLVqFaNGjWL48OG0a9eOr7/+2ufIy2706NGMHj3a7zCMKf9Wr3aD+F27wmmnAW7aZloaDBsGkbjSath6/CIyFugIpKnq8d6ygcAdQLq32qOqOq3MOysHdZlVlfvuu4+PPvqIxMREJk6cSL9+/Rg7dixDhgxh5cqVVKlShW3btlG3bl3uuuuuEn9LKM/atm3rdwjGRIdHHoG4OPAuXrR2rUv4118Pp58emRDCOdQzDngFeDvf8uGqOjSM+/XF/v37Wbx4MRdddBEA2dnZNGnSBIATTzyRG264gc6dO9O5c2cfowyfKVOmANCpUyefIzGmHJs7FyZMcOM6LVoA0K+fq8Y8eHDkwghb4lfV2SLSKlzbP0g5KBegqhx33HHMnTv3kMc++eQTZs+ezccff8xTTz3FkiVLfIgwvIYNGwZY4jemUDk58MAD0KQJ9O4NwPz58M477kvA4YdHLhQ/xvjvFZFFIjJWRAotOC0i3UVknojMS09PL2y1cqNKlSqkp6fnJf7MzEyWLFlCTk4Oa9eu5fzzz+e5555j27Zt7Nq1i1q1arFz506fozbGRMzEifD9926Ip2ZNVN30zcRE6Ns3sqFEOvG/ChwJtANSgWGFraiqY1Q1WVWTc6dJlmdxcXFMnjyZPn36cNJJJ9GuXTvmzJlDdnY2Xbt25YQTTuDkk0/mgQceoG7dunTq1IkPPvigwhzcNcYUYe9e6NMHTj4ZbroJgI8/hlmzXKWG2rUjG05Ep3Oq6qbc+yLyGjA1kvsPl4EDB+bdnz179iGPf/PNN4csa9OmDYsWLQpnWMaY8uKFF9xR3Hfegbg4MjPdaM8xx7jabJEW0cQvIk1UNdX79QpgcST3b4wxEZea6o7cXnEFnHceAKNGuWuuTJ0KlXw4myqc0znHA+2BhiKyDngcaC8i7QAFVgHhvb6YiZh33nnH7xCMKZ/693c1lp97DoBt29zwzgUXQIcO/oQUzlk91xew+I0Q7wOJxNkOPouGq6S18KamGWMCLFjgajE88EBejeWnn3YX3IrUyVoFidozd6tWrUpGRkZUJMWyUFUyMjKoWrWq36EUaeLEiUycONHvMIwpP1Rd9c369fNqLK9YAS+9BN26wUkn+Rda1Nbqad68OevWrSMapnqWVdWqVWnevLnfYRTp1VdfBaBLly4+R2JMOTFlCsyYAa+8AnXrAm6+fqVK4F2+wjdRm/gTEhJo3bq132EYY8yhDhxwV1Q55hi40x3KnDMH/vMfePxxaNrU3/CiNvEbY0y5NXIkpKTAJ59ApUp5J2s1aeKutOg3S/zGGBNKGRlu2s7//R9ceikAkybBd9+547w1avgcH1F8cNcYY8qlJ5+EHTvypu3s2+fG9k86Ke+kXd9Zj9+ExOTJk/0OwRj/LV3qhnm6d4fjjwfg5Zdh1SqYPh3i4/0NL5clfhMSDRs29DsEY/zXqxdUr+6GeoDNm928/csucydslReW+E1IjBs3DoBu3br5Gocxvpk+3dVgePZZaNQIcPl/1y54/nmfY8tHouEEqOTkZJ03b57fYZgitG/fHnDXEjYm5mRnu8qbu3bBr79C1aosXepGe+64w43++EFE5qtqcv7l1uM3xpiyGjsWfvnFTd/xzrLv3RuqVYOA4r3lhiV+Y4wpix074LHH4Jxz4OqrAXfC7scfu6Kc3qhPuWKJ3xhjymLwYEhLc+P7IuTkuJO1WraEnj39Dq5glviNMaa0Vq2C4cPhxhvhtNMAd62Vn3+G997LG/Upd+zgrgmJPXv2AFC9enWfIzEmgq67zo3pLFsGzZuzZw+0aeNq8Xz3HcT5fIqsHdw1YWUJ38ScOXPcBdQHDACveu6wYbB+PUyY4H/SL0o5Ds1Ek5EjRzLSrzlrxkRaTo67uErTpm76Du4Ki88+C1dd5Y7zlmeW+E1ITJo0iUmTJvkdhjGRMWEC/PADPPNMXtW1AQNcNeZnn/U5tiBY4jfGmJLYs8dVXTvlFHdQFzeFf+xYuPdeOPJIn+MLgo3xG2NMSbzwAqxdC+++mzeQ36sX1KnjpvNHA0v8xhgTrA0bYMgQuPJKOPdcAP73P/j8czers359n+MLkg31GGNMsPr3dwP5zz0HQFaW6+0fdRTcc4/PsZWA9fhNSFhxNlPh/fwzvPkmPPhg3kD+2LGwZAm8/z5UruxzfCVgJ3AZY0xxVOFvf4PFi921dOvWZedO19Nv2xZmzQIRv4M8VGEncIVtqEdExopImogsLuCxXiKiImJX76gghg4dytChQ/0Ow5jw+OgjmDnTFdivWxdwQ/1paXlXWIwq4RzjHwdckn+hiLQALgLWhHHfJsKmTp3K1KlT/Q7DmNA7cAAefhiOOcZdUhE3qeeFF+Af/8gr0RNVwpb4VXU2sKWAh4YDvYHyP8ZkjDEvvwzLl7uufSV3WPTRR91Dgwf7GFcZRHRWj4j8HVivqguDWLe7iMwTkXnp6ekRiM4YY/L55Rfo1w86doRLLwVg3jw3hf+BB1zp5WgUscQvItWBfsCAYNZX1TGqmqyqyYmJieENzhhj8tuzx1XfrFsX3ngDcMd4H3rIXVzlkUf8Da8sIjmd80igNbBQ3JGQ5sBPInK6qm6MYBwmDKpVq+Z3CMaE1gMPuOvnfv553mW0PvoIZs+GV1+F2rV9jq8MIpb4VfUXIO8iZCKyCkhW1c2RisGEz6effup3CMaEzuTJMGYM9OkDF10EuGO8vXvDscfC7bf7HF8ZhXM653hgLtBWRNaJyG3h2pcxxoTM6tVwxx1w+unw1FN5i0eNclP4n38+7xhv1Apb+Kp6fTGPtwrXvk3kPeX9g/Tv39/nSIwpg6wsuOEGyM6G8eMhIQGArVvdFP4LL8w7xhvVrFaPCYkvv/ySL7/80u8wjCmbJ5+Eb7+F0aPhiCPyFj/9tEv+Q4dG38laBbHEb4wx4M7MHTQIunWD6/8csFixwk3lv+UWOOkk36ILKUv8xhiTkQFdu0JSksvynpwc+Oc/3Zh+wHB/1IvyQxTGGFNGqnDrrZCeDlOmQM2aeQ8NGeLq7Y8c6S6vW1FY4jch0aBBA79DMKZ0Ro6Ejz92V1I5+eS8xV995crv/+MfcNddPsYXBlaW2RgTuxYuhDPOgAsugKlT847cbtjgPgMaNHDXVA/4EhBVCivLbD1+Y0xs2r3blWSoVw/GjctL+pmZ0KWLe3jmzOhN+kWxxG9Com/fvgAMjtZyhSb2PPAALF0KX3wBAfXA+vWDb76B995zlZgrIkv8JiTmzp3rdwjGBO8//4HXXnOV1i64IG/xhx+6M3PvuceN7VdUNp3TGBNbVq1yJRnOOMOdsOX54w83hT852V1kpSKzxG+MiR1ZWa4rr3pQSYa9e+HqqyEuzn0ZqFLF5zjDzIZ6jDGxY+BAmDvXJf3WrfMW338/LFjgJva0auVXcJFjid+ERPPmzf0OwZiizZgBzzzjTta67rq8xW+95Yb7H30ULrvMx/giyObxG2Mqvs2bXaGdWrVg/nyoUQOARYvgL39xt88/j/5yy/nZPH5jTGzKLcmweTN88kle0t+xw43r160L//53xUv6RYmhpppw6tmzJwAjRozwNQ5jDvHKK64Gz4svQrt2gPssuO02V3nzq6+gcWN/Q4y0YhO/iHypqhcUt8zEtgULFvgdgjGHWrgQevVyg/f33Ze3+KWX3NUVn3sOzj3Xx/h8UmjiF5GqQHWgoYjUA3IvP1AbqEB16owxFdLu3a72QoMG8OabeSUZ5s51nwWXX+5+xqKievx3Aj1xSX4+fyb+HcC/whuWMcaU0f33w7JlMH16XkmG9HS49lpo0eKg8jwxp9DEr6ovAi+KyH2q+nJh6xljTLkzcSK88Yabo/m3vwHuMrpdu7rkP2eOO6gbq4I5uLtRRGqp6k4ReQw4BRikqj+FOTYTRdq0aeN3CMY4K1dC9+5ujubAgXmLBw1yUzbHjIFTTvEvvPKg2Hn8IrJIVU8UkXOAwcBQ4FFVPSMSAYLN4zfGBCkz0x2t/fVXd2DXOw3388/hkkvgxhtja4insHn8wdTqyfZ+Xga8qqofAZVDGZwxxoTEwIHw3XeuW+8l/bVrXXme445zF9uKlaRflGAS/3oRGQ1cC0wTkSpBPs/EkO7du9O9e3e/wzCx7KuvYPBgN0G/SxcADhxwd/fvd9M3vXO3Yl4wY/zXApcAQ1V1m4g0AR4Ob1gm2ixbtszvEEwsS093R27btnUnann69HHTNydOdA8Zp9ieu6ruAf4ALhaRe4FGqvp5cc8TkbEikiYiiwOWPSUii0RkgYh8LiJ2PoAxpmxU4ZZbICMDJkzI69ZPngwjRkCPHm4Kp/lTsYlfRO4H3gMaebd3ReS+op8FwDjcN4VAz6vqiaraDpgKDChRtMYYk99LL7kaPEOHukJsuOn7t97qrrXy/PM+x1cOBTPUcxtwhqruBhCRZ4G5QJFz+1V1toi0yrdsR8CvNYDyXxrUGFN+/fwz9O4NnTrBvfcCsGePK75WuTJMmuR+moMFk/iFP2f24N0v9XFxEXkauAnYDpxfxHrdge4ALVu2LO3uTIS084pfGRMxu3a5uvoNG8LYsSCCqrte7uLFMG0aWOooWDCJ/03gexH5wPu9M/BGaXeoqv2AfiLSF7gXeLyQ9cYAY8DN4y/t/kxkWFVOE3E9ekBKCnz5pUv+uPz/1lswYICbt28KFszB3ReAW4AtwFbgFlUdEYJ9/xu4KgTbMcbEmvHjXeG1fv3gfDdwsGAB/POfcOGFLvGbwhVVnfM0oKGqfuqVZ/jJW/53EYlT1fkl3ZmIJKlqivfr34HfSxO0KX+6du0KwLvvvutzJKbCW7EC7roLzjwTHncDBtu2uXH9hg3dRVXi4/0NsbwraqjneaBbAct/xQ3B/K2oDYvIeKA9rqzzOtyQTgcRaQvkAKuBu0oesimP1q1b53cIJhZkZrrTcEXyLpuVO5tz9WqYOTOvEKcpQlGJv4Gqrsq/UFWXi0iD4jasqtcXsLjUxwaMMYYBA+D77910Ha8kwwsvwIcfup9nn+1rdFGjqDH+akU8Zic+G2Mia/p0ePZZuOMOuOYaAL75xp2de+WV4F390wShqMQ/XUSeFjm4pJGIPAF8Fd6wjDEmwPr1rrTm0Ue703GBTZvcGbmtW+fN5jRBKmqo5yHgdWC5iCzwlp0EzANuD3NcJsqceeaZfodgKqqdO901c3fvhi++gOrVyc52Q/1bt8Knn0KdOn4HGV2KugLXbuB6ETkCOM5bvERVV0QkMhNVBg8e7HcIpiLKynLd+sWLXVmG448HXPXlr75yPX2vSoMpgWJP4PISvSV7Y0xkqbqJ+f/7n6uvf/HFgOvhDxrkavHccovPMUYpq6tvQuKqq67iqqvsfDwTQs8/7xL+I4+4A7q4KZtdu8KJJ8Irr/gcXxQLpmSDMcXKyMjwOwRTkUya5KbrdOkCTz8NuIupXHutG/2ZPBmqFTXv0BQpqMQvIvHAYYHrq+qacAVljIlhc+bATTe5SfnjxkGcG5jo1Qt++AHefx+SkvwNMdoVm/i92vuPA5twZ9yCK6d8YhjjMsbEouXL4e9/d2U1P/wQqlYF4NVX3dDOAw+4OfumbILp8d8PtFVV+y5vjAmfzZvh0kvd/WnT8ipufvCBO8bbsSM895yP8VUgwST+tbja+cYU6oILLvA7BBPN9u2Dzp1h7Vo3T/OoowD49ls3X//0091VFSvZUcmQCObPuAKYKSKfAPtzF3rlmo0BoH///n6HYKJVTg506+ay/KRJcNZZAPz2m7uwVosWMHVq3qV0TQgEk/jXeLfK3s0YY0KnXz+YONGN43g1eDZscBdSqVwZPvssb9THhEgwJ3A9EYlATHS71Bub/fTTT32OxESV116DIUPgzjvdtB1g+3Y31L9lC8ya5WrxmNAq6kIsI1S1p4hMoYCLoqvq38MamYkqe/fu9TsEE20++wzuvtt17V95BUTYvx+uuAJ+/dUd3z3lFL+DrJiK6vG/4/0cGolAjDExZOFCN6xz/PFuXL9Spbyh/hkz4O234aKL/A6y4iqqSNt87+esyIVjjKnw1q931TZr13aF12rVAqB3bzdzZ8gQV4HZhE8wJ3AlAYOBY4GquctV9YgwxmWMqYhySyzv2OGuotKsGQDDh8OwYXDffe4DwIRXMLN63sSduTscOB+4BbBLHpiDdOzY0e8QTHmXv8Tyie7k/wkT4MEH3cXShw+3C6pEgqgectz24BVE5qvqqSLyi6qe4C37WlX/GpEIgeTkZJ03b16kdmeMCTVVuOsuV21zzJi8apszZrhju2ecAZ9/nlehwYSIl7+T8y8Ppse/T0TigBQRuRdYDzQKdYDGmAost8Ry3755SX/RIneyblISfPSRJf1ICqYef0+gOtADOBXoCtwcxphMFGrfvj3t27f3OwxTHgWWWB40CIA1a9xc/Vq13IVV6tXzOcYYU2SP3yvHfK2qPgzswo3vG2NMcAoosbxlixve2b0bvv7alWQwkVXUCVyVVDVLRE4VEdHiDgYYY0ygAkos793rFv3xhzt/64QT/A4yNhXV4/8BOAX4GfhIRP4D7M59UFX/G+bYjDHRqoASy9nZcMMN7kvAhAlgI4P+Cebgbn0gA/gbrnSDeD+LTPwiMhboCKSp6vHesueBTsAB4A/gFlXdVtrgjTHlUAElllWhRw9XW3/ECDer0/inqMTfSEQeBBbzZ8LPFcywzzjgFeDtgGVfAH29IaRngb5AnxJFbMqla+0/2UChJZaHDIGRI+Hhh+H++/0N0RSd+OOBmhR8slaxiV9VZ4tIq3zLPg/49Tvg6iBiNFHgnnvu8TsEUx4UUGL5rbfg0UfdMM+QIT7HZ4CiE3+qqj4Zxn3fCkws7EER6Q50B2jZsmUYwzChsGfPHgCqV6/ucyTGN7kllu+6K6/E8v/+B7fdBhdcAGPH5l033fisqJchbCdOi0g/IAt4r7B1VHWMqiaranJiYmK4QjEh0qFDBzp06OB3GMYvuSWWL70UXn4ZRJg3z5VhOOEE+O9/3UVVTPlQVI8/LBdRFZGbcQd9L7AposZUAIEllidOhEqV+OMPV4utYUM3qad2bb+DNIGKKsu8JdQ7E5FLcAdzz1PVPaHevjEmwgoosZyW5k7QyspyXwSaNPE7SJNf2K5ZLyLjgfZAQxFZh6vw2ReoAnwhrgTfd6p6V7hiMMaEUQEllnfvho4dYd06N5OzbVu/gzQFCVviV9XrC1j8Rrj2Z4yJoAJKLGdmukXz57sx/TPP9DtIU5iwJX4TW7p16+Z3CCZSVOGf/3RTdsaMgYsvzqu6PG0ajBoFl1/ud5CmKJb4TUhY4o8hBZRYHjjQTdfs3x/uvNPf8EzxbFatCYnNmzezefNmv8Mw4fbvfx9SYnn0aHjySbj1VnjiCZ/jM0GxHr8Jiauvdidhz5w5099ATPi8+y7cfLOrruaVWP74Y7jnHujQwQ3x2GUTo4P1+I0xxRs3ztXVb98epk6FqlWZOxeuuw5OPdWV5UlI8DtIEyxL/MaYor3+uhvHufBCmDIFatRg6VI3bbNZM/c5UKOG30GakrDEb4wp3KhR7gDuxRfDxx9D9eqkprpfK1VyE3sa2RW4o46N8RtjCvbKK3Dffa5rP3kyVKnCjh1uPH/zZpg5E4480u8gTWlY4jchcffdd/sdggmlESPggQfchPxJk6ByZVauhCuugCVL3IhPcrLfQZrSssRvQqJLly5+h2BCZehQd8WUq66C8eMhIYHp090MzuxsN+JzySV+B2nKwsb4TUisXbuWtWvX+h2GKavBg13S79IFxo9HKyUwbJgb02/cGH788c9L6ZroZT1+ExI33ngjYPP4o9pTT8GAAfCPf8Bbb7HnQCVuv9l1+q+80s3orFXL7yBNKFiP35hYpwqPP+6S/k03wdtvs3JtJc46CyZMgKefdsd2LelXHNbjNyaWqbrr5A4e7ObqjxnD9BnxeeP5U6e6WTymYrEevzGxStXV3Rk8GLp3R8e8xrAR8QeN51vSr5isx29MLFKFBx900zbvuYc9z77M7TfG2Xh+jLDEb0LioYce8jsEEyxV6NHDnaB1//2s7DGcK84RFi1y4/l9+1qxtYrOEr8JiU6dOvkdgglGTo67iMqoUfDQQ0y/+Hm6nCY2nh9jbIzfhMTSpUtZunSp32GYouTkuKukjBqF9u7DsMbPc/ElYuP5Mch6/CYk7vQuu2Tz+Mup7Gy4/XYYN47MPo9x8+onGf+c2Hh+jLLEb0xFl50N3brBu++y9f6BnP+/x208P8ZZ4jemIsvKghtvhAkTWN5tEGe808/G842N8RtTYWVmuvILEyYwu8MQ2r7dz8bzDWCJ35iK6cABV2jtP//h3ZOHcd60PnTuDN99B0lJfgdn/Ba2oR4RGQt0BNJU9Xhv2TXAQOAY4HRVnReu/ZvIeuyxx/wOweTavx+uuQamTOHZpi/Sd0EPG883BwnnGP844BXg7YBli4ErgdFh3K/xwYUXXuh3CAZg3z5XR3/aNB6u/i9e232PjeebQ4Qt8avqbBFplW/ZbwBi3Y4KZ8GCBQC0a9fO1zhi2t696BVXIJ99xl0ymq9bdefHD21oxxyq3M7qEZHuQHeAli1b+hyNKU7Pnj0Bm8fvmz17yO54OTLjS27jDbZfcSvfjbP5+aZg5fbgrqqOUdVkVU1OTEz0Oxxjyq/du9l7wWXIjC+5hXEc9fStVj/fFKnc9viNMUHYuZOtZ19G7V++pXu1d+ky+R82nm+KZYnfmCil23ewod2lHLbqe3o3+zd9ZnSx8XwTlLAN9YjIeGAu0FZE1onIbSJyhYisA84EPhGRz8K1f2Mqsj0btrH8yP+j0aofGH7GRAb+ZknfBC+cs3quL+ShD8K1T+OfZ555xu8QYsbi938nvuv1HLlvCR/eMJle71xu8/NNidhQjwmJs846y+8QKrw9O7P5quMLXDi7P3ukBoue+JBrBtiAvik5S/wmJObMmQPYB0C4zH3zd6rcfQsd93/Hz4d35ojPXiW5bWO/wzJRyhK/CYlHH30UsHn8obYlPZsvOgzn8nmPsS+uOr/2e4+Tn7reai+YMrHEb0w5pAqfvriMxIe70SVrLr8mXU7rz0ZxbGvr5Zuys8RvTDmzbnU2n3V4kX/82o8D8dVY/fQ7HNv3Buvlm5CxxG9MOZGTA+OfWMYRT9/Kbdnf8sexnTj8f6Op06KJ36GZCsYSvzHlwG+Ls/mi00vcsepRsipVJW3o2xz5YFfr5ZuwsMRvQmLEiBF+hxCVDhyAMb2X0+6lW+ih37D2pI40/2Q0tZo19Ts0U4FZ4jchYeWYS+67OTnMvOplemzsS05CFbYPe4sW995ovXwTduW2OqeJLtOnT2f69Ol+hxEVdu6Ep25ezoGz2/PIxp7sTP4bNVcups59N1nSNxFhPX4TEoMGDQLsSlzF+fSTHL678RX6bH0EKldm74tvctidN1vCNxFlPX5jIiA9HR74+x9U73g+T2y9n/1/aU/1PxZT7a5ulvRNxFmP35gwUoV3385hyT3/YtCeR4ivUonMF9+gXvdbLOEb31iP35gwWbUKup27ghbd/saQPT3Qs/9K1ZTFJNx5qyV94yvr8RsTYtnZ8PKLOax+5FX+ldmHhKrx5Lz0OjVvt4RvygdL/CYkRo8e7XcI5cKiRTDgxpX0WHQbPZnB3nP/jyrvvg4tWvgdmjF5LPGbkGjbtq3fIfhq3z54+qkctg4ZzXs5D1O5Whz64mtUu/026+WbcsfG+E1ITJkyhSlTpvgdhi9mz4YOx66i/TMX8UrOPSS0P4uE3xcjd9xuSd+US9bjNyExbNgwADp16uRzJJGTlgaPD1B09Gg+loepWg0YMZrKd9xhCd+Ua5b4jSmhn3+GF1+E39+bz6CsR7iQ6WS3v5D4N1+Hww/3OzxjimWJ35ggZGXBhx/CmGE7afXdeO6PG83JOT+RU70mvDCK+O7drZdvooYlfmOKsGULvPYazBr+E5dvGs1/5d/UZBfZR58A97xCXNeuUKeO32EaUyKW+I0pwOLFMHrYLnLeG0+3zDH0YR5ZlasRd30XuOtO4s84w3r4JmpZ4jch8c477/gdQpllZ8Mnn8Ang36m3Y9jeJr3qM1O9h11PNz/MpW6doW6df0O05gys8RvQqJFFJ+gtH07vP3qbtYPm8CVm0czmh/JrFSV7Ku7wH3dqXrmmda7NxVK2BK/iIwFOgJpqnq8t6w+MBFoBawCrlXVreGKwUTOxIkTAejSpYvPkQRv6VL47+MLafj+aG7Oepfa7GR782PJfvBFErrdSEK9en6HaExYiKqGZ8Mi5wK7gLcDEv9zwBZVHSIijwD1VLVPcdtKTk7WefPmlTiGWVe/TLXZn7G3bhMyGzaBJk2o1KIJVVs3oWZSE+od05jEZpWpXLnEmzb5tG/fHoCZM2f6GkdxcnLgy493s7j/RM5cPIa/8D0H4qqw85JrafDonXDWWda7NxWGiMxX1eT8y8PW41fV2SLSKt/iy4H23v23gJlAsYm/1DHs20/tXetplTGPhilpxHHoh9xmGpAW14StVRuzs2YT9uX7kKh2hPuQaNiqJo0aQb16lhei0a5dMPWZRWS/OoaO297hInaQnngMO3qMoPY9N9Kgfn2/QzQmYiI9xn+YqqYCqGqqiDQqbEUR6Q50B2jZsmWpdtZ+ai+gFwCamcWOP9LY9lsqu5ZvZN/KVLLXpSIbU0nYnEri9lSO3L6M+ukbqbzswCHb2klNUmnCYpqwtWoTdtZyHxJZie5DIqFFY6od0YQ6revT6DDhsMOgWTOIjy9V6CZEVi7Zw/cPTeKI6aO5Lvs79ksVNpx9DdWfupPE9mfbp7iJSeX24K6qjgHGgBvqKev2JKEStY9uSu2jmxa3Y9iyhay1qexYmsqu5ansX5lK9vqNyKZUWmxOpc32n6i9JZXq6bsg5eCn76cyG2nMepoyJ+4ItjZMIrNVEpVPaEODM5I4KrkubdpAjRplbZEpjCr8MHYxm58ezdkr3+E6trO+9tGs6vYCh/e/idYNG/gdojG+inTi3yQiTbzefhMgLcL7L54INGhApQYNqN/ueIocANi1C1JTITWVfStT2b08lf2rUolbn0rLDetpm/otddPGE5em8APwBqTTkIUksaFGEruatEGPSqLmKUk0PieJpJNrcthh1gktrT0Ze/nx4UnUnjiGM/bMYT+V+f24q8l84k6aXflX+8Ma4wnbwV0Ab4x/asDB3eeBjICDu/VVtXdx2yntwd1yYd8+WLGCA0tSyPg+hb0LU4j7Yxm1N6ZQf+/6g1bdQBNWxieRUT+JfS2SSDg2iTqntaH5eUfS+thqJCT41IYgbN68GYCGDRtGdL87V2WwadbvbHp5Esf99DZ1dRurqrQhrfOdnPD8TVRrEdl4jClPCju4G85ZPeNxB3IbApuAx4EPgUlAS2ANcI2qbiluW1Gd+Iuyezc5KX+w5fsUtn6/jMxfU6i8JoX6GSnUP7DpoFXX0IL11ZPY0SiJ7COSqHpiGxqemcTh5x9BncSKNy1JFbZtg3XrYOPS7ez6OYWs31KotCqF2qnLSNyWQot9KdTDzQbeT2V+aH4VtXt158T7zkPirHdvTMQTfyhV2MRflB072PlzCpu+TmHnzynoshRqrE+h0Y4U6mVn5K2WTRzr4g8nrXYSu5u3QdokUSXpcOIbJ5LQNJEqzROp0bQONWsJNWsStqmr48aNA6Bbt27FrqsKGRkuqefeNq3cQ+Zvy0lYuYyaG1NI3JrCEdnLSCKFwwJGBHMQNlVuQVrdNuw8LIkDhycRf0wbjrj+DFqcbL17YwJZ4q9AMjdtYcOsFDbPTWHfLynEr0ihTtoymu5OoQ47Dln/AAlspiHpJJIhiWxJaMTOKonsrJbInuqJ7K/tbtn1E8lpkEhcg3rUrB1HzZpQqxbUrMlB9wOX1awJcXF/zuP/6quZbN7skvnatfmS+5r9xK1aQa3UZRyemUISKbTBJffmHDzstb1GE5fYW7Uh/ugkarRzw14JbY+AatUi8Wc2JupZ4o8BmqOk/5rOlkXryNqQRtbGdEhLRzLSid+STuVt6VTZkU613enU3JNO9cztBW4ni3gyaEA6iQfd0mh0yLJ0EtlbrQH7sy4gLg7icqbTNHNVXkJPIoU2ksIx8ctomrWGeHLy9rO/VgOX2NsmUfWEJOKObgNJSXDUUe7TxRhTJhE/gctEnsQJjY5vRKPjCz094mD798PmzZCeftCtUno6iZvSqbcxnaRNaZC+kPgt6VTaUXB1jZy9wnkST47EM5tqxJP152O1aiNtkpA2Z0LSTS6xt3EJvkq9elQJRcONMSViiT+WVanizjJr1uyQh+KAQw4HZGa6wfl8HxRxaWnEv/468dnZxN96a15iJymJuEaNbBqlMeWMJX4TvIQEaNzY3fKbPdv9HDw4sjEZY0rMEr8JiWnTpvkdgjEmSJb4TUhUr17d7xCMMUGK8zsAUzGMHDmSkSNH+h2GMSYIlvhNSEyaNIlJkyb5HYYxJgiW+I0xJsZY4jfGmBhjid8YY2KMJX5jjIkxUVGrR0TSgdWlfHpDYHMIw4kG1ubYYG2ODWVp8+Gqmph/YVQk/rIQkXkFFSmqyKzNscHaHBvC0WYb6jHGmBhjid8YY2JMLCT+MX4H4ANrc2ywNseGkLe5wo/xG2OMOVgs9PiNMcYEsMRvjDExpsIkfhG5RESWishyEXmkgMdFRF7yHl8kIqf4EWcoBdHmo0VkrojsF5FefsQYakG0+Qbv9V0kInNE5CQ/4gyVINp7udfWBSIyT0TO8SPOUCquzQHrnSYi2SJydSTjC4cgXuf2IrLde50XiMiAMu1QVaP+BsQDfwBH4K4YuBA4Nt86HYBPAQH+Anzvd9wRaHMj4DTgaaCX3zFHqM1nAfW8+5dG8+scZHtr8uexuhOB3/2OO9xtDljvK2AacLXfcUfgdW4PTA3VPitKj/90YLmqrlDVA8AE4PJ861wOvK3Od0BdEWkS6UBDqNg2q2qaqv4IZPoRYBgE0+Y5qpp7VfjvgOYRjjGUgmnvLvUyA1ADiPbZGsH8LwPcB7wPpEUyuDAJts0hU1ESfzNgbcDv67xlJV0nmlS09gSjpG2+DfctL1oF1V4RuUJEfgc+AW6NUGzhUmybRaQZcAUwKoJxhVOw7+szRWShiHwqIseVZYcVJfFLAcvy93yCWSeaVLT2BCPoNovI+bjE3yesEYVXUO1V1Q9U9WigM/BUuIMKs2DaPALoo6rZ4Q8nIoJp80+4ujsnAS8DH5ZlhxUl8a8DWgT83hzYUIp1oklFa08wgmqziJwIvA5crqoZEYotHEr0GqvqbOBIEWkY7sDCKJg2JwMTRGQVcDUwUkQ6RyS68Ci2zaq6Q1V3efenAQlleZ0rSuL/EUgSkdYiUhm4Dvg43zofAzd5s3v+AmxX1dRIBxpCwbS5oim2zSLSEvgvcKOqLvMhxlAKpr1HiYh490/BHRyM5g+7Ytusqq1VtZWqtgImA/eo6ocRjzR0gnmdGwe8zqfjcnepX+dKZQi23FDVLBG5F/gMd4R8rKouEZG7vMdH4Y7+dwCWA3uAW/yKNxSCabOINAbmAbWBHBHpiZstsMOvuMsiyNd5ANAA1wsEyNIoreYYZHuvwnVoMoG9QJeAg71RJ8g2VyhBtvlq4G4RycK9zteV5XW2kg3GGBNjKspQjzHGmCBZ4jfGmBhjid8YY2KMJX5jjIkxlviNMSbGWOI3FZqINAioaLhRRNZ797eJyK9h2N/AklZCFZFdhSwfVxEqT5ryxxK/qdBUNUNV26lqO1xtl+He/XZATnHPF5EKca6LMYEs8ZtYFi8ir4nIEhH5XESqAYjITBF5RkRmAfeLyKkiMktE5ovIZ7lVXUWkh4j86tXDnxCw3WO9bawQkR65C0XkQRFZ7N165g/GO6v8FW+bn+DKahsTctabMbEsCbheVe8QkUm4s2Df9R6rq6rniUgCMAtX9yddRLrgrm9wK/AI0FpV94tI3YDtHg2cD9QClorIq7ha+bcAZ+CKcn0vIrNU9eeA510BtAVOAA4DfgXGhqPhJrZZ4jexbKWqLvDuzwdaBTw20fvZFjge+MIrAREP5NZ4WgS8JyIfcnC1xE9UdT+wX0TScEn8HOADVd0NICL/Bf4KBCb+c4HxXtXJDSLyVdmbaMyhLPGbWLY/4H42UC3g993eTwGWqOqZBTz/Mlyy/jvQP6BGev7tVqLg0rsFsRoqJuxsjN+Yoi0FEkXkTAARSRCR40QkDmihqjOA3kBd3GUQCzMb6Cwi1UWkBm5Y5+sC1rlOROK94wjnh7gtxgDW4zemSKp6wJtS+ZKI1MH9z4wAlgHvessEN1tomzccVNB2fhKRccAP3qLX843vA3wA/A34xdv+rBA3xxjAqnMaY0zMsaEeY4yJMZb4jTEmxljiN8aYGGOJ3xhjYowlfmOMiTGW+I0xJsZY4jfGmBjz//Bhv2LV5sDwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cost_tunning(logistic_cost_results, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, XGBoost (eXtreme Gradient Boosting) was used as a more complex nonlinear tree-based model.  This model significantly improved performance while maintaining some interpretability with feature importances.  The XGBoost model overfit the training set such that it achieved a perfect AUC=1.0, and this resulted in a maximum test AUC=0.9434.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = TuningClassificationModeling(loader.get_df(),'y',\n",
    "                                           StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=12343),\n",
    "                                           None, XGBClassifier, None, LabelEncoder(), beta=1,classification_type = 'xgb' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "[19:31:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:31:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:31:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:32:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:35:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:37:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:42:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:43:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:43:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:44:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:50:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:55:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:55:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:56:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:59:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:04:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:09:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:20:07] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:20:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:21:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:24:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:30:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:35:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:46:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:47:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:49:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:53:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:02:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:11:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:26:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:27:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:29:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:32:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:39:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:45:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb_results = xgb_classifier.parameter_tuning( { \n",
    "    'max_depth': [3,6,10],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    " }, XGBClassifier);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Best XGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.7,\n",
       "              enable_categorical=False, gamma=0, gpu_id=-1,\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=1000, n_jobs=8, num_parallel_tree=1,\n",
       "              predictor='auto', random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "              scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_xgb_model= xgb_classifier.find_best_model('auc')\n",
    "best_xgb_model['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 1.0,\n",
       " 'cost': 0.0,\n",
       " 'matrix': array([[67062,     0],\n",
       "        [    0, 44938]])}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ metric: best_xgb_model['train_metrics'][metric] for metric in ['auc', 'cost', 'matrix'] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 0.9434452613876319,\n",
       " 'cost': -3.0145833333333334,\n",
       " 'matrix': array([[27657,  1084],\n",
       "        [ 1452, 17807]])}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ metric: best_xgb_model['test_metrics'][metric] for metric in ['auc', 'cost', 'matrix'] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "###### todo: add some writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_model = xgb_classifier.find_best_model('auc')['model']\n",
    "xgboost.plot_importance(model, max_num_features=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tunning Treshold to for Lowest Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Train Cost</th>\n",
       "      <th>Test Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>10.030804</td>\n",
       "      <td>10.030729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.033705</td>\n",
       "      <td>2.707813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>2.445833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.398438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.398438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.470312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.526562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.616146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.752083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.850521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>3.014583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Threshold  Train Cost  Test Cost\n",
       "0        0.00   10.030804  10.030729\n",
       "1        0.05    0.033705   2.707813\n",
       "2        0.10    0.000223   2.445833\n",
       "3        0.15   -0.000000   2.398438\n",
       "4        0.20   -0.000000   2.398438\n",
       "5        0.25   -0.000000   2.470312\n",
       "6        0.30   -0.000000   2.526562\n",
       "7        0.35   -0.000000   2.616146\n",
       "8        0.40   -0.000000   2.752083\n",
       "9        0.45   -0.000000   2.850521\n",
       "10       0.50   -0.000000   3.014583"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_proba, test_proba, y_train, y_test, conf_train, conf_test = extract_best_model_metrics(xgb_classifier)\n",
    "xgb_cost_results = tune_cost_proba(train_proba[:,0], test_proba[:,0], y_train, y_test, conf_train, conf_test)\n",
    "xgb_cost_results[['Threshold', 'Train Cost','Test Cost' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cost_tunning(xgb_cost_results, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a Neural Network model was fit on the dataset, and its performance was compared against the rest of the models.  This was the most complex model with the least interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_modeling = TuningClassificationModeling(loader.get_df(),'y',\n",
    "                                           StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=12343),\n",
    "                                           SimpleImputer(missing_values=np.nan, strategy='mean'), NNModel, None, LabelEncoder(), beta=1,classification_type='neural' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_modeling.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 50, 'layer1': {'s': 300, 'activation': 'relu'}, 'layer2': {'s': 200, 'activation': 'relu'}, 'layer3': {'s': 100, 'activation': 'relu'}, 'layer4': {'s': 1, 'activation': 'sigmoid'}, 'loss': 'BinaryCrossentropy', 'metric': <keras.metrics.AUC object at 0x7f9f8fae16a0>, 'optimizer': 'adam'}\n",
      "<keras.engine.sequential.Sequential object at 0x7fa027eeed90>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9f91099f10>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9f910ab160>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9f90d05b20>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9f90ced2e0>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9f90597970>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9f905a1250>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9f905a9790>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9f905af310>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9f907e85e0>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9f905c53d0>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9f90811400>\n",
      "Epoch 1/10\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.4741 - auc: 0.8851\n",
      "Epoch 2/10\n",
      "11200/11200 [==============================] - 20s 2ms/step - loss: 0.2344 - auc: 0.9651\n",
      "Epoch 3/10\n",
      "11200/11200 [==============================] - 19s 2ms/step - loss: 0.1943 - auc: 0.9758\n",
      "Epoch 4/10\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1728 - auc: 0.9805\n",
      "Epoch 5/10\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1602 - auc: 0.9829\n",
      "Epoch 6/10\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.1493 - auc: 0.9848\n",
      "Epoch 7/10\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1434 - auc: 0.9858\n",
      "Epoch 8/10\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.1378 - auc: 0.9867\n",
      "Epoch 9/10\n",
      "11200/11200 [==============================] - 17s 1ms/step - loss: 0.1334 - auc: 0.9874\n",
      "Epoch 10/10\n",
      "11200/11200 [==============================] - 17s 1ms/step - loss: 0.1278 - auc: 0.9882\n",
      "Epoch 1/10\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.6611 - auc: 0.9350\n",
      "Epoch 2/10\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.3043 - auc: 0.9422\n",
      "Epoch 3/10\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.2642 - auc: 0.9562\n",
      "Epoch 4/10\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.2271 - auc: 0.9672\n",
      "Epoch 5/10\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1991 - auc: 0.9745\n",
      "Epoch 6/10\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1768 - auc: 0.9797\n",
      "Epoch 7/10\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1621 - auc: 0.9825\n",
      "Epoch 8/10\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1501 - auc: 0.9847\n",
      "Epoch 9/10\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1350 - auc: 0.9873\n",
      "Epoch 10/10\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1281 - auc: 0.9882\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 1.5138 - auc: 0.8838\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.4857 - auc: 0.8598\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.4170 - auc: 0.8948\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.3370 - auc: 0.9283\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.3161 - auc: 0.9373\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.3141 - auc: 0.9392\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2790 - auc: 0.9510\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.2475 - auc: 0.9611\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2490 - auc: 0.9608\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2394 - auc: 0.9637\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 8.7963 - auc: 0.7071\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 2.6101 - auc: 0.5906\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 0.9265 - auc: 0.7096\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.6769 - auc: 0.7607\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 0.5707 - auc: 0.7948\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 0.5549 - auc: 0.8030\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.5509 - auc: 0.8131\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.5123 - auc: 0.8322\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.4549 - auc: 0.8632\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.4199 - auc: 0.8839\n",
      "Epoch 1/30\n",
      "11200/11200 [==============================] - 19s 2ms/step - loss: 0.4801 - auc: 0.8852\n",
      "Epoch 2/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.2328 - auc: 0.9658\n",
      "Epoch 3/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1925 - auc: 0.9763\n",
      "Epoch 4/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1731 - auc: 0.9804\n",
      "Epoch 5/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1620 - auc: 0.9825\n",
      "Epoch 6/30\n",
      "11200/11200 [==============================] - 19s 2ms/step - loss: 0.1526 - auc: 0.9843\n",
      "Epoch 7/30\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.1468 - auc: 0.9853\n",
      "Epoch 8/30\n",
      "11200/11200 [==============================] - 21s 2ms/step - loss: 0.1408 - auc: 0.9864\n",
      "Epoch 9/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1368 - auc: 0.9870\n",
      "Epoch 10/30\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.1317 - auc: 0.9877\n",
      "Epoch 11/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1291 - auc: 0.9880\n",
      "Epoch 12/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1252 - auc: 0.9886\n",
      "Epoch 13/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1222 - auc: 0.9891\n",
      "Epoch 14/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1203 - auc: 0.9892\n",
      "Epoch 15/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1178 - auc: 0.9897\n",
      "Epoch 16/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1157 - auc: 0.9899\n",
      "Epoch 17/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1134 - auc: 0.9903\n",
      "Epoch 18/30\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.1114 - auc: 0.9903\n",
      "Epoch 19/30\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.1120 - auc: 0.9905\n",
      "Epoch 20/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1104 - auc: 0.9909\n",
      "Epoch 21/30\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.1067 - auc: 0.9912\n",
      "Epoch 22/30\n",
      "11200/11200 [==============================] - 17s 1ms/step - loss: 0.1054 - auc: 0.9914\n",
      "Epoch 23/30\n",
      "11200/11200 [==============================] - 17s 1ms/step - loss: 0.1036 - auc: 0.9915\n",
      "Epoch 24/30\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.1024 - auc: 0.9918\n",
      "Epoch 25/30\n",
      "11200/11200 [==============================] - 19s 2ms/step - loss: 0.1042 - auc: 0.9917\n",
      "Epoch 26/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1006 - auc: 0.9920\n",
      "Epoch 27/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0982 - auc: 0.9923\n",
      "Epoch 28/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0977 - auc: 0.9923\n",
      "Epoch 29/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0979 - auc: 0.9924\n",
      "Epoch 30/30\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0961 - auc: 0.9925\n",
      "Epoch 1/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.6463 - auc: 0.9416\n",
      "Epoch 2/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.3127 - auc: 0.9391\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.2615 - auc: 0.9570\n",
      "Epoch 4/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.2283 - auc: 0.9670\n",
      "Epoch 5/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.2047 - auc: 0.9732\n",
      "Epoch 6/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1824 - auc: 0.9785\n",
      "Epoch 7/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1631 - auc: 0.9823\n",
      "Epoch 8/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1483 - auc: 0.9850\n",
      "Epoch 9/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1374 - auc: 0.9869\n",
      "Epoch 10/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1263 - auc: 0.9886\n",
      "Epoch 11/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1205 - auc: 0.9894\n",
      "Epoch 12/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1142 - auc: 0.9903\n",
      "Epoch 13/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1098 - auc: 0.9908\n",
      "Epoch 14/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1033 - auc: 0.9916\n",
      "Epoch 15/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1009 - auc: 0.9920\n",
      "Epoch 16/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0964 - auc: 0.9925\n",
      "Epoch 17/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0935 - auc: 0.9930\n",
      "Epoch 18/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0893 - auc: 0.9934\n",
      "Epoch 19/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0851 - auc: 0.9939\n",
      "Epoch 20/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0823 - auc: 0.9942\n",
      "Epoch 21/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0802 - auc: 0.9946\n",
      "Epoch 22/30\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0767 - auc: 0.9950\n",
      "Epoch 23/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0744 - auc: 0.9953\n",
      "Epoch 24/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0706 - auc: 0.9956\n",
      "Epoch 25/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0672 - auc: 0.9960\n",
      "Epoch 26/30\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0671 - auc: 0.9959\n",
      "Epoch 27/30\n",
      "1120/1120 [==============================] - 4s 3ms/step - loss: 0.0636 - auc: 0.9965\n",
      "Epoch 28/30\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0623 - auc: 0.9965\n",
      "Epoch 29/30\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0596 - auc: 0.9966\n",
      "Epoch 30/30\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0596 - auc: 0.9967\n",
      "Epoch 1/30\n",
      "112/112 [==============================] - 2s 9ms/step - loss: 1.5044 - auc: 0.9024\n",
      "Epoch 2/30\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.4695 - auc: 0.8624\n",
      "Epoch 3/30\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.4124 - auc: 0.8967\n",
      "Epoch 4/30\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.3409 - auc: 0.9268\n",
      "Epoch 5/30\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.3003 - auc: 0.9428\n",
      "Epoch 6/30\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.3033 - auc: 0.9433\n",
      "Epoch 7/30\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.2584 - auc: 0.9577\n",
      "Epoch 8/30\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.2611 - auc: 0.9570\n",
      "Epoch 9/30\n",
      "112/112 [==============================] - 1s 12ms/step - loss: 0.2334 - auc: 0.9653\n",
      "Epoch 10/30\n",
      "112/112 [==============================] - 1s 11ms/step - loss: 0.2292 - auc: 0.9666\n",
      "Epoch 11/30\n",
      "112/112 [==============================] - 1s 13ms/step - loss: 0.2227 - auc: 0.9685\n",
      "Epoch 12/30\n",
      "112/112 [==============================] - 1s 12ms/step - loss: 0.2150 - auc: 0.9706\n",
      "Epoch 13/30\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.2029 - auc: 0.9736\n",
      "Epoch 14/30\n",
      "112/112 [==============================] - 2s 14ms/step - loss: 0.2019 - auc: 0.9738\n",
      "Epoch 15/30\n",
      "112/112 [==============================] - 1s 12ms/step - loss: 0.1919 - auc: 0.9763\n",
      "Epoch 16/30\n",
      "112/112 [==============================] - 2s 14ms/step - loss: 0.2083 - auc: 0.9723\n",
      "Epoch 17/30\n",
      "112/112 [==============================] - 2s 17ms/step - loss: 0.1810 - auc: 0.9787\n",
      "Epoch 18/30\n",
      "112/112 [==============================] - 2s 16ms/step - loss: 0.1781 - auc: 0.9794\n",
      "Epoch 19/30\n",
      "112/112 [==============================] - 2s 18ms/step - loss: 0.1705 - auc: 0.9809\n",
      "Epoch 20/30\n",
      "112/112 [==============================] - 2s 16ms/step - loss: 0.1623 - auc: 0.9826\n",
      "Epoch 21/30\n",
      "112/112 [==============================] - 2s 14ms/step - loss: 0.1594 - auc: 0.9831\n",
      "Epoch 22/30\n",
      "112/112 [==============================] - 1s 13ms/step - loss: 0.1563 - auc: 0.9838\n",
      "Epoch 23/30\n",
      "112/112 [==============================] - 2s 15ms/step - loss: 0.1580 - auc: 0.9835\n",
      "Epoch 24/30\n",
      "112/112 [==============================] - 2s 16ms/step - loss: 0.1496 - auc: 0.9851\n",
      "Epoch 25/30\n",
      "112/112 [==============================] - 1s 13ms/step - loss: 0.1436 - auc: 0.9862\n",
      "Epoch 26/30\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1483 - auc: 0.9852\n",
      "Epoch 27/30\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1350 - auc: 0.9877\n",
      "Epoch 28/30\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1358 - auc: 0.9875\n",
      "Epoch 29/30\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1334 - auc: 0.9879\n",
      "Epoch 30/30\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1368 - auc: 0.9874\n",
      "Epoch 1/30\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 14.1849 - auc: 0.7165\n",
      "Epoch 2/30\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 7.4369 - auc: 0.5278\n",
      "Epoch 3/30\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 2.4700 - auc: 0.6027\n",
      "Epoch 4/30\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 1.0718 - auc: 0.7188\n",
      "Epoch 5/30\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 0.6699 - auc: 0.7889\n",
      "Epoch 6/30\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 0.5494 - auc: 0.8171\n",
      "Epoch 7/30\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.6595 - auc: 0.7733\n",
      "Epoch 8/30\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.5662 - auc: 0.8058\n",
      "Epoch 9/30\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 0.4786 - auc: 0.8474\n",
      "Epoch 10/30\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.4453 - auc: 0.8679\n",
      "Epoch 11/30\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4214 - auc: 0.8841\n",
      "Epoch 12/30\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.4340 - auc: 0.8775\n",
      "Epoch 13/30\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.4767 - auc: 0.8575\n",
      "Epoch 14/30\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.5272 - auc: 0.8420\n",
      "Epoch 15/30\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.4118 - auc: 0.8897\n",
      "Epoch 16/30\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.3830 - auc: 0.9050\n",
      "Epoch 17/30\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 0.3910 - auc: 0.9007\n",
      "Epoch 18/30\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3961 - auc: 0.8989\n",
      "Epoch 19/30\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.3707 - auc: 0.9115\n",
      "Epoch 20/30\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.3574 - auc: 0.9180\n",
      "Epoch 21/30\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.3495 - auc: 0.9217\n",
      "Epoch 22/30\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3371 - auc: 0.9277\n",
      "Epoch 23/30\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 0.3340 - auc: 0.9287\n",
      "Epoch 24/30\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.3326 - auc: 0.9296\n",
      "Epoch 25/30\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.4017 - auc: 0.9014\n",
      "Epoch 26/30\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.3726 - auc: 0.9125\n",
      "Epoch 27/30\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.3402 - auc: 0.9263\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3196 - auc: 0.9349\n",
      "Epoch 29/30\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.3040 - auc: 0.9415\n",
      "Epoch 30/30\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 0.3063 - auc: 0.9405\n",
      "Epoch 1/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.4838 - auc: 0.9164\n",
      "Epoch 2/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.2280 - auc: 0.9670\n",
      "Epoch 3/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.1914 - auc: 0.9764\n",
      "Epoch 4/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.1721 - auc: 0.9806\n",
      "Epoch 5/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.1604 - auc: 0.9827\n",
      "Epoch 6/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.1492 - auc: 0.9848\n",
      "Epoch 7/100\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.1420 - auc: 0.9861\n",
      "Epoch 8/100\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.1380 - auc: 0.9867\n",
      "Epoch 9/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.1315 - auc: 0.9877\n",
      "Epoch 10/100\n",
      "11200/11200 [==============================] - 17s 1ms/step - loss: 0.1282 - auc: 0.9882\n",
      "Epoch 11/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.1252 - auc: 0.9887: 0s - loss: 0.1\n",
      "Epoch 12/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.1220 - auc: 0.9891\n",
      "Epoch 13/100\n",
      "11200/11200 [==============================] - 17s 1ms/step - loss: 0.1187 - auc: 0.9895\n",
      "Epoch 14/100\n",
      "11200/11200 [==============================] - 17s 1ms/step - loss: 0.1169 - auc: 0.9898\n",
      "Epoch 15/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.1146 - auc: 0.9901\n",
      "Epoch 16/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.1106 - auc: 0.9906\n",
      "Epoch 17/100\n",
      "11200/11200 [==============================] - 17s 1ms/step - loss: 0.1106 - auc: 0.9907\n",
      "Epoch 18/100\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.1067 - auc: 0.9911\n",
      "Epoch 19/100\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.1069 - auc: 0.9913\n",
      "Epoch 20/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1032 - auc: 0.9915\n",
      "Epoch 21/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1032 - auc: 0.9917\n",
      "Epoch 22/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1004 - auc: 0.9921\n",
      "Epoch 23/100\n",
      "11200/11200 [==============================] - 20s 2ms/step - loss: 0.0991 - auc: 0.9921\n",
      "Epoch 24/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0981 - auc: 0.9922\n",
      "Epoch 25/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0973 - auc: 0.9923\n",
      "Epoch 26/100\n",
      "11200/11200 [==============================] - 19s 2ms/step - loss: 0.1012 - auc: 0.9922\n",
      "Epoch 27/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.1107 - auc: 0.9926\n",
      "Epoch 28/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.0923 - auc: 0.9930\n",
      "Epoch 29/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.0916 - auc: 0.9929\n",
      "Epoch 30/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0929 - auc: 0.9929\n",
      "Epoch 31/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0885 - auc: 0.9935\n",
      "Epoch 32/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.0885 - auc: 0.9933\n",
      "Epoch 33/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0886 - auc: 0.9936\n",
      "Epoch 34/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.0874 - auc: 0.9936\n",
      "Epoch 35/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.0871 - auc: 0.9937\n",
      "Epoch 36/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.0868 - auc: 0.9938\n",
      "Epoch 37/100\n",
      "11200/11200 [==============================] - 19s 2ms/step - loss: 0.0837 - auc: 0.9941\n",
      "Epoch 38/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0854 - auc: 0.9940\n",
      "Epoch 39/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0828 - auc: 0.9941\n",
      "Epoch 40/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.0959 - auc: 0.9940\n",
      "Epoch 41/100\n",
      "11200/11200 [==============================] - 19s 2ms/step - loss: 0.0791 - auc: 0.9946\n",
      "Epoch 42/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0820 - auc: 0.9942\n",
      "Epoch 43/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0788 - auc: 0.9948\n",
      "Epoch 44/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0818 - auc: 0.9945\n",
      "Epoch 45/100\n",
      "11200/11200 [==============================] - 19s 2ms/step - loss: 0.0801 - auc: 0.9947\n",
      "Epoch 46/100\n",
      "11200/11200 [==============================] - 19s 2ms/step - loss: 0.0776 - auc: 0.9948\n",
      "Epoch 47/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0771 - auc: 0.9949\n",
      "Epoch 48/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.0795 - auc: 0.9947\n",
      "Epoch 49/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0826 - auc: 0.9947\n",
      "Epoch 50/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.0768 - auc: 0.9951\n",
      "Epoch 51/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.0764 - auc: 0.9951\n",
      "Epoch 52/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0873 - auc: 0.9948\n",
      "Epoch 53/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0817 - auc: 0.9951\n",
      "Epoch 54/100\n",
      "11200/11200 [==============================] - 20s 2ms/step - loss: 0.0744 - auc: 0.9951\n",
      "Epoch 55/100\n",
      "11200/11200 [==============================] - 20s 2ms/step - loss: 0.0723 - auc: 0.9954\n",
      "Epoch 56/100\n",
      "11200/11200 [==============================] - 20s 2ms/step - loss: 0.0726 - auc: 0.9955\n",
      "Epoch 57/100\n",
      "11200/11200 [==============================] - 19s 2ms/step - loss: 0.0720 - auc: 0.9954\n",
      "Epoch 58/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0744 - auc: 0.9955\n",
      "Epoch 59/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0797 - auc: 0.9950\n",
      "Epoch 60/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0699 - auc: 0.9958\n",
      "Epoch 61/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0710 - auc: 0.9956\n",
      "Epoch 62/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0685 - auc: 0.9959\n",
      "Epoch 63/100\n",
      "11200/11200 [==============================] - 20s 2ms/step - loss: 0.0754 - auc: 0.9956\n",
      "Epoch 64/100\n",
      "11200/11200 [==============================] - 19s 2ms/step - loss: 0.0692 - auc: 0.9959\n",
      "Epoch 65/100\n",
      "11200/11200 [==============================] - 19s 2ms/step - loss: 0.0667 - auc: 0.9960\n",
      "Epoch 66/100\n",
      "11200/11200 [==============================] - 19s 2ms/step - loss: 0.0691 - auc: 0.9959\n",
      "Epoch 67/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0754 - auc: 0.9956\n",
      "Epoch 68/100\n",
      "11200/11200 [==============================] - 19s 2ms/step - loss: 0.0712 - auc: 0.9960\n",
      "Epoch 69/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0696 - auc: 0.9959\n",
      "Epoch 70/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0777 - auc: 0.9959\n",
      "Epoch 71/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.0741 - auc: 0.9956\n",
      "Epoch 72/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0703 - auc: 0.9960\n",
      "Epoch 73/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0679 - auc: 0.9960\n",
      "Epoch 74/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0699 - auc: 0.9959\n",
      "Epoch 75/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.0707 - auc: 0.9960\n",
      "Epoch 76/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0663 - auc: 0.9960\n",
      "Epoch 77/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0656 - auc: 0.9961\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0689 - auc: 0.9960\n",
      "Epoch 79/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.0666 - auc: 0.9957\n",
      "Epoch 80/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0694 - auc: 0.9952\n",
      "Epoch 81/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0703 - auc: 0.9954\n",
      "Epoch 82/100\n",
      "11200/11200 [==============================] - 19s 2ms/step - loss: 0.0748 - auc: 0.9945\n",
      "Epoch 83/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0662 - auc: 0.9952\n",
      "Epoch 84/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0700 - auc: 0.9950\n",
      "Epoch 85/100\n",
      "11200/11200 [==============================] - 17s 1ms/step - loss: 0.0636 - auc: 0.9953\n",
      "Epoch 86/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.0706 - auc: 0.9947\n",
      "Epoch 87/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0639 - auc: 0.9951\n",
      "Epoch 88/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.0694 - auc: 0.9946\n",
      "Epoch 89/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0643 - auc: 0.9951\n",
      "Epoch 90/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0629 - auc: 0.9950\n",
      "Epoch 91/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0660 - auc: 0.9949\n",
      "Epoch 92/100\n",
      "11200/11200 [==============================] - 19s 2ms/step - loss: 0.0635 - auc: 0.9952\n",
      "Epoch 93/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0649 - auc: 0.9951\n",
      "Epoch 94/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0585 - auc: 0.9953\n",
      "Epoch 95/100\n",
      "11200/11200 [==============================] - 19s 2ms/step - loss: 0.0698 - auc: 0.9948\n",
      "Epoch 96/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0631 - auc: 0.9950\n",
      "Epoch 97/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0634 - auc: 0.9953\n",
      "Epoch 98/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.0623 - auc: 0.9953\n",
      "Epoch 99/100\n",
      "11200/11200 [==============================] - 18s 2ms/step - loss: 0.0608 - auc: 0.9952\n",
      "Epoch 100/100\n",
      "11200/11200 [==============================] - 17s 2ms/step - loss: 0.0622 - auc: 0.9953\n",
      "Epoch 1/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.6594 - auc: 0.9476\n",
      "Epoch 2/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.2992 - auc: 0.9436\n",
      "Epoch 3/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.2566 - auc: 0.9584\n",
      "Epoch 4/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.2228 - auc: 0.9683\n",
      "Epoch 5/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.1997 - auc: 0.9743\n",
      "Epoch 6/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.1785 - auc: 0.9791\n",
      "Epoch 7/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1610 - auc: 0.9826\n",
      "Epoch 8/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.1460 - auc: 0.9854\n",
      "Epoch 9/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.1365 - auc: 0.9869\n",
      "Epoch 10/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1273 - auc: 0.9884\n",
      "Epoch 11/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.1201 - auc: 0.9895\n",
      "Epoch 12/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.1160 - auc: 0.9900\n",
      "Epoch 13/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.1092 - auc: 0.9909\n",
      "Epoch 14/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.1028 - auc: 0.9917\n",
      "Epoch 15/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0992 - auc: 0.9923\n",
      "Epoch 16/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0956 - auc: 0.9926\n",
      "Epoch 17/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0898 - auc: 0.9934\n",
      "Epoch 18/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0884 - auc: 0.9936\n",
      "Epoch 19/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0842 - auc: 0.9939\n",
      "Epoch 20/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0819 - auc: 0.9944\n",
      "Epoch 21/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0781 - auc: 0.9948\n",
      "Epoch 22/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0745 - auc: 0.9952\n",
      "Epoch 23/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0721 - auc: 0.9954\n",
      "Epoch 24/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0701 - auc: 0.9957\n",
      "Epoch 25/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0682 - auc: 0.9958\n",
      "Epoch 26/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0640 - auc: 0.9963\n",
      "Epoch 27/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0625 - auc: 0.9964\n",
      "Epoch 28/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0620 - auc: 0.9965\n",
      "Epoch 29/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0582 - auc: 0.9968\n",
      "Epoch 30/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0574 - auc: 0.9970\n",
      "Epoch 31/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0563 - auc: 0.9970\n",
      "Epoch 32/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0534 - auc: 0.9973\n",
      "Epoch 33/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0515 - auc: 0.9973\n",
      "Epoch 34/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0502 - auc: 0.9974\n",
      "Epoch 35/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0502 - auc: 0.9975\n",
      "Epoch 36/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0504 - auc: 0.9975\n",
      "Epoch 37/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0463 - auc: 0.9979\n",
      "Epoch 38/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0460 - auc: 0.9978\n",
      "Epoch 39/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0440 - auc: 0.9981\n",
      "Epoch 40/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0439 - auc: 0.9981\n",
      "Epoch 41/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0437 - auc: 0.9980\n",
      "Epoch 42/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0422 - auc: 0.9981\n",
      "Epoch 43/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0413 - auc: 0.9982\n",
      "Epoch 44/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0400 - auc: 0.9982\n",
      "Epoch 45/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0414 - auc: 0.9981\n",
      "Epoch 46/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0386 - auc: 0.9984\n",
      "Epoch 47/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0385 - auc: 0.9984\n",
      "Epoch 48/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0374 - auc: 0.9985\n",
      "Epoch 49/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0371 - auc: 0.9985\n",
      "Epoch 50/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0376 - auc: 0.9984\n",
      "Epoch 51/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0360 - auc: 0.9986\n",
      "Epoch 52/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0350 - auc: 0.9986\n",
      "Epoch 53/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0335 - auc: 0.9987\n",
      "Epoch 54/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0359 - auc: 0.9986\n",
      "Epoch 55/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0341 - auc: 0.9986\n",
      "Epoch 56/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0333 - auc: 0.9986\n",
      "Epoch 57/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0295 - auc: 0.9989\n",
      "Epoch 58/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0334 - auc: 0.9986\n",
      "Epoch 59/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0314 - auc: 0.9989\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0310 - auc: 0.9988\n",
      "Epoch 61/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0314 - auc: 0.9988\n",
      "Epoch 62/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0297 - auc: 0.9990\n",
      "Epoch 63/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0310 - auc: 0.9988\n",
      "Epoch 64/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0292 - auc: 0.9989\n",
      "Epoch 65/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0296 - auc: 0.9988\n",
      "Epoch 66/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0274 - auc: 0.9989\n",
      "Epoch 67/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0307 - auc: 0.9988\n",
      "Epoch 68/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0258 - auc: 0.9991\n",
      "Epoch 69/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0307 - auc: 0.9987\n",
      "Epoch 70/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0277 - auc: 0.9989\n",
      "Epoch 71/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0273 - auc: 0.9989\n",
      "Epoch 72/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0259 - auc: 0.9991\n",
      "Epoch 73/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0259 - auc: 0.9991\n",
      "Epoch 74/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0256 - auc: 0.9991\n",
      "Epoch 75/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0269 - auc: 0.9990\n",
      "Epoch 76/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0253 - auc: 0.9990\n",
      "Epoch 77/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0262 - auc: 0.9991\n",
      "Epoch 78/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0267 - auc: 0.9990\n",
      "Epoch 79/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0245 - auc: 0.9990\n",
      "Epoch 80/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0264 - auc: 0.9989\n",
      "Epoch 81/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0251 - auc: 0.9990\n",
      "Epoch 82/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0235 - auc: 0.9992\n",
      "Epoch 83/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0253 - auc: 0.9991\n",
      "Epoch 84/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0257 - auc: 0.9990\n",
      "Epoch 85/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0234 - auc: 0.9991\n",
      "Epoch 86/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0246 - auc: 0.9991\n",
      "Epoch 87/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0225 - auc: 0.9991\n",
      "Epoch 88/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0226 - auc: 0.9991\n",
      "Epoch 89/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0237 - auc: 0.9991\n",
      "Epoch 90/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0232 - auc: 0.9992\n",
      "Epoch 91/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0223 - auc: 0.9992\n",
      "Epoch 92/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0287 - auc: 0.9987\n",
      "Epoch 93/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0205 - auc: 0.9993\n",
      "Epoch 94/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0251 - auc: 0.9990\n",
      "Epoch 95/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0233 - auc: 0.9991\n",
      "Epoch 96/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0199 - auc: 0.9994\n",
      "Epoch 97/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0237 - auc: 0.9991\n",
      "Epoch 98/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0224 - auc: 0.9992\n",
      "Epoch 99/100\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 0.0228 - auc: 0.9991\n",
      "Epoch 100/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0198 - auc: 0.9992\n",
      "Epoch 1/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 2.4551 - auc: 0.8917\n",
      "Epoch 2/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.4963 - auc: 0.8502\n",
      "Epoch 3/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.4939 - auc: 0.8678\n",
      "Epoch 4/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.3576 - auc: 0.9195\n",
      "Epoch 5/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.3203 - auc: 0.9351\n",
      "Epoch 6/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.3562 - auc: 0.9264\n",
      "Epoch 7/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2790 - auc: 0.9509\n",
      "Epoch 8/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2682 - auc: 0.9547\n",
      "Epoch 9/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2409 - auc: 0.9632\n",
      "Epoch 10/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2658 - auc: 0.9562\n",
      "Epoch 11/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2378 - auc: 0.9644\n",
      "Epoch 12/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2287 - auc: 0.9670\n",
      "Epoch 13/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2141 - auc: 0.9707\n",
      "Epoch 14/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.2069 - auc: 0.9726\n",
      "Epoch 15/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2054 - auc: 0.9731\n",
      "Epoch 16/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1995 - auc: 0.9746\n",
      "Epoch 17/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1906 - auc: 0.9767\n",
      "Epoch 18/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1832 - auc: 0.9783\n",
      "Epoch 19/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1842 - auc: 0.9781\n",
      "Epoch 20/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1807 - auc: 0.9789\n",
      "Epoch 21/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1726 - auc: 0.9806\n",
      "Epoch 22/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1609 - auc: 0.9829\n",
      "Epoch 23/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1592 - auc: 0.9833\n",
      "Epoch 24/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1707 - auc: 0.9809\n",
      "Epoch 25/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1575 - auc: 0.9837\n",
      "Epoch 26/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1555 - auc: 0.9840\n",
      "Epoch 27/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1486 - auc: 0.9852\n",
      "Epoch 28/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1449 - auc: 0.9859\n",
      "Epoch 29/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1410 - auc: 0.9866\n",
      "Epoch 30/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1337 - auc: 0.9879\n",
      "Epoch 31/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1384 - auc: 0.9871\n",
      "Epoch 32/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1315 - auc: 0.9882\n",
      "Epoch 33/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1300 - auc: 0.9884\n",
      "Epoch 34/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1347 - auc: 0.9876\n",
      "Epoch 35/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1259 - auc: 0.9890\n",
      "Epoch 36/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1172 - auc: 0.9905\n",
      "Epoch 37/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1205 - auc: 0.9899\n",
      "Epoch 38/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1165 - auc: 0.9905\n",
      "Epoch 39/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1182 - auc: 0.9902\n",
      "Epoch 40/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1178 - auc: 0.9903\n",
      "Epoch 41/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1179 - auc: 0.9903\n",
      "Epoch 42/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1087 - auc: 0.9916\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1116 - auc: 0.9911\n",
      "Epoch 44/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1090 - auc: 0.9916\n",
      "Epoch 45/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1039 - auc: 0.9923\n",
      "Epoch 46/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1120 - auc: 0.9912\n",
      "Epoch 47/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1009 - auc: 0.9926\n",
      "Epoch 48/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0998 - auc: 0.9928\n",
      "Epoch 49/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0989 - auc: 0.9929\n",
      "Epoch 50/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1022 - auc: 0.9925\n",
      "Epoch 51/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0987 - auc: 0.9929\n",
      "Epoch 52/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0950 - auc: 0.9933\n",
      "Epoch 53/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0896 - auc: 0.9941\n",
      "Epoch 54/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0896 - auc: 0.9939\n",
      "Epoch 55/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0906 - auc: 0.9939\n",
      "Epoch 56/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0867 - auc: 0.9943\n",
      "Epoch 57/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0873 - auc: 0.9941\n",
      "Epoch 58/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0886 - auc: 0.9941\n",
      "Epoch 59/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0862 - auc: 0.9943\n",
      "Epoch 60/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0805 - auc: 0.9949\n",
      "Epoch 61/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0800 - auc: 0.9950\n",
      "Epoch 62/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0823 - auc: 0.9948\n",
      "Epoch 63/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0798 - auc: 0.9950\n",
      "Epoch 64/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0785 - auc: 0.9952\n",
      "Epoch 65/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0768 - auc: 0.9954\n",
      "Epoch 66/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0757 - auc: 0.9955\n",
      "Epoch 67/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0759 - auc: 0.9954\n",
      "Epoch 68/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0723 - auc: 0.9958\n",
      "Epoch 69/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0700 - auc: 0.9959\n",
      "Epoch 70/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0694 - auc: 0.9961\n",
      "Epoch 71/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0675 - auc: 0.9962\n",
      "Epoch 72/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0695 - auc: 0.9961\n",
      "Epoch 73/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0670 - auc: 0.9961\n",
      "Epoch 74/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0665 - auc: 0.9962\n",
      "Epoch 75/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0664 - auc: 0.9963\n",
      "Epoch 76/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0682 - auc: 0.9961\n",
      "Epoch 77/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0647 - auc: 0.9965\n",
      "Epoch 78/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0601 - auc: 0.9969\n",
      "Epoch 79/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0601 - auc: 0.9968\n",
      "Epoch 80/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0589 - auc: 0.9970\n",
      "Epoch 81/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0571 - auc: 0.9971\n",
      "Epoch 82/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0565 - auc: 0.9972\n",
      "Epoch 83/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0559 - auc: 0.9973\n",
      "Epoch 84/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0570 - auc: 0.9972\n",
      "Epoch 85/100\n",
      "112/112 [==============================] - 1s 11ms/step - loss: 0.0571 - auc: 0.9971\n",
      "Epoch 86/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0543 - auc: 0.9974\n",
      "Epoch 87/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0530 - auc: 0.9976\n",
      "Epoch 88/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0566 - auc: 0.9971\n",
      "Epoch 89/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0501 - auc: 0.9976\n",
      "Epoch 90/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0522 - auc: 0.9975\n",
      "Epoch 91/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0476 - auc: 0.9978\n",
      "Epoch 92/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0468 - auc: 0.9979\n",
      "Epoch 93/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0497 - auc: 0.9976\n",
      "Epoch 94/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0481 - auc: 0.9978\n",
      "Epoch 95/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0469 - auc: 0.9978\n",
      "Epoch 96/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0439 - auc: 0.9982\n",
      "Epoch 97/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0440 - auc: 0.9981\n",
      "Epoch 98/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0447 - auc: 0.9980\n",
      "Epoch 99/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.0418 - auc: 0.9983\n",
      "Epoch 100/100\n",
      "112/112 [==============================] - 1s 11ms/step - loss: 0.0448 - auc: 0.9979\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 9.0508 - auc: 0.7728\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 3.5931 - auc: 0.5636\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 1.3883 - auc: 0.6583\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 0.7582 - auc: 0.7564\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 0.5638 - auc: 0.8083\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 0.4914 - auc: 0.8383\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 0.4689 - auc: 0.8519\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 0.4849 - auc: 0.8435\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 0.4825 - auc: 0.8469\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.4759 - auc: 0.8529\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.4686 - auc: 0.8584\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3990 - auc: 0.8967\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3967 - auc: 0.8975\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.4151 - auc: 0.8885\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.4005 - auc: 0.8967\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3839 - auc: 0.9049\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3596 - auc: 0.9175\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.3480 - auc: 0.9231\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3389 - auc: 0.9272\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3683 - auc: 0.9146\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.3598 - auc: 0.9172\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3282 - auc: 0.9317\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3177 - auc: 0.9364\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.5118 - auc: 0.8721\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.4339 - auc: 0.8927\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.3457 - auc: 0.9240\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 56ms/step - loss: 0.3116 - auc: 0.9390\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.3210 - auc: 0.9345\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.3001 - auc: 0.9434\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.2982 - auc: 0.9440\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.3461 - auc: 0.9253\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3193 - auc: 0.9352\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 0.2950 - auc: 0.9451\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2948 - auc: 0.9450\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.2933 - auc: 0.9454\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.2720 - auc: 0.9537\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.2652 - auc: 0.9560\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.2567 - auc: 0.9590\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.2615 - auc: 0.9569\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.2752 - auc: 0.9520\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.2501 - auc: 0.9610\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 0.2434 - auc: 0.9632\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 0.3032 - auc: 0.9423\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3427 - auc: 0.9303\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3215 - auc: 0.9362\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.2652 - auc: 0.9554\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.2558 - auc: 0.9586\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2451 - auc: 0.9621\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 0.2333 - auc: 0.9659\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.2303 - auc: 0.9666\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.2250 - auc: 0.9683\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2227 - auc: 0.9689\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.2233 - auc: 0.9686\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.2160 - auc: 0.9708\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2136 - auc: 0.9713\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2173 - auc: 0.9702\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2147 - auc: 0.9709\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.2210 - auc: 0.9691\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.2147 - auc: 0.9707\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.2024 - auc: 0.9742\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.2027 - auc: 0.9740\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2049 - auc: 0.9734\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.2069 - auc: 0.9727\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2145 - auc: 0.9707\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2332 - auc: 0.9654\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2229 - auc: 0.9683\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2999 - auc: 0.9480\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.2411 - auc: 0.9637\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2072 - auc: 0.9726\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2183 - auc: 0.9697\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.1972 - auc: 0.9752\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.1931 - auc: 0.9761\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.2059 - auc: 0.9730\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.2020 - auc: 0.9739\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.1942 - auc: 0.9758\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.1852 - auc: 0.9780\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.1834 - auc: 0.9785\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.1853 - auc: 0.9780\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.1886 - auc: 0.9772\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.1761 - auc: 0.9801\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.1723 - auc: 0.9808\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.1725 - auc: 0.9808\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.1715 - auc: 0.9810\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.1797 - auc: 0.9791\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.1843 - auc: 0.9780\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.1767 - auc: 0.9797\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.1738 - auc: 0.9804\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.1903 - auc: 0.9766\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.1767 - auc: 0.9799\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.2000 - auc: 0.9744\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.1784 - auc: 0.9795\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.1701 - auc: 0.9813\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.1659 - auc: 0.9821\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.1608 - auc: 0.9831\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.1627 - auc: 0.9827\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.1653 - auc: 0.9822\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 0.1662 - auc: 0.9819\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.1617 - auc: 0.9828\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.1573 - auc: 0.9838\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.1559 - auc: 0.9840\n"
     ]
    }
   ],
   "source": [
    "nn_model_tunning = nn_modeling.parameter_tuning( { \n",
    "        'input':50,\n",
    "        'layer1':{'s':300, 'activation': 'relu'}, \n",
    "        'layer2':{'s':200, 'activation': 'relu'}, \n",
    "        'layer3':{'s':100, 'activation': 'relu'},\n",
    "        'layer4':{'s':1, 'activation':'sigmoid'},\n",
    "        'loss':'BinaryCrossentropy',\n",
    "        'metric': tf.keras.metrics.AUC(),\n",
    "        'epoch':[10,30,100],\n",
    "        'bs':[10,100,1000,10000], \n",
    "        'optimizer':'adam'\n",
    "        }, NNModel)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Best Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 100,\n",
       " 'epoch': 100,\n",
       " 'loss': 'BinaryCrossentropy',\n",
       " 'metric': <keras.metrics.AUC at 0x7f9f8fae16a0>,\n",
       " 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_nn_model = nn_modeling.find_best_model('auc')\n",
    "{\n",
    "    'batch_size': best_nn_model['model'].batch_size, \n",
    "    'epoch': best_nn_model['model'].epoch,\n",
    "    'loss': best_nn_model['model'].loss,\n",
    "    'metric': best_nn_model['model'].metric,\n",
    "    'optimizer': best_nn_model['model'].optimizer,   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_84 (Dense)             (None, 300)               15300     \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 95,701\n",
      "Trainable params: 95,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_nn_model['model'].model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 0.9932595957474247,\n",
       " 'cost': -0.5254464285714285,\n",
       " 'matrix': array([[66537,   525],\n",
       "        [  254, 44684]])}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ metric: best_nn_model['train_metrics'][metric] for metric in ['auc', 'cost', 'matrix'] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 0.9602827258901632,\n",
       " 'cost': -2.5692708333333334,\n",
       " 'matrix': array([[27719,  1022],\n",
       "        [  845, 18414]])}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ metric: best_nn_model['test_metrics'][metric] for metric in ['auc', 'cost', 'matrix'] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tunning Treshold to for Lowest Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Train Cost</th>\n",
       "      <th>Test Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>10.030804</td>\n",
       "      <td>10.030729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.366295</td>\n",
       "      <td>1.963542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>2.026042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.346429</td>\n",
       "      <td>2.107292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.369643</td>\n",
       "      <td>2.179688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.380357</td>\n",
       "      <td>2.230208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.400893</td>\n",
       "      <td>2.302083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.429241</td>\n",
       "      <td>2.360417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.450893</td>\n",
       "      <td>2.428125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.491295</td>\n",
       "      <td>2.491146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.525446</td>\n",
       "      <td>2.569271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Threshold  Train Cost  Test Cost\n",
       "0        0.00   10.030804  10.030729\n",
       "1        0.05    0.366295   1.963542\n",
       "2        0.10    0.343527   2.026042\n",
       "3        0.15    0.346429   2.107292\n",
       "4        0.20    0.369643   2.179688\n",
       "5        0.25    0.380357   2.230208\n",
       "6        0.30    0.400893   2.302083\n",
       "7        0.35    0.429241   2.360417\n",
       "8        0.40    0.450893   2.428125\n",
       "9        0.45    0.491295   2.491146\n",
       "10       0.50    0.525446   2.569271"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_proba, test_proba, y_train, y_test, conf_train, conf_test = extract_best_model_metrics(nn_modeling)\n",
    "nn_cost_results = tune_cost_proba(1-train_proba, 1-test_proba, y_train, y_test, conf_train, conf_test)\n",
    "nn_cost_results[['Threshold', 'Train Cost','Test Cost' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cost_tunning(nn_cost_results, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, your primary task is to train a model (or models) capable of generalizing on a binary-target that will minimize the monetary loss for your customer and will involve the following steps:\n",
    "\n",
    "- Specify your sampling methodology\n",
    "- Setup your model(s) - highlighting any important parameters\n",
    "- Analyze the performance of your model(s) - referencing your chosen evaluation metric (including supplemental visuals and analysis where appropriate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the results from the three models tried for this dataset and their comparison against predictions using the test dataset.\n",
    "\n",
    "__Logistic Regression:__ This model was the quickest to train and had a result AUC of __0.6718__ and Cost per Prediction of __\\\\$9.73__ for the test dataset.\n",
    " \n",
    "__XGBoost:__ This model was the longest to train and provided a significant improvement compared to the logistic regression. This model had a tendency to overfit showing difference in the train and test results. This model had a result AUC of __0.9434__ and Cost per Prediction of __\\\\$2.40__  for the test dataset.\n",
    "\n",
    " \n",
    "__Neural Network:__ This model took significantly longer than the logistic, but much faster than XGB. It provided a slight improvement over the XGBModel and did not overfit to the training data. This model had a result AUC of __0.9572__ and Cost per Prediction of  __\\\\$2.22__ for the test dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Cost</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Train Cost</th>\n",
       "      <th>conf_test</th>\n",
       "      <th>conf_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.030729</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.030804</td>\n",
       "      <td>[[28741, 0], [19259, 0]]</td>\n",
       "      <td>[[67062, 0], [44938, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.963542</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.366295</td>\n",
       "      <td>[[28154, 587], [1422, 17837]]</td>\n",
       "      <td>[[66934, 128], [1129, 43809]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.026042</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>[[28079, 662], [1242, 18017]]</td>\n",
       "      <td>[[66877, 185], [799, 44139]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.107292</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.346429</td>\n",
       "      <td>[[28013, 728], [1134, 18125]]</td>\n",
       "      <td>[[66835, 227], [644, 44294]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.179688</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.369643</td>\n",
       "      <td>[[27962, 779], [1069, 18190]]</td>\n",
       "      <td>[[66788, 274], [560, 44378]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.230208</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.380357</td>\n",
       "      <td>[[27926, 815], [1022, 18237]]</td>\n",
       "      <td>[[66756, 306], [480, 44458]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.302083</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.400893</td>\n",
       "      <td>[[27882, 859], [984, 18275]]</td>\n",
       "      <td>[[66721, 341], [432, 44506]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.360417</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.429241</td>\n",
       "      <td>[[27844, 897], [944, 18315]]</td>\n",
       "      <td>[[66677, 385], [383, 44555]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.428125</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.450893</td>\n",
       "      <td>[[27802, 939], [906, 18353]]</td>\n",
       "      <td>[[66642, 420], [340, 44598]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.491146</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.491295</td>\n",
       "      <td>[[27764, 977], [875, 18384]]</td>\n",
       "      <td>[[66587, 475], [301, 44637]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.569271</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.525446</td>\n",
       "      <td>[[27719, 1022], [845, 18414]]</td>\n",
       "      <td>[[66537, 525], [254, 44684]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Test Cost  Threshold  Train Cost                      conf_test  \\\n",
       "0   10.030729       0.00   10.030804       [[28741, 0], [19259, 0]]   \n",
       "1    1.963542       0.05    0.366295  [[28154, 587], [1422, 17837]]   \n",
       "2    2.026042       0.10    0.343527  [[28079, 662], [1242, 18017]]   \n",
       "3    2.107292       0.15    0.346429  [[28013, 728], [1134, 18125]]   \n",
       "4    2.179688       0.20    0.369643  [[27962, 779], [1069, 18190]]   \n",
       "5    2.230208       0.25    0.380357  [[27926, 815], [1022, 18237]]   \n",
       "6    2.302083       0.30    0.400893   [[27882, 859], [984, 18275]]   \n",
       "7    2.360417       0.35    0.429241   [[27844, 897], [944, 18315]]   \n",
       "8    2.428125       0.40    0.450893   [[27802, 939], [906, 18353]]   \n",
       "9    2.491146       0.45    0.491295   [[27764, 977], [875, 18384]]   \n",
       "10   2.569271       0.50    0.525446  [[27719, 1022], [845, 18414]]   \n",
       "\n",
       "                       conf_train  \n",
       "0        [[67062, 0], [44938, 0]]  \n",
       "1   [[66934, 128], [1129, 43809]]  \n",
       "2    [[66877, 185], [799, 44139]]  \n",
       "3    [[66835, 227], [644, 44294]]  \n",
       "4    [[66788, 274], [560, 44378]]  \n",
       "5    [[66756, 306], [480, 44458]]  \n",
       "6    [[66721, 341], [432, 44506]]  \n",
       "7    [[66677, 385], [383, 44555]]  \n",
       "8    [[66642, 420], [340, 44598]]  \n",
       "9    [[66587, 475], [301, 44637]]  \n",
       "10   [[66537, 525], [254, 44684]]  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_cost_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below compares the key metrics between the models for the test dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model |Cost Per Prediction | AUC | # False Positives | # False Negatives | \n",
    "|-------|-----|-----|-------------------|-------------------|\n",
    "|Logistic Regression | \\\\$9.73 |  0.6718 | 163 | 18043 |\n",
    "|XGBoost | \\\\$2.40 | 0.9434 | 452 | 2797 |\n",
    "|Neural Network | \\\\$1.96  | 0.9603 | 587 | 1422 |\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "models = ['Logistic Regression', 'XGBoost', 'Neural Network']\n",
    "costs = [9.73, 2.40, 1.96]\n",
    "ax.bar(models, costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion <a id='conclusion'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhUWTUQleFn-"
   },
   "source": [
    "## Final Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The team recommends using the Neural Network model. The model selected has an input layer and 3 hidden layers, with 300, 200, and 100 neurons respectively that use 'Relu' for the activation function and 1 output layer that uses sigmoid for its activation function. This model provided the best fit (auc), which was then tuned to for lowest cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monetary Outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the expected monetary cost (or loss) associated with your model and how might you best translate this to your customer?  Remember, predicting class 1 incorrectly costs the customer \\\\$100  while incorrectly predicting class 0 costs the customer \\\\$25; or said another way, False Positives = -\\\\$100 and False Negatives = -\\\\$25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the stakeholder is not interested in the key features for prediction, the below plot depicts the top 15 feature for predictions using the XGBoost model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_tuned = linear_modeling.find_best_model('auc')\n",
    "\n",
    "feat_coef = []\n",
    "feat = zip(linear_modeling.X_train.columns, lr_tuned['model'].coef_[0])\n",
    "[feat_coef.append([i,j]) for i,j in feat]\n",
    "feat_coef = pd.DataFrame(feat_coef, columns = ['feature','coef'])\n",
    "top_feat_lr = feat_coef.loc[abs(feat_coef['coef'])>0].sort_values(by='coef')\n",
    "\n",
    "feat_plot = sns.barplot(data=top_feat_lr, x='feature', y='coef', palette = \"ch:s=.25,rot=-.25\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('LR Feature Importance with L1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbAhMB1x8g_e"
   },
   "source": [
    "## todo add xgboost model feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of your technical analysis and modeling; what are you proposing to your audience and why?  How should they view your results and what should they consider when moving forward?  Are there other approaches you'd recommend exploring?  This is where you \"bring it all home\" in language they understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oX8fXYczN5D-"
   },
   "source": [
    "### Future Considerations, Model Enhancements and Alternative Modeling Approaches <a id='model-enhancements'/>\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0yVc5DKeFn_"
   },
   "source": [
    "## References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Case_Study_6_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
