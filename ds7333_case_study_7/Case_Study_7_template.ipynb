{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoCXzNvN8g-8"
   },
   "source": [
    "# Case Study 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBy24RcB8g-9"
   },
   "source": [
    "__Team Members:__ Amber Clark, Andrew Leppla, Jorge Olmos, Paritosh Rai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4O0up-U8g-9"
   },
   "source": [
    "# Content\n",
    "* [Business Understanding](#business-understanding)\n",
    "    - [Introduction](#introduction)\n",
    "    - [Methods](#methods)\n",
    "    - [Results](#results)\n",
    "* [Data Evaluation](#data-evaluation)\n",
    "    - [Loading Data](#loading-data) \n",
    "    - [Data Summary](#data-summary)\n",
    "    - [Missing Values](#missing-values)\n",
    "    - [Exploratory Data Analysis (EDA)](#eda)\n",
    "* [Model Preparations](#model-preparations)\n",
    "    - [Sampling & Scaling Data](#sampling-scaling-data)\n",
    "    - [Proposed Method](#proposed-metrics)\n",
    "    - [Evaluation Metrics](#evaluation-metrics)\n",
    "    - [Feature Selection](#feature-selection)\n",
    "* [Model Building & Evaluations](#model-building)\n",
    "    - [Performance Analysis](#performance-analysis)\n",
    "* [Model Interpretability & Explainability](#model-explanation)\n",
    "    - [Examining Feature Importance](#examining-feature-importance)\n",
    "* [Conclusion](#conclusion)\n",
    "    - [Final Model Proposal](#final-model-proposal)\n",
    "    - [Future Considerations, Model Enhancements and Alternative Modeling Approaches](#model-enhancements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRedT-FB8g_A"
   },
   "source": [
    "# Business Understanding & Executive Summary <a id='business-understanding'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-4BiuuQOEh4"
   },
   "source": [
    "## Objective:\n",
    "\n",
    "The objective of this case study is to classify a binary target in an anonymous dataset with the goal of reducing monetary losses as much as possible for the customer.\n",
    "\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Modeling:\n",
    "\n",
    "### Training and Test Split\n",
    "\n",
    "\n",
    "### Key Metrics\n",
    "       \n",
    "\n",
    "### Model Building\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "\n",
    "\n",
    "## Future Considerations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVtcYu5j8g_B"
   },
   "source": [
    "# Data Evaluation <a id='data-evaluation'>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Owfb6XnGfPKI"
   },
   "source": [
    "Summarize the data being used in the case using appropriate mediums (charts, graphs, tables); address questions such as: Are there missing values? Which variables are needed (which ones are not)? What assumptions or conclusions are you drawing that need to be relayed to your audience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WcvKI3y8g_C"
   },
   "source": [
    "## Loading Data <a id='loading-data'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import re\n",
    "import os\n",
    "from IPython.display import Image\n",
    "from abc import ABC, abstractmethod\n",
    "import time\n",
    "#import sklearn\n",
    "#import time\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tabulate import tabulate\n",
    "from IPython.display import clear_output\n",
    "import xgboost\n",
    "\n",
    "# data pre-processing\n",
    "from scipy.io import arff\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.impute._base import _BaseImputer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection._split import BaseShuffleSplit\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# prediction models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm._base import BaseSVC \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "\n",
    "# import warnings filter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from warnings import simplefilter \n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilePathManager:\n",
    "    def __init__(self, local_dir: str):\n",
    "        self.local_dir = local_dir\n",
    "    \n",
    "    def retrieve_full_path(self):\n",
    "        return os.getcwd()+'/'+self.local_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    def load_data(self, file_name):\n",
    "        pass\n",
    "    \n",
    "    def get_df(self):\n",
    "        pass\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    " \n",
    "class CSVLoader(Loader):\n",
    "    def __init__(self, file_path_manager: FilePathManager):\n",
    "        self.file_path_manager = file_path_manager\n",
    "        \n",
    "    def load_data(self, _prepare_data: Callable[[pd.DataFrame], pd.DataFrame] = None):\n",
    "        self.df = pd.read_csv(self.file_path_manager.retrieve_full_path())\n",
    "        if _prepare_data:\n",
    "            self.df = _prepare_data(self.df)\n",
    "    \n",
    "    def get_df(self):\n",
    "        return self.df;\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df['y'] = df['y'].astype(int)\n",
    "    df['x32'] = df['x32'].str.replace('%','').astype(float)\n",
    "    df['x37'] = df['x37'].str.replace('$','').astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(FilePathManager('final_project(5).csv'))\n",
    "loader.load_data(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ul_6nw48N5Dy"
   },
   "source": [
    "## Data Summary <a id='data-summary'>\n",
    "    \n",
    "The dataset consists of fifty (50) features and a binary target class. There is no metadata or other descriptive information for the dataset, and the fifty feature labels are numbered from \"x0\" to \"x49\". There are 160,000 observations in the dataset; less than 0.03% of the features were missing data, and the imputation of these missing values is described below in the Missing Data section. Most of the features provided are numeric, but five were initially imported as text features.\n",
    "    \n",
    "Three of the five text features were identified as continents, months of the year, and days of the week. The values were cleaned up for spelling correction and consistency. The other two text object columns were numeric columns with a special character introduced in the data; column x32 had a trailing \"%\" and column x37 had a leading \"$\". These characters were removed so that these columns would be treated as numeric.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aws5HAx98g_E"
   },
   "source": [
    "## Missing Values <a id='missing-values'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the variables, except the target class, had missing values. The chart below depicts the number of observations missing values for each feature. Note: Even though the plot doesn't show missing values for categorical features, they do have missing values which are represented as nan's and so are missing from the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/olmosjorge28/QTW-SPRING-2022/main/ds7333_case_study_7/visuals/missing_values.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of missing values was consistently around 20-40 missing observations for each column (less than 0.03% of 160,000 observations). For the logistic regression and neural network models, the mean of each column was used to impute the missing values for the numeric data, and the mode of each column was used for the missing categorical features.  For the XGBoost model, the algorithm can automatically handle missing values and find their optimal split for modeling, so no imputation was done prior to modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbAmkozvN5Dz"
   },
   "source": [
    "## Exploratory Data Analysis (EDA) <a id='eda'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numeric data was examined to view the scales of the variables, and the data needs normalization to be effectively used in most types of models without issues. \n",
    "\n",
    "For two model types, logistic regression and neural network, the categorical data for the three text columns were one-hot encoded to produce binary features for each of the values within those variables. In this data, there were three continents, twelve months, and five days of the week, so the one-hot encoding process did not contribute to creating an excess of sparsity in the dataframe that would be used for modeling. After one-hot encoding, the total number of explanatory features has increased to 67.\n",
    "For the third model type, XGBoost, the categorical data were not one-hot encoded but rather label-encoded so the tree-based algorithm could split the data effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance of Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target classes are considered balanced in the dataset, with roughly 40:60 split between the positive and negative classes, as depicted below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/olmosjorge28/QTW-SPRING-2022/main/ds7333_case_study_7/visuals/y_dist.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmuI_mep8g_b"
   },
   "source": [
    "# Model Preparations <a id='model-preparations'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseImputer:\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X):\n",
    "        pass\n",
    "\n",
    "class BaseModel:\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modeling:\n",
    "    _X_train_fitted = None\n",
    "    _X_test_fitted = None\n",
    "    _y_train = None\n",
    "    _y_test = None\n",
    "    _y_preds = None\n",
    "    _y_preds_proba = None\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, \n",
    "                 target_name: str, \n",
    "                 shuffle_splitter: BaseShuffleSplit, \n",
    "                 imputer: BaseImputer, \n",
    "                 model: BaseModel, scaler = None, encoder = None):\n",
    "        self._data = data\n",
    "        self._target_name = target_name\n",
    "        self._shuffle_splitter = shuffle_splitter\n",
    "        self._imputer = imputer\n",
    "        self._model = model\n",
    "        self._encoder = encoder\n",
    "        self._X, self._y = self._split_data()\n",
    "        self._scaler = scaler\n",
    "        \n",
    "    @property\n",
    "    def X(self):\n",
    "        return self._X\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._y\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "    \n",
    "    @model.setter\n",
    "    def model(self, model):\n",
    "        self._model = model\n",
    "     \n",
    "    @property\n",
    "    def X_train(self):\n",
    "        return self._X_train_fitted\n",
    "    \n",
    "    @property\n",
    "    def X_test(self):\n",
    "        return self._X_test_fitted\n",
    "    \n",
    "    @property\n",
    "    def y_train(self):\n",
    "        return self._y_train\n",
    "    \n",
    "    @property\n",
    "    def y_test(self):\n",
    "        return self._y_test\n",
    "    \n",
    "    @property\n",
    "    def y_preds(self):\n",
    "        return self._y_preds\n",
    "    \n",
    "    def _split_data(self):\n",
    "        X = self._data.copy()\n",
    "        return X.drop([self._target_name], axis=1) , X[self._target_name]\n",
    "    \n",
    "    def _shuffle_split(self):\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        for train_index, test_index in self._shuffle_splitter.split(X,y):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def _fit_imputer(self, train):\n",
    "        if self._imputer is not None:\n",
    "            self._imputer.fit(train)\n",
    "    \n",
    "    def _fit_scaler(self, train, cont_vars = None):\n",
    "        transform_cols = None\n",
    "        if cont_vars is None:\n",
    "            transform_cols = self.X.columns\n",
    "        else:\n",
    "            transform_cols = cont_vars\n",
    "            \n",
    "        if self._scaler is not None:\n",
    "            self._scaler.fit(train[transform_cols])\n",
    "    \n",
    "    def _impute_data(self, X: pd.DataFrame):\n",
    "        if self._imputer is not None:\n",
    "            return pd.DataFrame(self._imputer.transform(X), columns = self.X.columns, index = X.index)\n",
    "        return X\n",
    "    \n",
    "    def _scale_data(self, X: pd.DataFrame, cont_vars = None):\n",
    "        transform_cols = None\n",
    "        if cont_vars is None:\n",
    "            transform_cols = X.columns\n",
    "        else:\n",
    "            transform_cols = cont_vars\n",
    "        scaled_data = X[transform_cols]\n",
    "        if self._scaler is not None:\n",
    "            scaled_data = pd.DataFrame(self._scaler.transform(X[transform_cols]), columns = transform_cols)\n",
    "        X[transform_cols] = scaled_data\n",
    "        return X\n",
    "    \n",
    "    def _encode_data(self):\n",
    "        df = self.X.copy()\n",
    "        cont_vars = df.describe().columns\n",
    "        cat_vars = set(df.columns) - set(cont_vars)\n",
    "        for column in [*cat_vars]:\n",
    "            df[column] = self._encoder.fit_transform(df[column].astype(str))\n",
    "        self._X = df\n",
    "        return cont_vars, cat_vars\n",
    "        \n",
    "    \n",
    "    def prepare(self):\n",
    "        cont_vars = None\n",
    "        if self._encoder is not None: \n",
    "            cont_vars, _ = self._encode_data()\n",
    "        X_train, X_test, y_train, y_test = self._shuffle_split()   \n",
    "        self._fit_imputer(X_train)\n",
    "        X_train = self._impute_data(X_train)\n",
    "        X_test = self._impute_data(X_test)\n",
    "        self._fit_scaler(X_train, cont_vars)\n",
    "        self._X_train_fitted = self._scale_data(X_train, cont_vars)\n",
    "        self._X_test_fitted = self._scale_data(X_test, cont_vars)\n",
    "        self._y_train = y_train\n",
    "        self._y_test = y_test\n",
    "        \n",
    "    def prepare_and_train(self):\n",
    "        self.prepare()\n",
    "        return self.train()\n",
    "        \n",
    "    def train(self):\n",
    "        self._model.fit(self.X_train, self.y_train)\n",
    "        self._y_preds = self._model.predict(self.X_train)\n",
    "        self._y_preds_proba = self._model.predict_proba(self.X_train)\n",
    "        \n",
    "        return self.metrics(self.y_train, self.y_preds, self._y_preds_proba)\n",
    "        \n",
    "    def test(self):\n",
    "        return self.metrics(self.y_test, self._model.predict(self.X_test), self._model.predict_proba(self.X_test))\n",
    "       \n",
    "    @abstractmethod\n",
    "    def metrics(self, y_true = None, y_pred = None, y_preds_proba = None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModeling(Modeling):\n",
    "    def __init__(self, \n",
    "                 data: pd.DataFrame, \n",
    "                 target_name: str, \n",
    "                 shuffle_splitter: BaseShuffleSplit, \n",
    "                 imputer: BaseImputer, \n",
    "                 model: BaseModel, \n",
    "                 scaler = None,\n",
    "                 encoder = None,\n",
    "                 beta: int = 1, \n",
    "                 classification: str = 'binary'):\n",
    "        super().__init__(data, target_name, shuffle_splitter, imputer, model, scaler, encoder)\n",
    "        self.beta = beta\n",
    "        self.classification = classification\n",
    "        \n",
    "    @abstractmethod\n",
    "    def metrics(self, y_true = None, y_pred = None, y_preds_proba=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, TypeVar\n",
    "\n",
    "class TuningClassificationModeling(ClassificationModeling):\n",
    "    TClass = None\n",
    "    all_models = [];\n",
    "    \n",
    "    def __init__(self, \n",
    "             data: pd.DataFrame, \n",
    "             target_name: str, \n",
    "             shuffle_splitter: BaseShuffleSplit, \n",
    "             imputer: BaseImputer, \n",
    "             model: BaseModel, \n",
    "             scaler = None,\n",
    "             encoder = None,\n",
    "             beta: int = 1, \n",
    "             classification: str = 'binary',\n",
    "                 classification_type: str = 'logistic'):\n",
    "        super().__init__(data, target_name, shuffle_splitter, imputer, model, scaler, encoder, beta, classification)\n",
    "        if classification_type == 'logistic':\n",
    "            TClass = TypeVar(\"TClass\", bound=LogisticRegression)\n",
    "        elif classification_type == 'xgb':\n",
    "            TClass = TypeVar(\"TClass\", bound=XGBClassifier)\n",
    "        elif classification_type == 'neural':\n",
    "            TClass = TypeVar(\"TClass\", bound=NNModel)\n",
    "            \n",
    "\n",
    "    def parameter_tuning(self, params, class_to_instantiate: Type[TClass]):\n",
    "        list_of_models = []\n",
    "        combination = []\n",
    "        params_base = {}\n",
    "        output = []\n",
    "        for key, value in params.items():\n",
    "            if isinstance(value, list):\n",
    "                combination.append((key,value))\n",
    "            else:\n",
    "                params_base[key]=value\n",
    "        result = {}\n",
    "        if len(combination) > 0:       \n",
    "            result = TuningClassificationModeling.get_combinations(combination)\n",
    "        print(params_base)\n",
    "        for r in result:\n",
    "            list_of_models.append(class_to_instantiate(**{**params_base, **r}))\n",
    "            \n",
    "        for a_model in list_of_models:\n",
    "            self.model = a_model\n",
    "            startTrain = time.time()\n",
    "            train_metrics = self.train()\n",
    "            endTrain = time.time()\n",
    "            test_metrics = self.test()\n",
    "            endTest = time.time()\n",
    "            train_time = endTrain - startTrain\n",
    "            test_time = endTest - endTrain\n",
    "            output.append({'model': a_model, 'train_metrics': {**train_metrics,**{'elapsed_time':train_time}}, 'test_metrics': {**test_metrics,**{'elapsed_time':test_time}}})\n",
    "        self.all_models = output\n",
    "        return output\n",
    "        \n",
    "    def find_best_model(self, metric):\n",
    "        max_accuracy = self.all_models[0]['test_metrics'][metric]\n",
    "        location = 0\n",
    "        for indx, output_metrics in enumerate(self.all_models):\n",
    "            if max_accuracy < output_metrics['test_metrics'][metric]:\n",
    "                max_accuracy = output_metrics['test_metrics'][metric]\n",
    "                location = indx\n",
    "            elif max_accuracy == output_metrics['test_metrics'][metric]:\n",
    "                if output_metrics['test_metrics']['elapsed_time'] < self.all_models[location]['test_metrics']['elapsed_time']:\n",
    "                    location = indx\n",
    "                \n",
    "        return self.all_models[location]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_combinations(tuples):\n",
    "        length = len(tuples)\n",
    "        if length > 1:\n",
    "            total_params = []\n",
    "            tuple_copy = tuples.copy()\n",
    "            a_tuple = tuple_copy.pop(0)\n",
    "            params_list = TuningClassificationModeling.get_combinations(tuple_copy)\n",
    "            for value in a_tuple[1]:\n",
    "                for a_params in params_list:\n",
    "                    temp = { a_tuple[0]: value}\n",
    "                    total_params.append({**temp, **a_params})\n",
    "            return total_params\n",
    "        else:\n",
    "            params_list = []\n",
    "            a_tuple =  tuples[0]\n",
    "            for value in a_tuple[1]:\n",
    "                temp = {}\n",
    "                temp[a_tuple[0]] = value\n",
    "                params_list.append(temp)\n",
    "            return params_list\n",
    "            \n",
    "    \n",
    "    def metrics(self, y_true = None, y_pred = None, y_pred_proba=None):\n",
    "        if y_true is None and y_pred is None:\n",
    "            y_true = self.y_train\n",
    "            y_pred = self.y_preds\n",
    "        conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        return  {\n",
    "                'matrix': conf_matrix, \n",
    "                'auc': roc_auc_score(y_true, y_pred),\n",
    "                'accuracy': round(accuracy_score(y_true, y_pred), 5), \n",
    "                'precision': precision_score(y_true, y_pred, average=self.classification), \n",
    "                'recall': recall_score(y_true, y_pred, average=self.classification),\n",
    "                'f1': f1_score(y_true, y_pred),\n",
    "                'cost': TuningClassificationModeling.cost_calc(conf_matrix),\n",
    "                'y_pred': y_pred,\n",
    "                'y_pred_proba': y_pred_proba\n",
    "               }\n",
    "    \n",
    "    @staticmethod\n",
    "    def cost_calc(conf_matrix):\n",
    "        cost_matrix = np.array([[0,-100],[-25,0]])\n",
    "        cost = np.sum(cost_matrix*conf_matrix)/np.sum(conf_matrix)\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel:\n",
    "    model = None\n",
    "    epoch = 50\n",
    "    batch_size = 32\n",
    "    loss = 'BinaryCrossentropy',\n",
    "    metric = 'accuracy'\n",
    "    optimizer = 'adam'\n",
    "    \n",
    "    def __init__(self,**inputs):\n",
    "        self.model = tf.keras.Sequential()\n",
    "        for arg, content in inputs.items():\n",
    "            if arg.startswith('input'):\n",
    "                self.model.add( tf.keras.layers.Input( shape=(content,) ) )\n",
    "            if arg.startswith('layer'):\n",
    "                self.model.add( tf.keras.layers.Dense(content['s'], activation = content['activation']) )\n",
    "            if arg == 'epoch':\n",
    "                self.epoch = content\n",
    "            if arg == 'bs':\n",
    "                self.batch_size = content\n",
    "            if arg == 'optimizer':\n",
    "                self.optimizer = content\n",
    "            if arg == 'loss':\n",
    "                self.loss = content\n",
    "            if arg == 'metric':\n",
    "                self.metric = content\n",
    "        self.model.compile(optimizer=self.optimizer, loss=self.loss, metrics=[self.metric])\n",
    "        print(self.model)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y, batch_size=self.batch_size, epochs=self.epoch)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        return pd.Series( (y_pred_proba>0.5).astype(int))\n",
    "        \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        y_pred_proba = self.model.predict(X)\n",
    "        return pd.Series(y_pred_proba.reshape((y_pred_proba.shape[1], y_pred_proba.shape[0]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_cost_proba(train_proba, test_proba, y_train, y_test, conf_train, conf_test):\n",
    "    cost_results = pd.DataFrame()\n",
    "    thresh = 0\n",
    "    for i in range(11):\n",
    "        yhat_train = pd.Series(train_proba < thresh).astype(int)\n",
    "        yhat_test = pd.Series(test_proba < thresh).astype(int)\n",
    "        conf_train = confusion_matrix(y_train, yhat_train)\n",
    "        conf_test = confusion_matrix(y_test, yhat_test)\n",
    "        cost_results = cost_results.append({\"Threshold\": thresh,\n",
    "                                        \"Train Cost\": -TuningClassificationModeling.cost_calc(conf_train),\n",
    "                                        \"Test Cost\":  -TuningClassificationModeling.cost_calc(conf_test)},\n",
    "                                        ignore_index=True)\n",
    "        thresh = thresh + 0.05\n",
    "    return cost_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iDJqPa8fUNp"
   },
   "source": [
    "Which methods are you proposing to utilize to solve the problem?  Why is this method appropriate given the business objective? How will you determine if your approach is useful (or how will you differentiate which approach is more useful than another)?  More specifically, what evaluation metrics are most useful given that the problem is a binary-classification one (ex., Accuracy, F1-score, Precision, Recall, AUC, etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final metric used for model evaluation was Cost per Prediction.  This was calculated as follows:\n",
    "\n",
    "Cost per Prediction = (- \\\\$100×FP - \\\\$ 25×FN)/(Total # Predictions)  \n",
    "where FP = false positive, FN = false negative.\n",
    "\n",
    "The cost of a false positive (predicting 1 when it is actually 0) is \\\\$100, and the cost of a false negative (predicting 0 when it is actually 1) is \\\\$25.  These costs are normalized by the total number of predictions so the costs can be compared between training and test sets and fairly assessed for any number of future predictions. \n",
    "\n",
    "\n",
    "Before evaluating the model(s) for Cost per Prediction, the models were tuned to maximize ROC Area Under the Curve (AUC).  The ROC (Receiver Operator Characteristic) curve plots the True Positive (TP) rate vs. the False Positive (FP) rate.  The Area Under this Curve typically has a range of 0.5 to 1.0.  A 50:50 random guess for classification would give an AUC = 0.5 with a diagonal line going from the lower left to upper right.  A perfect (ideal) classifier would have an AUC = 1.0 with a line that goes straight up and then straight across. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/olmosjorge28/QTW-SPRING-2022/main/ds7333_case_study_7/visuals/ROC_AUC_curve.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC was chosen as a standard metric that was quickly and easily implemented during initial model building and assessment.  AUC was an appropriate metric given that the target classes are fairly balanced (40:60), and AUC is also independent of the prediction threshold which is discussed in the following paragraph.\n",
    "\n",
    "Once the models were assessed for AUC, they were further tuned to minimize Cost per Prediction.  This was done by adjusting the probability threshold for predicting a positive (1) vs. negative (0) class.  The default threshold is 0.5 such that a probability < 0.5 is predicted as a negative class and ≥ 0.5 is predicted as a positive class.  This threshold can be adjusted away from 0.5 such that more positive or negative classes are predicted.  In this way, the number of FPs vs. FNs can be adjusted to minimize the Cost per Prediction.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuRjMsjg8g_d"
   },
   "source": [
    "# Model Building & Evaluations <a id='model-building'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and test sets were created from the data using the stratified splitting method to maintain the ratio of the binary outcome, although the class is relatively balanced between the two outcomes. 30% of the data was withheld for the test set, and the explanatory features were normalized using StandardScaler while avoiding data leakage into the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, logistic regression was run as a baseline model with fast implementation and high interpretability.  This model did not necessarily satisfy the customer requirements of minimizing cost, but it served as a starting point to increase model complexity and improve the model performance.  L1 (Lasso) regularization was used for feature selection with the logistic regression model.         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_modeling = TuningClassificationModeling(loader.get_df(),'y',\n",
    "                                           StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=12343),\n",
    "                                           SimpleImputer(missing_values=np.nan, strategy='mean'), LogisticRegression, None, LabelEncoder(), beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_modeling.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1', 'random_state': 1, 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "linear_result = linear_modeling.parameter_tuning( { \n",
    "    'penalty':'l1',\n",
    "    'random_state':1,\n",
    "    'solver': 'liblinear',\n",
    "    'C':  [0.001, 0.01, 1, 10],\n",
    " }, LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': LogisticRegression(C=0.001, penalty='l1', random_state=1, solver='liblinear'),\n",
       " 'train_metrics': {'matrix': array([[55175, 11887],\n",
       "         [21306, 23632]]),\n",
       "  'auc': 0.6743131085040095,\n",
       "  'accuracy': 0.70363,\n",
       "  'precision': 0.6653340465666263,\n",
       "  'recall': 0.5258801014731408,\n",
       "  'f1': 0.5874442248654561,\n",
       "  'cost': -15.369196428571428,\n",
       "  'y_pred': array([1, 1, 0, ..., 0, 0, 1]),\n",
       "  'y_pred_proba': array([[0.1317954 , 0.8682046 ],\n",
       "         [0.30796321, 0.69203679],\n",
       "         [0.62385404, 0.37614596],\n",
       "         ...,\n",
       "         [0.83326334, 0.16673666],\n",
       "         [0.64663224, 0.35336776],\n",
       "         [0.31716649, 0.68283351]]),\n",
       "  'elapsed_time': 5.558510065078735},\n",
       " 'test_metrics': {'matrix': array([[23539,  5202],\n",
       "         [ 9156, 10103]]),\n",
       "  'auc': 0.6717950589503955,\n",
       "  'accuracy': 0.70088,\n",
       "  'precision': 0.6601110748121529,\n",
       "  'recall': 0.5245859078872216,\n",
       "  'f1': 0.5845966901978937,\n",
       "  'cost': -15.60625,\n",
       "  'y_pred': array([1, 0, 0, ..., 0, 1, 0]),\n",
       "  'y_pred_proba': array([[0.33479881, 0.66520119],\n",
       "         [0.56876588, 0.43123412],\n",
       "         [0.85378871, 0.14621129],\n",
       "         ...,\n",
       "         [0.72475699, 0.27524301],\n",
       "         [0.08520023, 0.91479977],\n",
       "         [0.86343798, 0.13656202]]),\n",
       "  'elapsed_time': 0.10918092727661133}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_modeling.find_best_model('auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proba = linear_modeling.find_best_model('auc')['train_metrics']['y_pred_proba']\n",
    "test_proba = linear_modeling.find_best_model('auc')['test_metrics']['y_pred_proba']\n",
    "conf_train = linear_modeling.find_best_model('auc')['train_metrics']['matrix']\n",
    "conf_test = linear_modeling.find_best_model('auc')['test_metrics']['matrix']\n",
    "   \n",
    "cost_results = tune_cost_proba(train_proba[:,0], test_proba[:,0], linear_modeling.y_train, linear_modeling.y_test, conf_train, conf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, XGBoost (eXtreme Gradient Boosting) was used as a more complex nonlinear tree-based model.  This model significantly improved performance while maintaining some interpretability with feature importances.  The XGBoost model overfit the training set such that it achieved a perfect AUC=1.0, and this resulted in a maximum test AUC=XX.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = TuningClassificationModeling(loader.get_df(),'y',\n",
    "                                           StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=12343),\n",
    "                                           None, XGBClassifier, None, LabelEncoder(), beta=1,classification_type = 'xgb' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "[19:31:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:31:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:31:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:32:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:35:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:37:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:42:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:43:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:43:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:44:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:47:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:50:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:55:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:55:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:56:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:59:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:04:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:09:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:20:07] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:20:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:21:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:24:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:30:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:35:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:46:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:47:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:49:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:53:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:02:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:11:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:26:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:27:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:29:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:32:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:39:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:45:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb_results = xgb_classifier.parameter_tuning( { \n",
    "    'max_depth': [3,6,10],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    " }, XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=0.7,\n",
       "               enable_categorical=False, gamma=0, gpu_id=-1,\n",
       "               importance_type=None, interaction_constraints='',\n",
       "               learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "               n_estimators=1000, n_jobs=8, num_parallel_tree=1,\n",
       "               predictor='auto', random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "               scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "               validate_parameters=1, verbosity=None),\n",
       " 'train_metrics': {'matrix': array([[67062,     0],\n",
       "         [    0, 44938]]),\n",
       "  'auc': 1.0,\n",
       "  'accuracy': 1.0,\n",
       "  'precision': 1.0,\n",
       "  'recall': 1.0,\n",
       "  'f1': 1.0,\n",
       "  'cost': 0.0,\n",
       "  'y_pred': array([0, 0, 0, ..., 0, 0, 1]),\n",
       "  'y_pred_proba': array([[9.9518055e-01, 4.8194379e-03],\n",
       "         [9.9975085e-01, 2.4912396e-04],\n",
       "         [9.5752424e-01, 4.2475760e-02],\n",
       "         ...,\n",
       "         [9.9999070e-01, 9.3061881e-06],\n",
       "         [9.9998665e-01, 1.3345468e-05],\n",
       "         [3.6323071e-04, 9.9963677e-01]], dtype=float32),\n",
       "  'elapsed_time': 794.1996490955353},\n",
       " 'test_metrics': {'matrix': array([[27657,  1084],\n",
       "         [ 1452, 17807]]),\n",
       "  'auc': 0.9434452613876319,\n",
       "  'accuracy': 0.94717,\n",
       "  'precision': 0.9426181779683447,\n",
       "  'recall': 0.9246066773975804,\n",
       "  'f1': 0.9335255570117955,\n",
       "  'cost': -3.0145833333333334,\n",
       "  'y_pred': array([0, 1, 1, ..., 0, 1, 0]),\n",
       "  'y_pred_proba': array([[7.5970745e-01, 2.4029252e-01],\n",
       "         [2.8648430e-01, 7.1351570e-01],\n",
       "         [2.0623207e-05, 9.9997938e-01],\n",
       "         ...,\n",
       "         [9.9993175e-01, 6.8268790e-05],\n",
       "         [2.0265579e-06, 9.9999797e-01],\n",
       "         [9.9825048e-01, 1.7495404e-03]], dtype=float32),\n",
       "  'elapsed_time': 1.11393404006958}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_classifier.find_best_model('auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proba = xgb_classifier.find_best_model('auc')['train_metrics']['y_pred_proba']\n",
    "test_proba = xgb_classifier.find_best_model('auc')['test_metrics']['y_pred_proba']\n",
    "conf_train = xgb_classifier.find_best_model('auc')['train_metrics']['matrix']\n",
    "conf_test = xgb_classifier.find_best_model('auc')['test_metrics']['matrix']\n",
    "   \n",
    "cost_results = tune_cost_proba(train_proba[:,0], test_proba[:,0], xgb_classifier.y_train, xgb_classifier.y_test, conf_train, conf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Cost</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Train Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.030729</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.030804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.707813</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.033705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.445833</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.398438</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.398438</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.470312</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.526562</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.616146</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.752083</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.850521</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.014583</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Test Cost  Threshold  Train Cost\n",
       "0   10.030729       0.00   10.030804\n",
       "1    2.707813       0.05    0.033705\n",
       "2    2.445833       0.10    0.000223\n",
       "3    2.398438       0.15   -0.000000\n",
       "4    2.398438       0.20   -0.000000\n",
       "5    2.470312       0.25   -0.000000\n",
       "6    2.526562       0.30   -0.000000\n",
       "7    2.616146       0.35   -0.000000\n",
       "8    2.752083       0.40   -0.000000\n",
       "9    2.850521       0.45   -0.000000\n",
       "10   3.014583       0.50   -0.000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a Neural Network model was fit on the dataset, and its performance was compared against the rest of the models.  This was the most complex model with the least interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_modeling = TuningClassificationModeling(loader.get_df(),'y',\n",
    "                                           StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=12343),\n",
    "                                           SimpleImputer(missing_values=np.nan, strategy='mean'), NNModel, None, LabelEncoder(), beta=1,classification_type='neural' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_modeling.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 50, 'layer1': {'s': 300, 'activation': 'relu'}, 'layer2': {'s': 200, 'activation': 'relu'}, 'layer3': {'s': 100, 'activation': 'relu'}, 'layer4': {'s': 1, 'activation': 'sigmoid'}, 'loss': 'BinaryCrossentropy', 'metric': 'accuracy', 'optimizer': 'adam'}\n",
      "<keras.engine.sequential.Sequential object at 0x7fa02d044be0>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9fc1fa55e0>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9fc22127c0>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9fc221b3d0>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9fc222fee0>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9fc2237be0>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9fc22373d0>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9fc2485a00>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9fc247ffa0>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9fc4016be0>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9fc2470070>\n",
      "<keras.engine.sequential.Sequential object at 0x7f9fc402bf40>\n",
      "Epoch 1/10\n",
      "11200/11200 [==============================] - 20s 2ms/step - loss: 0.4673 - accuracy: 0.8281\n",
      "Epoch 2/10\n",
      "11200/11200 [==============================] - 20s 2ms/step - loss: 0.2306 - accuracy: 0.9089\n",
      "Epoch 3/10\n",
      "11200/11200 [==============================] - 17s 1ms/step - loss: 0.1920 - accuracy: 0.9276\n",
      "Epoch 4/10\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1726 - accuracy: 0.9375\n",
      "Epoch 5/10\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.1608 - accuracy: 0.9429\n",
      "Epoch 6/10\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.1528 - accuracy: 0.9469\n",
      "Epoch 7/10\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1480 - accuracy: 0.9488\n",
      "Epoch 8/10\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1405 - accuracy: 0.9519\n",
      "Epoch 9/10\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.1371 - accuracy: 0.9535\n",
      "Epoch 10/10\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1331 - accuracy: 0.9551\n",
      "Epoch 1/10\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.8188 - accuracy: 0.7823\n",
      "Epoch 2/10\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.3041 - accuracy: 0.8732\n",
      "Epoch 3/10\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.2643 - accuracy: 0.8928\n",
      "Epoch 4/10\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.2330 - accuracy: 0.9073\n",
      "Epoch 5/10\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.2062 - accuracy: 0.9201\n",
      "Epoch 6/10\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9306\n",
      "Epoch 7/10\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1646 - accuracy: 0.9388\n",
      "Epoch 8/10\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1492 - accuracy: 0.9458\n",
      "Epoch 9/10\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1378 - accuracy: 0.9502\n",
      "Epoch 10/10\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1299 - accuracy: 0.9549\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.2836 - accuracy: 0.7116\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.4833 - accuracy: 0.7929\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.4759 - accuracy: 0.8152\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.3295 - accuracy: 0.8635\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.3123 - accuracy: 0.8719\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.2812 - accuracy: 0.8851\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.2661 - accuracy: 0.8927\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.2459 - accuracy: 0.9014\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.2325 - accuracy: 0.9066\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.2315 - accuracy: 0.9086\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 10.6013 - accuracy: 0.5233\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 4.0213 - accuracy: 0.5762\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 1s 46ms/step - loss: 1.4020 - accuracy: 0.6620\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.7130 - accuracy: 0.7184\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.5713 - accuracy: 0.7431\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.4996 - accuracy: 0.7686\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.4785 - accuracy: 0.7777\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 0.4501 - accuracy: 0.7950\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 0.4576 - accuracy: 0.7921\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 0.4509 - accuracy: 0.7971\n",
      "Epoch 1/30\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.4474 - accuracy: 0.8311\n",
      "Epoch 2/30\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.2240 - accuracy: 0.9128\n",
      "Epoch 3/30\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.1884 - accuracy: 0.9298\n",
      "Epoch 4/30\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1700 - accuracy: 0.9379\n",
      "Epoch 5/30\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1572 - accuracy: 0.9441\n",
      "Epoch 6/30\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1489 - accuracy: 0.9485\n",
      "Epoch 7/30\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.1415 - accuracy: 0.9512\n",
      "Epoch 8/30\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1375 - accuracy: 0.9533\n",
      "Epoch 9/30\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.1328 - accuracy: 0.9559\n",
      "Epoch 10/30\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.1296 - accuracy: 0.9568\n",
      "Epoch 11/30\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.1251 - accuracy: 0.9588\n",
      "Epoch 12/30\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.1224 - accuracy: 0.9590\n",
      "Epoch 13/30\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.1190 - accuracy: 0.9606\n",
      "Epoch 14/30\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.1157 - accuracy: 0.9618\n",
      "Epoch 15/30\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.1147 - accuracy: 0.9628\n",
      "Epoch 16/30\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.1131 - accuracy: 0.9640\n",
      "Epoch 17/30\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1112 - accuracy: 0.9651\n",
      "Epoch 18/30\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1102 - accuracy: 0.9648\n",
      "Epoch 19/30\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1075 - accuracy: 0.9662\n",
      "Epoch 20/30\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.1057 - accuracy: 0.9669\n",
      "Epoch 21/30\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.1041 - accuracy: 0.9670\n",
      "Epoch 22/30\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.1009 - accuracy: 0.9690\n",
      "Epoch 23/30\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1017 - accuracy: 0.9680\n",
      "Epoch 24/30\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1022 - accuracy: 0.9689\n",
      "Epoch 25/30\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0988 - accuracy: 0.9694\n",
      "Epoch 26/30\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0973 - accuracy: 0.9695\n",
      "Epoch 27/30\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0956 - accuracy: 0.9697\n",
      "Epoch 28/30\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0951 - accuracy: 0.9702\n",
      "Epoch 29/30\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0936 - accuracy: 0.9706\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0909 - accuracy: 0.9719\n",
      "Epoch 1/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.6344 - accuracy: 0.7927\n",
      "Epoch 2/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.3072 - accuracy: 0.8718\n",
      "Epoch 3/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.2623 - accuracy: 0.8933\n",
      "Epoch 4/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.2300 - accuracy: 0.9086\n",
      "Epoch 5/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.2079 - accuracy: 0.9195\n",
      "Epoch 6/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9298\n",
      "Epoch 7/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1664 - accuracy: 0.9383\n",
      "Epoch 8/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1509 - accuracy: 0.9454\n",
      "Epoch 9/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1402 - accuracy: 0.9498\n",
      "Epoch 10/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1327 - accuracy: 0.9527\n",
      "Epoch 11/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1244 - accuracy: 0.9567\n",
      "Epoch 12/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1172 - accuracy: 0.9603\n",
      "Epoch 13/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1133 - accuracy: 0.9610\n",
      "Epoch 14/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1064 - accuracy: 0.9642\n",
      "Epoch 15/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1031 - accuracy: 0.9648\n",
      "Epoch 16/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1005 - accuracy: 0.9659\n",
      "Epoch 17/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0949 - accuracy: 0.9681\n",
      "Epoch 18/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0912 - accuracy: 0.9692\n",
      "Epoch 19/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0881 - accuracy: 0.9705\n",
      "Epoch 20/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0845 - accuracy: 0.9719\n",
      "Epoch 21/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0820 - accuracy: 0.9728\n",
      "Epoch 22/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0783 - accuracy: 0.9742\n",
      "Epoch 23/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0765 - accuracy: 0.9744\n",
      "Epoch 24/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0725 - accuracy: 0.9756\n",
      "Epoch 25/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0729 - accuracy: 0.9757\n",
      "Epoch 26/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0689 - accuracy: 0.9774\n",
      "Epoch 27/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0665 - accuracy: 0.9776\n",
      "Epoch 28/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0636 - accuracy: 0.9791\n",
      "Epoch 29/30\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0608 - accuracy: 0.9797\n",
      "Epoch 30/30\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0619 - accuracy: 0.9797\n",
      "Epoch 1/30\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.9727 - accuracy: 0.6984\n",
      "Epoch 2/30\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.4507 - accuracy: 0.7985\n",
      "Epoch 3/30\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.4166 - accuracy: 0.8255\n",
      "Epoch 4/30\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.3996 - accuracy: 0.8444\n",
      "Epoch 5/30\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.3015 - accuracy: 0.8743\n",
      "Epoch 6/30\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.2855 - accuracy: 0.8820\n",
      "Epoch 7/30\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.2743 - accuracy: 0.8880\n",
      "Epoch 8/30\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.2639 - accuracy: 0.8949\n",
      "Epoch 9/30\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.2470 - accuracy: 0.9022\n",
      "Epoch 10/30\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2236 - accuracy: 0.9116\n",
      "Epoch 11/30\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.2378 - accuracy: 0.9064\n",
      "Epoch 12/30\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.2211 - accuracy: 0.9127\n",
      "Epoch 13/30\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.2127 - accuracy: 0.9172\n",
      "Epoch 14/30\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1909 - accuracy: 0.9268\n",
      "Epoch 15/30\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1906 - accuracy: 0.9272\n",
      "Epoch 16/30\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1904 - accuracy: 0.9278\n",
      "Epoch 17/30\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1863 - accuracy: 0.9289\n",
      "Epoch 18/30\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1772 - accuracy: 0.9324\n",
      "Epoch 19/30\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.1793 - accuracy: 0.9320\n",
      "Epoch 20/30\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.1698 - accuracy: 0.9363\n",
      "Epoch 21/30\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1619 - accuracy: 0.9388\n",
      "Epoch 22/30\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1629 - accuracy: 0.9389\n",
      "Epoch 23/30\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1593 - accuracy: 0.9402\n",
      "Epoch 24/30\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1477 - accuracy: 0.9452\n",
      "Epoch 25/30\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1489 - accuracy: 0.9451\n",
      "Epoch 26/30\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1532 - accuracy: 0.9434\n",
      "Epoch 27/30\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1404 - accuracy: 0.9481\n",
      "Epoch 28/30\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1421 - accuracy: 0.9479\n",
      "Epoch 29/30\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1331 - accuracy: 0.9520\n",
      "Epoch 30/30\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1340 - accuracy: 0.9511\n",
      "Epoch 1/30\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 9.3228 - accuracy: 0.5318\n",
      "Epoch 2/30\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 3.9602 - accuracy: 0.5525\n",
      "Epoch 3/30\n",
      "12/12 [==============================] - 1s 46ms/step - loss: 1.8878 - accuracy: 0.6323\n",
      "Epoch 4/30\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 1.0241 - accuracy: 0.6884\n",
      "Epoch 5/30\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 0.7105 - accuracy: 0.7344\n",
      "Epoch 6/30\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.5304 - accuracy: 0.7648\n",
      "Epoch 7/30\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.5347 - accuracy: 0.7526\n",
      "Epoch 8/30\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 0.5242 - accuracy: 0.7598\n",
      "Epoch 9/30\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.4905 - accuracy: 0.7764\n",
      "Epoch 10/30\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.4720 - accuracy: 0.7831\n",
      "Epoch 11/30\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.4312 - accuracy: 0.8046\n",
      "Epoch 12/30\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.4180 - accuracy: 0.8112\n",
      "Epoch 13/30\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.4526 - accuracy: 0.7945\n",
      "Epoch 14/30\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.4774 - accuracy: 0.7877\n",
      "Epoch 15/30\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 0.4218 - accuracy: 0.8108\n",
      "Epoch 16/30\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 0.3923 - accuracy: 0.8265\n",
      "Epoch 17/30\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 0.3986 - accuracy: 0.8263\n",
      "Epoch 18/30\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 0.4382 - accuracy: 0.8091\n",
      "Epoch 19/30\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 0.3645 - accuracy: 0.8401\n",
      "Epoch 20/30\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.3478 - accuracy: 0.8491\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 54ms/step - loss: 0.3365 - accuracy: 0.8546\n",
      "Epoch 22/30\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.3446 - accuracy: 0.8491\n",
      "Epoch 23/30\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 0.3571 - accuracy: 0.8456\n",
      "Epoch 24/30\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.3231 - accuracy: 0.8622\n",
      "Epoch 25/30\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 0.3149 - accuracy: 0.8661\n",
      "Epoch 26/30\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 0.3041 - accuracy: 0.8729\n",
      "Epoch 27/30\n",
      "12/12 [==============================] - 1s 46ms/step - loss: 0.3038 - accuracy: 0.8720\n",
      "Epoch 28/30\n",
      "12/12 [==============================] - 1s 46ms/step - loss: 0.2910 - accuracy: 0.8795\n",
      "Epoch 29/30\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 0.2875 - accuracy: 0.8814\n",
      "Epoch 30/30\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.2815 - accuracy: 0.8841\n",
      "Epoch 1/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.4830 - accuracy: 0.8237\n",
      "Epoch 2/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.2277 - accuracy: 0.9103\n",
      "Epoch 3/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1903 - accuracy: 0.9283\n",
      "Epoch 4/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1728 - accuracy: 0.9374\n",
      "Epoch 5/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.1602 - accuracy: 0.9431\n",
      "Epoch 6/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.1501 - accuracy: 0.9475\n",
      "Epoch 7/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1420 - accuracy: 0.9508\n",
      "Epoch 8/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.1366 - accuracy: 0.9536\n",
      "Epoch 9/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1332 - accuracy: 0.9550\n",
      "Epoch 10/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1284 - accuracy: 0.9571\n",
      "Epoch 11/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1258 - accuracy: 0.9583\n",
      "Epoch 12/100\n",
      "11200/11200 [==============================] - 13s 1ms/step - loss: 0.1216 - accuracy: 0.9601\n",
      "Epoch 13/100\n",
      "11200/11200 [==============================] - 13s 1ms/step - loss: 0.1171 - accuracy: 0.9617\n",
      "Epoch 14/100\n",
      "11200/11200 [==============================] - 13s 1ms/step - loss: 0.1168 - accuracy: 0.9619\n",
      "Epoch 15/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1130 - accuracy: 0.9637\n",
      "Epoch 16/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.1115 - accuracy: 0.9645\n",
      "Epoch 17/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.1083 - accuracy: 0.9657\n",
      "Epoch 18/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1078 - accuracy: 0.9660\n",
      "Epoch 19/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1066 - accuracy: 0.9663\n",
      "Epoch 20/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1040 - accuracy: 0.9671\n",
      "Epoch 21/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1032 - accuracy: 0.9677\n",
      "Epoch 22/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.1000 - accuracy: 0.9689\n",
      "Epoch 23/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.1001 - accuracy: 0.9684\n",
      "Epoch 24/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0972 - accuracy: 0.9697\n",
      "Epoch 25/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0968 - accuracy: 0.9695\n",
      "Epoch 26/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0950 - accuracy: 0.9703\n",
      "Epoch 27/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0911 - accuracy: 0.9715\n",
      "Epoch 28/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0908 - accuracy: 0.9717\n",
      "Epoch 29/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0900 - accuracy: 0.9731\n",
      "Epoch 30/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0891 - accuracy: 0.9722\n",
      "Epoch 31/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0855 - accuracy: 0.9732\n",
      "Epoch 32/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0866 - accuracy: 0.9732\n",
      "Epoch 33/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0843 - accuracy: 0.9740\n",
      "Epoch 34/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0846 - accuracy: 0.9745\n",
      "Epoch 35/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0817 - accuracy: 0.9752\n",
      "Epoch 36/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0808 - accuracy: 0.9749\n",
      "Epoch 37/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0796 - accuracy: 0.9755\n",
      "Epoch 38/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0832 - accuracy: 0.9752\n",
      "Epoch 39/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0786 - accuracy: 0.9759\n",
      "Epoch 40/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0775 - accuracy: 0.9764\n",
      "Epoch 41/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0802 - accuracy: 0.9760\n",
      "Epoch 42/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0861 - accuracy: 0.9765\n",
      "Epoch 43/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0753 - accuracy: 0.9770\n",
      "Epoch 44/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0738 - accuracy: 0.9775\n",
      "Epoch 45/100\n",
      "11200/11200 [==============================] - 13s 1ms/step - loss: 0.0756 - accuracy: 0.9771\n",
      "Epoch 46/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0756 - accuracy: 0.9775\n",
      "Epoch 47/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0763 - accuracy: 0.9774\n",
      "Epoch 48/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0787 - accuracy: 0.9770\n",
      "Epoch 49/100\n",
      "11200/11200 [==============================] - 13s 1ms/step - loss: 0.0723 - accuracy: 0.9778\n",
      "Epoch 50/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0731 - accuracy: 0.9777\n",
      "Epoch 51/100\n",
      "11200/11200 [==============================] - 13s 1ms/step - loss: 0.0702 - accuracy: 0.9793\n",
      "Epoch 52/100\n",
      "11200/11200 [==============================] - 13s 1ms/step - loss: 0.0700 - accuracy: 0.9788\n",
      "Epoch 53/100\n",
      "11200/11200 [==============================] - 13s 1ms/step - loss: 0.0719 - accuracy: 0.9786\n",
      "Epoch 54/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0711 - accuracy: 0.9790\n",
      "Epoch 55/100\n",
      "11200/11200 [==============================] - 13s 1ms/step - loss: 0.0661 - accuracy: 0.9798\n",
      "Epoch 56/100\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.0687 - accuracy: 0.9781\n",
      "Epoch 57/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0668 - accuracy: 0.9788\n",
      "Epoch 58/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0674 - accuracy: 0.9790\n",
      "Epoch 59/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0754 - accuracy: 0.9778\n",
      "Epoch 60/100\n",
      "11200/11200 [==============================] - 13s 1ms/step - loss: 0.0647 - accuracy: 0.9801\n",
      "Epoch 61/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0651 - accuracy: 0.9790\n",
      "Epoch 62/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0663 - accuracy: 0.9794\n",
      "Epoch 63/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0650 - accuracy: 0.9797\n",
      "Epoch 64/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0742 - accuracy: 0.9785\n",
      "Epoch 65/100\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.0624 - accuracy: 0.9800\n",
      "Epoch 66/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0629 - accuracy: 0.9804\n",
      "Epoch 67/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0737 - accuracy: 0.9794\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11200/11200 [==============================] - 13s 1ms/step - loss: 0.0678 - accuracy: 0.9794\n",
      "Epoch 69/100\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.0613 - accuracy: 0.9809\n",
      "Epoch 70/100\n",
      "11200/11200 [==============================] - 13s 1ms/step - loss: 0.0835 - accuracy: 0.9808\n",
      "Epoch 71/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0618 - accuracy: 0.9811\n",
      "Epoch 72/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0619 - accuracy: 0.9802\n",
      "Epoch 73/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0692 - accuracy: 0.9805\n",
      "Epoch 74/100\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.0594 - accuracy: 0.9814\n",
      "Epoch 75/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0646 - accuracy: 0.9813\n",
      "Epoch 76/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0611 - accuracy: 0.9808\n",
      "Epoch 77/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0663 - accuracy: 0.9814\n",
      "Epoch 78/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0579 - accuracy: 0.9815\n",
      "Epoch 79/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0625 - accuracy: 0.9810\n",
      "Epoch 80/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0759 - accuracy: 0.9808\n",
      "Epoch 81/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0567 - accuracy: 0.9824\n",
      "Epoch 82/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0611 - accuracy: 0.9821\n",
      "Epoch 83/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0591 - accuracy: 0.9825\n",
      "Epoch 84/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0569 - accuracy: 0.9824\n",
      "Epoch 85/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0561 - accuracy: 0.9830\n",
      "Epoch 86/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0584 - accuracy: 0.9824\n",
      "Epoch 87/100\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.0534 - accuracy: 0.9833\n",
      "Epoch 88/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0601 - accuracy: 0.9821\n",
      "Epoch 89/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0638 - accuracy: 0.9823\n",
      "Epoch 90/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0629 - accuracy: 0.9819\n",
      "Epoch 91/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0730 - accuracy: 0.9824\n",
      "Epoch 92/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0582 - accuracy: 0.9835\n",
      "Epoch 93/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0612 - accuracy: 0.9826\n",
      "Epoch 94/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0639 - accuracy: 0.9829\n",
      "Epoch 95/100\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.0642 - accuracy: 0.9823\n",
      "Epoch 96/100\n",
      "11200/11200 [==============================] - 16s 1ms/step - loss: 0.0841 - accuracy: 0.9806\n",
      "Epoch 97/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0666 - accuracy: 0.9820\n",
      "Epoch 98/100\n",
      "11200/11200 [==============================] - 15s 1ms/step - loss: 0.0588 - accuracy: 0.9836\n",
      "Epoch 99/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0616 - accuracy: 0.9830\n",
      "Epoch 100/100\n",
      "11200/11200 [==============================] - 14s 1ms/step - loss: 0.0662 - accuracy: 0.9825\n",
      "Epoch 1/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.6132 - accuracy: 0.7964\n",
      "Epoch 2/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.3056 - accuracy: 0.8753\n",
      "Epoch 3/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.2518 - accuracy: 0.8986\n",
      "Epoch 4/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.2190 - accuracy: 0.9140\n",
      "Epoch 5/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9270\n",
      "Epoch 6/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1723 - accuracy: 0.9359\n",
      "Epoch 7/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1545 - accuracy: 0.9439\n",
      "Epoch 8/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1447 - accuracy: 0.9478\n",
      "Epoch 9/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1331 - accuracy: 0.9528\n",
      "Epoch 10/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1235 - accuracy: 0.9568\n",
      "Epoch 11/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1189 - accuracy: 0.9582\n",
      "Epoch 12/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1129 - accuracy: 0.9616\n",
      "Epoch 13/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1080 - accuracy: 0.9635\n",
      "Epoch 14/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.1032 - accuracy: 0.9652\n",
      "Epoch 15/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0987 - accuracy: 0.9666\n",
      "Epoch 16/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0953 - accuracy: 0.9681\n",
      "Epoch 17/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0909 - accuracy: 0.9694\n",
      "Epoch 18/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0892 - accuracy: 0.9702\n",
      "Epoch 19/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0853 - accuracy: 0.9721\n",
      "Epoch 20/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0807 - accuracy: 0.9734\n",
      "Epoch 21/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0787 - accuracy: 0.9738\n",
      "Epoch 22/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0766 - accuracy: 0.9746\n",
      "Epoch 23/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0724 - accuracy: 0.9764\n",
      "Epoch 24/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0724 - accuracy: 0.9760\n",
      "Epoch 25/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0676 - accuracy: 0.9776\n",
      "Epoch 26/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0662 - accuracy: 0.9782\n",
      "Epoch 27/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0632 - accuracy: 0.9790\n",
      "Epoch 28/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0613 - accuracy: 0.9801\n",
      "Epoch 29/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0597 - accuracy: 0.9803\n",
      "Epoch 30/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0578 - accuracy: 0.9811\n",
      "Epoch 31/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0565 - accuracy: 0.9810\n",
      "Epoch 32/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0556 - accuracy: 0.9816\n",
      "Epoch 33/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0528 - accuracy: 0.9823\n",
      "Epoch 34/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0524 - accuracy: 0.9820\n",
      "Epoch 35/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0488 - accuracy: 0.9836\n",
      "Epoch 36/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0486 - accuracy: 0.9834\n",
      "Epoch 37/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0484 - accuracy: 0.9834\n",
      "Epoch 38/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0454 - accuracy: 0.9846\n",
      "Epoch 39/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0441 - accuracy: 0.9853\n",
      "Epoch 40/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0457 - accuracy: 0.9844\n",
      "Epoch 41/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0430 - accuracy: 0.9853\n",
      "Epoch 42/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0433 - accuracy: 0.9855\n",
      "Epoch 43/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0401 - accuracy: 0.9864\n",
      "Epoch 44/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0425 - accuracy: 0.9860\n",
      "Epoch 45/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0384 - accuracy: 0.9868\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0375 - accuracy: 0.9876\n",
      "Epoch 47/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0400 - accuracy: 0.9864\n",
      "Epoch 48/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0359 - accuracy: 0.9877\n",
      "Epoch 49/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0389 - accuracy: 0.9864\n",
      "Epoch 50/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0375 - accuracy: 0.9875\n",
      "Epoch 51/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0332 - accuracy: 0.9887\n",
      "Epoch 52/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0367 - accuracy: 0.9871\n",
      "Epoch 53/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0342 - accuracy: 0.9887\n",
      "Epoch 54/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0352 - accuracy: 0.9884\n",
      "Epoch 55/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0340 - accuracy: 0.9884\n",
      "Epoch 56/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0300 - accuracy: 0.9904\n",
      "Epoch 57/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0336 - accuracy: 0.9886\n",
      "Epoch 58/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0313 - accuracy: 0.9893\n",
      "Epoch 59/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0312 - accuracy: 0.9892\n",
      "Epoch 60/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0312 - accuracy: 0.9893\n",
      "Epoch 61/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0292 - accuracy: 0.9903\n",
      "Epoch 62/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0314 - accuracy: 0.9894\n",
      "Epoch 63/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0281 - accuracy: 0.9908\n",
      "Epoch 64/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0290 - accuracy: 0.9905\n",
      "Epoch 65/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0282 - accuracy: 0.9902\n",
      "Epoch 66/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0306 - accuracy: 0.9898\n",
      "Epoch 67/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0289 - accuracy: 0.9903\n",
      "Epoch 68/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0257 - accuracy: 0.9914\n",
      "Epoch 69/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0280 - accuracy: 0.9907\n",
      "Epoch 70/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0272 - accuracy: 0.9907\n",
      "Epoch 71/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0272 - accuracy: 0.9910\n",
      "Epoch 72/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0261 - accuracy: 0.9910\n",
      "Epoch 73/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0271 - accuracy: 0.9906\n",
      "Epoch 74/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0259 - accuracy: 0.9908\n",
      "Epoch 75/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0253 - accuracy: 0.9915\n",
      "Epoch 76/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0255 - accuracy: 0.9914\n",
      "Epoch 77/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0255 - accuracy: 0.9917\n",
      "Epoch 78/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0257 - accuracy: 0.9918\n",
      "Epoch 79/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0240 - accuracy: 0.9920\n",
      "Epoch 80/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0250 - accuracy: 0.9918\n",
      "Epoch 81/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0251 - accuracy: 0.9915\n",
      "Epoch 82/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0218 - accuracy: 0.9926\n",
      "Epoch 83/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0236 - accuracy: 0.9923\n",
      "Epoch 84/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0247 - accuracy: 0.9918\n",
      "Epoch 85/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0231 - accuracy: 0.9926\n",
      "Epoch 86/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0239 - accuracy: 0.9917\n",
      "Epoch 87/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0234 - accuracy: 0.9925\n",
      "Epoch 88/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0241 - accuracy: 0.9923\n",
      "Epoch 89/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0235 - accuracy: 0.9921\n",
      "Epoch 90/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0256 - accuracy: 0.9922\n",
      "Epoch 91/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0198 - accuracy: 0.9934\n",
      "Epoch 92/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0232 - accuracy: 0.9922\n",
      "Epoch 93/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0206 - accuracy: 0.9933\n",
      "Epoch 94/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0212 - accuracy: 0.9930\n",
      "Epoch 95/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0235 - accuracy: 0.9923\n",
      "Epoch 96/100\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 0.0216 - accuracy: 0.9930\n",
      "Epoch 97/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0207 - accuracy: 0.9934\n",
      "Epoch 98/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0213 - accuracy: 0.9930\n",
      "Epoch 99/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0233 - accuracy: 0.9926\n",
      "Epoch 100/100\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 0.0199 - accuracy: 0.9935\n",
      "Epoch 1/100\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 1.9023 - accuracy: 0.6968\n",
      "Epoch 2/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.4803 - accuracy: 0.7892\n",
      "Epoch 3/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.4004 - accuracy: 0.8278\n",
      "Epoch 4/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.3433 - accuracy: 0.8558\n",
      "Epoch 5/100\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.3306 - accuracy: 0.8630\n",
      "Epoch 6/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.2823 - accuracy: 0.8839\n",
      "Epoch 7/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.2791 - accuracy: 0.8867\n",
      "Epoch 8/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.2566 - accuracy: 0.8963\n",
      "Epoch 9/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2408 - accuracy: 0.9043\n",
      "Epoch 10/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.2283 - accuracy: 0.9095\n",
      "Epoch 11/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.2320 - accuracy: 0.9082\n",
      "Epoch 12/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.2132 - accuracy: 0.9175\n",
      "Epoch 13/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.2041 - accuracy: 0.9197\n",
      "Epoch 14/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.2000 - accuracy: 0.9230\n",
      "Epoch 15/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1946 - accuracy: 0.9245\n",
      "Epoch 16/100\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.1885 - accuracy: 0.9275\n",
      "Epoch 17/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1866 - accuracy: 0.9281\n",
      "Epoch 18/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1864 - accuracy: 0.9294\n",
      "Epoch 19/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1674 - accuracy: 0.9367\n",
      "Epoch 20/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1633 - accuracy: 0.9389\n",
      "Epoch 21/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1662 - accuracy: 0.9379\n",
      "Epoch 22/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1593 - accuracy: 0.9405\n",
      "Epoch 23/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1609 - accuracy: 0.9399\n",
      "Epoch 24/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.1583 - accuracy: 0.9409\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1444 - accuracy: 0.9468\n",
      "Epoch 26/100\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.1426 - accuracy: 0.9473\n",
      "Epoch 27/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1433 - accuracy: 0.9473\n",
      "Epoch 28/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1374 - accuracy: 0.9492\n",
      "Epoch 29/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1314 - accuracy: 0.9528\n",
      "Epoch 30/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1326 - accuracy: 0.9525\n",
      "Epoch 31/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1339 - accuracy: 0.9512\n",
      "Epoch 32/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1320 - accuracy: 0.9521\n",
      "Epoch 33/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1249 - accuracy: 0.9553\n",
      "Epoch 34/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1237 - accuracy: 0.9557\n",
      "Epoch 35/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1229 - accuracy: 0.9557\n",
      "Epoch 36/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1175 - accuracy: 0.9582\n",
      "Epoch 37/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1174 - accuracy: 0.9577\n",
      "Epoch 38/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1150 - accuracy: 0.9589\n",
      "Epoch 39/100\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.1168 - accuracy: 0.9578\n",
      "Epoch 40/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1098 - accuracy: 0.9604\n",
      "Epoch 41/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1180 - accuracy: 0.9575\n",
      "Epoch 42/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1102 - accuracy: 0.9608\n",
      "Epoch 43/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1029 - accuracy: 0.9633\n",
      "Epoch 44/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.1018 - accuracy: 0.9643\n",
      "Epoch 45/100\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.1001 - accuracy: 0.9650\n",
      "Epoch 46/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0984 - accuracy: 0.9655\n",
      "Epoch 47/100\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.0971 - accuracy: 0.9661\n",
      "Epoch 48/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0970 - accuracy: 0.9660\n",
      "Epoch 49/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0943 - accuracy: 0.9671\n",
      "Epoch 50/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0942 - accuracy: 0.9665\n",
      "Epoch 51/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0923 - accuracy: 0.9676\n",
      "Epoch 52/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0893 - accuracy: 0.9695\n",
      "Epoch 53/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0905 - accuracy: 0.9692\n",
      "Epoch 54/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0888 - accuracy: 0.9693\n",
      "Epoch 55/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0873 - accuracy: 0.9695\n",
      "Epoch 56/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0824 - accuracy: 0.9712\n",
      "Epoch 57/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0816 - accuracy: 0.9721\n",
      "Epoch 58/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0803 - accuracy: 0.9725\n",
      "Epoch 59/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0799 - accuracy: 0.9724\n",
      "Epoch 60/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0791 - accuracy: 0.9730\n",
      "Epoch 61/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0783 - accuracy: 0.9728\n",
      "Epoch 62/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0828 - accuracy: 0.9715\n",
      "Epoch 63/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0739 - accuracy: 0.9747\n",
      "Epoch 64/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0748 - accuracy: 0.9742\n",
      "Epoch 65/100\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.0706 - accuracy: 0.9765\n",
      "Epoch 66/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0686 - accuracy: 0.9767\n",
      "Epoch 67/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0688 - accuracy: 0.9764\n",
      "Epoch 68/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0696 - accuracy: 0.9758\n",
      "Epoch 69/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0727 - accuracy: 0.9749\n",
      "Epoch 70/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0681 - accuracy: 0.9766\n",
      "Epoch 71/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0667 - accuracy: 0.9774\n",
      "Epoch 72/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0661 - accuracy: 0.9774\n",
      "Epoch 73/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0678 - accuracy: 0.9765\n",
      "Epoch 74/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0653 - accuracy: 0.9775\n",
      "Epoch 75/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0585 - accuracy: 0.9808\n",
      "Epoch 76/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0590 - accuracy: 0.9803\n",
      "Epoch 77/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0611 - accuracy: 0.9791\n",
      "Epoch 78/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0577 - accuracy: 0.9804\n",
      "Epoch 79/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0550 - accuracy: 0.9815\n",
      "Epoch 80/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0575 - accuracy: 0.9803\n",
      "Epoch 81/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0552 - accuracy: 0.9812\n",
      "Epoch 82/100\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.0492 - accuracy: 0.9838\n",
      "Epoch 83/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0515 - accuracy: 0.9827\n",
      "Epoch 84/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0524 - accuracy: 0.9822\n",
      "Epoch 85/100\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.0522 - accuracy: 0.9824\n",
      "Epoch 86/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0537 - accuracy: 0.9818\n",
      "Epoch 87/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0523 - accuracy: 0.9820\n",
      "Epoch 88/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0503 - accuracy: 0.9833\n",
      "Epoch 89/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0439 - accuracy: 0.9856\n",
      "Epoch 90/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0480 - accuracy: 0.9837\n",
      "Epoch 91/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0439 - accuracy: 0.9854\n",
      "Epoch 92/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0419 - accuracy: 0.9861\n",
      "Epoch 93/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0414 - accuracy: 0.9860\n",
      "Epoch 94/100\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.0455 - accuracy: 0.9849\n",
      "Epoch 95/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0419 - accuracy: 0.9860\n",
      "Epoch 96/100\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.0464 - accuracy: 0.9841\n",
      "Epoch 97/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0406 - accuracy: 0.9864\n",
      "Epoch 98/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0415 - accuracy: 0.9858\n",
      "Epoch 99/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0446 - accuracy: 0.9852\n",
      "Epoch 100/100\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0421 - accuracy: 0.9854\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 42ms/step - loss: 9.9002 - accuracy: 0.5303\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1s 43ms/step - loss: 2.5432 - accuracy: 0.5640\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 1.2985 - accuracy: 0.6441\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 1s 44ms/step - loss: 0.7541 - accuracy: 0.7056\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 48ms/step - loss: 0.5705 - accuracy: 0.7459\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.5738 - accuracy: 0.7390\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.4948 - accuracy: 0.7706\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.4805 - accuracy: 0.7763\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 0.6064 - accuracy: 0.7499\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 0.5359 - accuracy: 0.7622\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 0.4474 - accuracy: 0.7953\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 0.4158 - accuracy: 0.8129\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 0.4026 - accuracy: 0.8211\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.4641 - accuracy: 0.7910\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.4388 - accuracy: 0.8010\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.4369 - accuracy: 0.8066\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.4118 - accuracy: 0.8144\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 0.3711 - accuracy: 0.8376\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 0.3450 - accuracy: 0.8524\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3738 - accuracy: 0.8397\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 1s 58ms/step - loss: 0.3513 - accuracy: 0.8468\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 0.3381 - accuracy: 0.8548\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.3283 - accuracy: 0.8611\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 0.3192 - accuracy: 0.8652\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 0.3175 - accuracy: 0.8650\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.3002 - accuracy: 0.8745\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.2934 - accuracy: 0.8777\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.3016 - accuracy: 0.8726\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.3149 - accuracy: 0.8671\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.2893 - accuracy: 0.8793\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.2739 - accuracy: 0.8880\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.2694 - accuracy: 0.8901\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 0.2821 - accuracy: 0.8833\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.2697 - accuracy: 0.8894\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 0.2944 - accuracy: 0.8775\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 0.2884 - accuracy: 0.8793\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 0.2642 - accuracy: 0.8927\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.2528 - accuracy: 0.8980\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 0.2594 - accuracy: 0.8943\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 0.2626 - accuracy: 0.8919\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.2521 - accuracy: 0.8968\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.2505 - accuracy: 0.8979\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 0.2420 - accuracy: 0.9024\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 1s 44ms/step - loss: 0.2333 - accuracy: 0.9074\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.2477 - accuracy: 0.8993\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 1s 46ms/step - loss: 0.2391 - accuracy: 0.9042\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.2568 - accuracy: 0.8953\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 0.2512 - accuracy: 0.8981\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 0.2376 - accuracy: 0.9049\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.2208 - accuracy: 0.9135\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 0.2369 - accuracy: 0.9050\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 1s 55ms/step - loss: 0.2456 - accuracy: 0.9013\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 0.2617 - accuracy: 0.8946\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.3678 - accuracy: 0.8628\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 0.2676 - accuracy: 0.8917\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2367 - accuracy: 0.9056\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.2700 - accuracy: 0.8904\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.4928 - accuracy: 0.8494\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.3763 - accuracy: 0.8588\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.2831 - accuracy: 0.8878\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.2489 - accuracy: 0.9006\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.2584 - accuracy: 0.8957\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2408 - accuracy: 0.9037\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.2279 - accuracy: 0.9089\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2219 - accuracy: 0.9114\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.2102 - accuracy: 0.9179\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.2148 - accuracy: 0.9149\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.2115 - accuracy: 0.9171\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2035 - accuracy: 0.9206\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2040 - accuracy: 0.9200\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.2116 - accuracy: 0.9160\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.2075 - accuracy: 0.9183\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.1984 - accuracy: 0.9228\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.1949 - accuracy: 0.9243\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.1959 - accuracy: 0.9243\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 0.1921 - accuracy: 0.9256\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 0.1866 - accuracy: 0.9289\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.1910 - accuracy: 0.9261\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.1865 - accuracy: 0.9287\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.1811 - accuracy: 0.9315\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.1849 - accuracy: 0.9296\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.1780 - accuracy: 0.9328\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.1795 - accuracy: 0.9324\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.1790 - accuracy: 0.9324\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.1764 - accuracy: 0.9339\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 70ms/step - loss: 0.1753 - accuracy: 0.9337\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.1858 - accuracy: 0.9283\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.1884 - accuracy: 0.9267\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.1876 - accuracy: 0.9282\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.1767 - accuracy: 0.9323\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.1813 - accuracy: 0.9309\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.1782 - accuracy: 0.9323\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.1750 - accuracy: 0.9329\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.1691 - accuracy: 0.9362\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.1931 - accuracy: 0.9256\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.1839 - accuracy: 0.9298\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.1746 - accuracy: 0.9342\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.1620 - accuracy: 0.9395\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.1634 - accuracy: 0.9386\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.1552 - accuracy: 0.9430\n"
     ]
    }
   ],
   "source": [
    "nn_model_tunning = nn_modeling.parameter_tuning( { \n",
    "        'input':50,\n",
    "        'layer1':{'s':300, 'activation': 'relu'}, \n",
    "        'layer2':{'s':200, 'activation': 'relu'}, \n",
    "        'layer3':{'s':100, 'activation': 'relu'},\n",
    "        'layer4':{'s':1, 'activation':'sigmoid'},\n",
    "        'loss':'BinaryCrossentropy',\n",
    "        'metric':'accuracy',\n",
    "        'epoch':[10,30,100],\n",
    "        'bs':[10,100,1000,10000], \n",
    "        'optimizer':'adam'\n",
    "        }, NNModel)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': <__main__.NNModel at 0x7f9fc4016700>,\n",
       " 'train_metrics': {'matrix': array([[66586,   476],\n",
       "         [  230, 44708]]),\n",
       "  'auc': 0.9938919639003215,\n",
       "  'accuracy': 0.9937,\n",
       "  'precision': 0.9894652974504249,\n",
       "  'recall': 0.9948818371979171,\n",
       "  'f1': 0.9921661747409068,\n",
       "  'cost': -0.4763392857142857,\n",
       "  'y_pred': 0         0\n",
       "  1         0\n",
       "  2         0\n",
       "  3         0\n",
       "  4         0\n",
       "           ..\n",
       "  111995    1\n",
       "  111996    1\n",
       "  111997    0\n",
       "  111998    0\n",
       "  111999    1\n",
       "  Length: 112000, dtype: int64,\n",
       "  'y_pred_proba': 0         8.364528e-03\n",
       "  1         7.204711e-04\n",
       "  2         1.746266e-06\n",
       "  3         1.460797e-12\n",
       "  4         9.244084e-04\n",
       "                ...     \n",
       "  111995    1.000000e+00\n",
       "  111996    1.000000e+00\n",
       "  111997    2.671874e-16\n",
       "  111998    1.337676e-18\n",
       "  111999    1.000000e+00\n",
       "  Length: 112000, dtype: float32,\n",
       "  'elapsed_time': 217.56609201431274},\n",
       " 'test_metrics': {'matrix': array([[27601,  1140],\n",
       "         [  883, 18376]]),\n",
       "  'auc': 0.9572433576142492,\n",
       "  'accuracy': 0.95785,\n",
       "  'precision': 0.9415863906538225,\n",
       "  'recall': 0.9541513058829638,\n",
       "  'f1': 0.9478272082527401,\n",
       "  'cost': -2.8348958333333334,\n",
       "  'y_pred': 0        1\n",
       "  1        1\n",
       "  2        1\n",
       "  3        1\n",
       "  4        0\n",
       "          ..\n",
       "  47995    1\n",
       "  47996    0\n",
       "  47997    0\n",
       "  47998    1\n",
       "  47999    0\n",
       "  Length: 48000, dtype: int64,\n",
       "  'y_pred_proba': 0        9.824805e-01\n",
       "  1        9.432876e-01\n",
       "  2        1.000000e+00\n",
       "  3        1.000000e+00\n",
       "  4        4.967928e-03\n",
       "               ...     \n",
       "  47995    9.999976e-01\n",
       "  47996    4.600529e-14\n",
       "  47997    1.722584e-11\n",
       "  47998    9.999980e-01\n",
       "  47999    4.794389e-03\n",
       "  Length: 48000, dtype: float32,\n",
       "  'elapsed_time': 1.922071933746338}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_modeling.find_best_model('auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proba = nn_modeling.find_best_model('auc')['train_metrics']['y_pred_proba']\n",
    "test_proba = nn_modeling.find_best_model('auc')['test_metrics']['y_pred_proba']\n",
    "conf_train = nn_modeling.find_best_model('auc')['train_metrics']['matrix']\n",
    "conf_test = nn_modeling.find_best_model('auc')['test_metrics']['matrix']\n",
    "\n",
    "cost_results = tune_cost_proba(1-train_proba, 1-test_proba, nn_modeling.y_train, nn_modeling.y_test, conf_train, conf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Cost</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Train Cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.030729</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.030804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.215104</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.322991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.313542</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.279687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.400521</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.474479</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.303795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.513021</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.325223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.579167</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.341071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.647917</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.364955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.696875</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.392857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.755208</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.428125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.834896</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.476339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Test Cost  Threshold  Train Cost\n",
       "0   10.030729       0.00   10.030804\n",
       "1    2.215104       0.05    0.322991\n",
       "2    2.313542       0.10    0.279687\n",
       "3    2.400521       0.15    0.287500\n",
       "4    2.474479       0.20    0.303795\n",
       "5    2.513021       0.25    0.325223\n",
       "6    2.579167       0.30    0.341071\n",
       "7    2.647917       0.35    0.364955\n",
       "8    2.696875       0.40    0.392857\n",
       "9    2.755208       0.45    0.428125\n",
       "10   2.834896       0.50    0.476339"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, your primary task is to train a model (or models) capable of generalizing on a binary-target that will minimize the monetary loss for your customer and will involve the following steps:\n",
    "\n",
    "- Specify your sampling methodology\n",
    "- Setup your model(s) - highlighting any important parameters\n",
    "- Analyze the performance of your model(s) - referencing your chosen evaluation metric (including supplemental visuals and analysis where appropriate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the results from the four models tried for this dataset and their comparison against predictions using the complete dataset.\n",
    "Logistic Regression: This model was the quickest to train and has resulted in a test AUC of X.XX and Cost per Prediction of $0.00\n",
    " \n",
    "XGBoost: This model was ............ resulted in a test AUC of XX and Cost per Prediction of $0.00\n",
    "\n",
    " \n",
    "Neural Network: This model was .......... train and resulted in a test AUC of X.XX and Cost per Prediction of $0.00\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below compares the key metrics between the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | AUC | # False Positives | # False Negatives | \n",
    "|-------|-----|-------------------|-------------------|\n",
    "|Logistic Regression | xxxx | xxxx | xxxx |\n",
    "|XGBoost | xxxx | xxxx | xxxx |\n",
    "|Neural Network | xxxx | xxxx | xxxx |\n",
    "|Ensemble | xxxx | xxxx | xxxx |\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhUWTUQleFn-"
   },
   "source": [
    "### Final Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monetary Outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the expected monetary cost (or loss) associated with your model and how might you best translate this to your customer?  Remember, predicting class 1 incorrectly costs the customer \\\\$100  while incorrectly predicting class 0 costs the customer \\\\$25; or said another way, False Positives = -\\\\$100 and False Negatives = -\\\\$25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the stakeholder is not interested in the key features for prediction, the below plot depicts the top 15 feature for predictions using the XGBoost model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO add feature importance image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbAhMB1x8g_e"
   },
   "source": [
    "# Conclusion <a id='conclusion'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of your technical analysis and modeling; what are you proposing to your audience and why?  How should they view your results and what should they consider when moving forward?  Are there other approaches you'd recommend exploring?  This is where you \"bring it all home\" in language they understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oX8fXYczN5D-"
   },
   "source": [
    "### Future Considerations, Model Enhancements and Alternative Modeling Approaches <a id='model-enhancements'/>\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0yVc5DKeFn_"
   },
   "source": [
    "## References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Case_Study_6_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
