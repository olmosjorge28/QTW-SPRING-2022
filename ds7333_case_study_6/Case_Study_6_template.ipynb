{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoCXzNvN8g-8"
   },
   "source": [
    "# Case Study 6 - Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBy24RcB8g-9"
   },
   "source": [
    "__Team Members:__ Amber Clark, Andrew Leppla, Jorge Olmos, Paritosh Rai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4O0up-U8g-9"
   },
   "source": [
    "# Content\n",
    "* [Business Understanding](#business-understanding)\n",
    "    - [Scope](#scope)\n",
    "    - [Introduction](#introduction)\n",
    "    - [Methods](#methods)\n",
    "    - [Results](#results)\n",
    "* [Data Evaluation](#data-evaluation)\n",
    "    - [Loading Data](#loading-data) \n",
    "    - [Data Summary](#data-summary)\n",
    "    - [Missing Values](#missing-values)\n",
    "    - [Feature Removal](#feature-removal)\n",
    "    - [Exploratory Data Analysis (EDA)](#eda)\n",
    "* [Model Preparations](#model-preparations)\n",
    "    - [Sampling & Scaling Data](#sampling-scaling-data)\n",
    "    - [Proposed Method](#proposed-metrics)\n",
    "    - [Evaluation Metrics](#evaluation-metrics)\n",
    "    - [Feature Selection](#feature-selection)\n",
    "* [Model Building & Evaluations](#model-building)\n",
    "    - [Sampling Methodology](#sampling-methodology)\n",
    "    - [Model](#model)\n",
    "    - [Performance Analysis](#performance-analysis)\n",
    "* [Model Interpretability & Explainability](#model-explanation)\n",
    "    - [Examining Feature Importance](#examining-feature-importance)\n",
    "* [Conclusion](#conclusion)\n",
    "    - [Final Model Proposal](#final-model-proposal)\n",
    "    - [Future Considerations, Model Enhancements and Alternative Modeling Approaches](#model-enhancements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRedT-FB8g_A"
   },
   "source": [
    "# Business Understanding & Executive Summary <a id='business-understanding'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-4BiuuQOEh4"
   },
   "source": [
    "## Objective:\n",
    "\n",
    "The objective of this case study is to predict the detection of a new subatomic particle with high accuracy from a dataset with 7 million records.  \n",
    "\n",
    "## Introduction:\n",
    "No information regarding the data in the case study was provided; the only stipulation given was to classify a binary variable representing \"the existence of a particle\" using a neural network. In terms of data detection of the binary classifier, 1 represents detection, and 0 represents non-detection. The client has advised that this is a massive amount of data best modeled with Neural Networks, and a high level of accuracy is critical.\n",
    "\n",
    "### Artificial Neural Networks\n",
    "\n",
    "Neural networks are based on brain biology and stimulate the brain's function. Based on neuroscience, neurons are connected by axons to other neurons. This concept is applied in an Artificial Neural Network (ANN). An ANN comprises groups of \"neurons\" called layers. These layers are connected in a network to take inputs from the dataset, fit model weights to the inputs, and eventually produce outputs that can be used to classify a target variable. The layers between the inputs and the target outputs in a neural network are called hidden layers.   \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/olmosjorge28/QTW-SPRING-2022/main/ds7333_case_study_6/Neural_Network_fig.png\" width=300 height=300 />\n",
    "\n",
    "Physiologically, neurons work by firing signals only when a certain signal \"threshold\" is reached. This behavior is mimicked by ANNs. Any signal input below the threshold will not result in an output from the neuron, while any signal at or above the threshold will result in a constant output. Various activation functions are used to mathematically approximate how a neuron works. Activation functions are equations that determine the output of a neural network model. \n",
    "\n",
    "Some of the common activation functions are discussed below:\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/olmosjorge28/QTW-SPRING-2022/main/ds7333_case_study_6/Activation%20Function.png\" width=300 height=300 />\n",
    "\n",
    "\n",
    "Each neuron represents a regression in the neural network and calculates an output. A neural network is an ensemble of many regressors that will take the outputs of previous regressors as inputs. This results in a large ensemble of regression models.\n",
    "\n",
    "\n",
    "## Modeling:\n",
    "\n",
    "### Training and Test Split\n",
    "70/30 split ---> batch and epochs?\n",
    "\n",
    "\n",
    "### Key Metrics\n",
    "talk about accuracy/auc and why that's our metric \n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "\n",
    "\n",
    "### Feature Importance\n",
    "Add linear feature importances \n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "\n",
    "\n",
    "## Future Considerations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVtcYu5j8g_B"
   },
   "source": [
    "# Data Evaluation <a id='data-evaluation'>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Owfb6XnGfPKI"
   },
   "source": [
    "Summarize the data being used in the case using appropriate mediums (charts, graphs, tables); address questions such as: Are there missing values? Which variables are needed (which ones are not)? What assumptions or conclusions are you drawing that need to be relayed to your audience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WcvKI3y8g_C"
   },
   "source": [
    "## Loading Data <a id='loading-data'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import warnings\\nwarnings.filterwarnings('ignore')\\nfrom warnings import simplefilter \\nsimplefilter(action='ignore', category=FutureWarning)\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import Image\n",
    "#from IPython.display import clear_output\n",
    "#import sklearn\n",
    "import time\n",
    "#import re\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tabulate import tabulate\n",
    "\n",
    "# data pre-processing\n",
    "from sklearn.impute._base import _BaseImputer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection._split import BaseShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# prediction models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# import warnings filter\n",
    "'''import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from warnings import simplefilter \n",
    "simplefilter(action='ignore', category=FutureWarning)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "class FilePathManager:\n",
    "    def __init__(self, local_dir: str):\n",
    "        self.local_dir = local_dir\n",
    "    \n",
    "    def retrieve_full_path(self):\n",
    "        return os.getcwd()+'/'+self.local_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    #@abstractmethod\n",
    "    def load_data(self, file_name):\n",
    "        pass\n",
    "    \n",
    "    #@abstractmethod\n",
    "    def get_df(self):\n",
    "        pass\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    " \n",
    "class CSVLoader(Loader):\n",
    "    def __init__(self, file_path_manager: FilePathManager):\n",
    "        self.file_path_manager = file_path_manager\n",
    "        \n",
    "    def load_data(self, _prepare_data: Callable[[pd.DataFrame], pd.DataFrame] = None):\n",
    "        self.df = pd.read_csv(self.file_path_manager.retrieve_full_path())\n",
    "        if _prepare_data:\n",
    "            self.df = _prepare_data(self.df)\n",
    "    \n",
    "    def get_df(self):\n",
    "        return self.df;\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df['# label'] = df['# label'].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(FilePathManager('data/all_train.csv'))\n",
    "loader.load_data(clean_data)\n",
    "df = loader.get_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ul_6nw48N5Dy"
   },
   "source": [
    "## Data Summary <a id='data-summary'>\n",
    "    \n",
    "### Data Exploration and Manipulation:\n",
    "    \n",
    "The provided data, although not described in detail, is a large dataset consisting of 28 features and a binary class. The column mass is the only named feature; all others are arbitrarily numbered, and all features are numeric. There are seven million observations. There are no known missing values in the data with the caveat that it is unknown whether zeros could constitute missing data.\n",
    "The only manipulation required for preparing this data for use in a neural network model is to change the target class object type to Boolean to save a small amount of space and to normalize the range of the features, which was performed after the data was split into test/train data set. In addition, the target classes are very well balanced in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aws5HAx98g_E"
   },
   "source": [
    "## Missing Values <a id='missing-values'>\n",
    "There are no missing Values -- elaborate on this point later \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbAmkozvN5Dz"
   },
   "source": [
    "## Exploratory Data Analysis (EDA) <a id='eda'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_summary = df.iloc[:, 1:29].describe().T\n",
    "feature_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highly Skewed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_skew = feature_summary.loc[feature_summary['max'] > feature_summary['mean'] + feature_summary['std']*4]\n",
    "left_skew = feature_summary.loc[feature_summary['min'] < feature_summary['mean'] - feature_summary['std']*4]\n",
    "skew = pd.concat([right_skew.T, left_skew.T], axis=0, join='outer')\n",
    "skew.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skewed, no major outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 6, figsize=(14, 5))\n",
    "fig.suptitle('Skewed Features - Boxplots')\n",
    "for i,j in zip(skew.columns, range(11)):\n",
    "    sns.boxplot(ax = axes[int(j/6), j%6], x = df[i])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "### Target Variable Class Distribution\n",
    "\n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/olmosjorge28/QTW-SPRING-2022/main/ds7333_case_study_6/Target_Variable_Class_Distribution.png\" width=300 height=300 />\n",
    "\n",
    "    \n",
    "Also, correlations between features f6, f10, f14, f18, and f26 were observed and also with the target variable. However, all the variables will be included in the model fitting exercise as there is no domain knowledge of the features to assess if some of them can be excluded from the analysis instead of the others.\n",
    "\n",
    "    \n",
    "        \n",
    "<img src=\"https://raw.githubusercontent.com/olmosjorge28/QTW-SPRING-2022/main/ds7333_case_study_6/Correlation_heat_map.png\" width=300 height=300 />\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmuI_mep8g_b"
   },
   "source": [
    "# Model Preparations <a id='model-preparations'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iDJqPa8fUNp"
   },
   "source": [
    "Which methods are you proposing to utilize to solve the problem?  Why is this method appropriate given the business objective? How will you determine if your approach is useful (or how will you differentiate which approach is more useful than another)?  More specifically, what evaluation metrics are most useful given that the problem is a classification one (ex., Accuracy, F1-score, Precision, Recall, AUC, etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnCsXV_c8g_V"
   },
   "source": [
    "## Sampling & Scaling Data <a id='sampling-scaling-data' />\n",
    "\n",
    "\n",
    "Training and test sets were created from the data using the stratified method to maintain the ratio of the binary outcome.  This was done in an abundance of caution, because the classes are almost perfectly balanced. 30% of the data was withheld for the test set, and the defining features were normalized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXqoLTm_8g_c"
   },
   "source": [
    "## Proposed Method <a id='proposed-metrics' />\n",
    "\n",
    "The stakeholders wanted our team to focus on creating a model that would predict the existance of a new particle with high accuracy above all, and the model interpretability was not a priority. With this mind our team decided on using an Artificial Neural Network to achieve a high accuracy model. Our has input layer of 28 neurons for each of the feature with X hidden layers and a single neuron output layer with a sigmoid activation function and a BinaryCrossentropy loss since our target variable is binary. The hidden layers used a ReLu activation, which was chosen for its non-linear characteristics that helps estimating non-linear functions. The team decided to have X neurons and X neurons...for each hidden layer respectively. \n",
    "\n",
    "In experimentations our best results were achieved with a batch size of 1000. This gave a large sample size limit uncessary fluctations, this gave the model the right balance between variance and bias. Additionally, the batch sizes were small enough to compute in memory, but not so small it would increase processing time dramatically. The team ran 100 epochs however after 30 epochs there was no further improvment was observed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseImputer:\n",
    "    #@abstractmethod\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    \n",
    "    #@abstractmethod\n",
    "    def transform(self, X):\n",
    "        pass\n",
    "\n",
    "class BaseModel:\n",
    "    #@abstractmethod\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        pass\n",
    "    \n",
    "    #@abstractmethod\n",
    "    def predict(self, X):\n",
    "        passb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modeling:\n",
    "    _X_train_fitted = None\n",
    "    _X_test_fitted = None\n",
    "    _y_train = None\n",
    "    _y_test = None\n",
    "    _y_preds = None\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, \n",
    "                 target_name: str, \n",
    "                 shuffle_splitter: BaseShuffleSplit, \n",
    "                 imputer: BaseImputer, \n",
    "                 model: BaseModel, \n",
    "                 scaler = None):\n",
    "        self._data = data\n",
    "        self._target_name = target_name\n",
    "        self._shuffle_splitter = shuffle_splitter\n",
    "        self._imputer = imputer\n",
    "        self._model = model\n",
    "        self._X, self._y = self._split_data()\n",
    "        self._scaler = scaler\n",
    "        \n",
    "    @property\n",
    "    def X(self):\n",
    "        return self._X\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._y\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "    \n",
    "    @model.setter\n",
    "    def model(self, model):\n",
    "        self._model = model\n",
    "     \n",
    "    @property\n",
    "    def X_train(self):\n",
    "        return self._X_train_fitted\n",
    "    \n",
    "    @property\n",
    "    def X_test(self):\n",
    "        return self._X_test_fitted\n",
    "    \n",
    "    @property\n",
    "    def y_train(self):\n",
    "        return self._y_train\n",
    "    \n",
    "    @property\n",
    "    def y_test(self):\n",
    "        return self._y_test\n",
    "    \n",
    "    @property\n",
    "    def y_preds(self):\n",
    "        return self._y_preds\n",
    "    \n",
    "    def _split_data(self):\n",
    "        X = self._data.copy()\n",
    "        return X.drop([self._target_name], axis=1) , X[self._target_name]\n",
    "    \n",
    "    def _shuffle_split(self):\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        for train_index, test_index in self._shuffle_splitter.split(X,y):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def _fit_imputer(self, train):\n",
    "        if self._imputer is not None:\n",
    "            self._imputer.fit(train)\n",
    "    \n",
    "    def _fit_scaler(self, train):\n",
    "        if self._scaler is not None:\n",
    "            self._scaler.fit(train)\n",
    "    \n",
    "    def _impute_data(self, X: pd.DataFrame):\n",
    "        if self._imputer is not None:\n",
    "            return pd.DataFrame(self._imputer.transform(X), columns = self.X.columns, index = X.index)\n",
    "        return X\n",
    "    \n",
    "    def _scale_data(self, X: pd.DataFrame):\n",
    "        if self._scaler is not None:\n",
    "            X = pd.DataFrame(self._scaler.transform(X), columns = self._X.columns)\n",
    "        return X\n",
    "    \n",
    "    def prepare(self):\n",
    "        X_train, X_test, y_train, y_test = self._shuffle_split()   \n",
    "        self._fit_imputer(X_train)\n",
    "        X_train = self._impute_data(X_train)\n",
    "        X_test = self._impute_data(X_test)\n",
    "        self._fit_scaler(X_train)\n",
    "        self._X_train_fitted = self._scale_data(X_train)\n",
    "        self._X_test_fitted = self._scale_data(X_test)\n",
    "        self._y_train = y_train\n",
    "        self._y_test = y_test\n",
    "        \n",
    "    def prepare_and_train(self):\n",
    "        self.prepare()\n",
    "        return self.train()\n",
    "        \n",
    "    def train(self): #, epoch=None, batch=None\n",
    "        self._model.fit(self.X_train, self.y_train) #, batch_size=batch, epochs=epoch\n",
    "        self._y_preds = self._model.predict(self.X_train)\n",
    "        \n",
    "        return self.metrics(self.y_train, self.y_preds)\n",
    "        \n",
    "    def test(self):\n",
    "        return self.metrics(self.y_test, self._model.predict(self.X_test))\n",
    "       \n",
    "        \n",
    "    def metrics(self, y_true = None, y_pred = None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModeling(Modeling):\n",
    "    def __init__(self, \n",
    "                 data: pd.DataFrame, \n",
    "                 target_name: str, \n",
    "                 shuffle_splitter: BaseShuffleSplit, \n",
    "                 imputer: BaseImputer, \n",
    "                 model: BaseModel, \n",
    "                 scaler = None,\n",
    "                 beta: int = 1,\n",
    "                 classification: str = 'binary'):\n",
    "        super().__init__(data, target_name, shuffle_splitter, imputer, model, scaler)\n",
    "        self.beta = beta\n",
    "        self.classification = classification\n",
    "    \n",
    "    def metrics(self, y_true = None, y_pred = None):\n",
    "        if y_true is None and y_pred is None:\n",
    "            y_true = self.y_train\n",
    "            y_pred = self.y_preds\n",
    "        return ({'matrix': confusion_matrix(y_true, y_pred), \n",
    "            'accuracy': accuracy_score(y_true, y_pred), \n",
    "            'precision': precision_score(y_true, y_pred, average=self.classification), \n",
    "            'recall': recall_score(y_true, y_pred, average=self.classification),\n",
    "             'f1': f1_score(y_true, y_pred),\n",
    "            'f{}'.format(self.beta) : fbeta_score(y_true, y_pred, average=self.classification, beta=self.beta) } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClassificationModeling(ClassificationModeling):\n",
    "    def __init__(self, \n",
    "             data: pd.DataFrame, \n",
    "             target_name: str, \n",
    "             shuffle_splitter: BaseShuffleSplit, \n",
    "             imputer: BaseImputer, \n",
    "             model: BaseModel, \n",
    "             scaler = None,\n",
    "             beta: int = 1,\n",
    "             classification: str = 'binary', tb_callback = TensorBoard(log_dir=\"logs/\", histogram_freq=1)):\n",
    "        super().__init__(data, target_name, shuffle_splitter, imputer, model, scaler, beta, classification)\n",
    "        self.tb_callback=tb_callback\n",
    "        \n",
    "        \n",
    "    def train(self, epoch, batch):\n",
    "        logDir = \"logs/{epoch}-{batchsize}-{time}\".format(epoch=epoch, batchsize=batch, time=time.time())\n",
    "        self.tb_callback.log_dir = logDir\n",
    "        self._model.fit(self.X_train, self.y_train, batch_size=batch, epochs=epoch, validation_data=(self.X_test, self.y_test), callbacks=[self.tb_callback])\n",
    "        self._y_preds = self._model.predict(self.X_train)\n",
    "        return self.metrics(self.y_train, self.y_preds)\n",
    "    \n",
    "    def metrics(self, y_true = None, y_pred = None):\n",
    "        if y_true is None and y_pred is None:\n",
    "            y_true = self.y_train\n",
    "            y_pred = self.y_preds\n",
    "            \n",
    "        y_pred = pd.Series(y_pred.reshape((y_pred.shape[1], y_pred.shape[0]))[0], index=y_true.index)\n",
    "        y_pred = pd.Series( (y_pred>0.5).astype(int), index=y_true.index)\n",
    "        return super().metrics(y_true,y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HT4eeZsX8g_c"
   },
   "source": [
    "## Evaluation Metrics <a id='evaluation-metrics' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeWgSmQW8g_Z"
   },
   "source": [
    "### Baseline Model\n",
    "\n",
    "For our baseline model the team decided to run a logistic regression model. The model used a 70/30 stratified split, with L1 penalty and saga solver. The logistic model was chosen as this a simple, quick, and interprateable model. This gave the team a benchmark for accuracy to compare the proposed artificial neural network accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = ClassificationModeling(df,'# label',\n",
    "                           StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=12343),\n",
    "                           None,\n",
    "                           LogisticRegression(penalty='l1', solver='saga', random_state=12343),\n",
    "                           StandardScaler(), beta=2)\n",
    "\n",
    "baseline.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = pd.DataFrame()\n",
    "\n",
    "for i in [0.0001, 0.0005, 0.001, .005, 1, 100]:\n",
    "    baseline.model.C = i\n",
    "    baseline_results = baseline_results.append({\"C\": i,\n",
    "                                               \"Train Accuracy\": round( baseline.train()['accuracy'], 4),\n",
    "                                               \"Test Accuracy\": round( baseline.test()['accuracy'], 4)},\n",
    "                                              ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=baseline_results, x='C', y='Train Accuracy', color='blue')\n",
    "sns.lineplot(data=baseline_results, x='C', y='Test Accuracy', color='red')\n",
    "plt.title('Tuning C for Logistic Regression with L1')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.xscale('log')\n",
    "plt.axvline(0.001, color='black', ls='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Baseline Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-104997518431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbaseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#epoch=None, batch=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'baseline' is not defined"
     ]
    }
   ],
   "source": [
    "baseline.model.C = 0.001\n",
    "baseline.train() #epoch=None, batch=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rADZ1qTz8g_c"
   },
   "source": [
    "## Feature Selection <a id='feature-selection' />\n",
    "\n",
    "All the features were used in the proposed neural network model. The team chose not use regularization since the training and test set evalution metric results aligned, which indicates that the neural network model was not overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuRjMsjg8g_d"
   },
   "source": [
    "# Model Building & Evaluations <a id='model-building'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSLQokJ1fZA6"
   },
   "source": [
    "In this case, your primary task is to construct a neural network to detect the existence of new particles and will involve the following steps:\n",
    "\n",
    "- Construct your neural network's architecture\n",
    "- Fit your neural network to your training data\n",
    "- Analyze your model's performance - referencing your chosen evaluation metric (including supplemental visuals and analysis where appropriate)\n",
    "\n",
    "\n",
    "The team initially fit a Logistic Regression model to the data set to get a baseline accuracy rate for the prediction. Then, a neural network model was fit to assess the improvement in the accuracy rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UU9VF8AM8g_d"
   },
   "source": [
    "## Sampling Methodology <a id='sampling-methodology'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrxXXO1jeFn6"
   },
   "source": [
    "## Modeling -- TODO: Talk about final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhUWTUQleFn-"
   },
   "source": [
    "### Final Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = NNClassificationModeling(df,'# label',\n",
    "                           StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=12343),\n",
    "                           None,\n",
    "                           None,\n",
    "                           StandardScaler(), beta=2)\n",
    "\n",
    "NN.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.model = tf.keras.Sequential() # model object\n",
    "\n",
    "NN.model.add( tf.keras.layers.Input( shape=(NN.X_train.shape[1],) ) )\n",
    "# specify data shape for first input layer\n",
    "# columns (features) only, # rows specified by batch size later in fit() \n",
    "\n",
    "NN.model.add( tf.keras.layers.Dense(200, activation = 'relu') )\n",
    "# add these layers sequentially with decreasing # neurons\n",
    "NN.model.add( tf.keras.layers.Dense(50, activation = 'relu') )\n",
    "\n",
    "NN.model.add( tf.keras.layers.Dense(1, activation = 'sigmoid') )\n",
    "# Final layer, Regression Output\n",
    "# For Classification, use activation = 'sigmoid' or 'softmax' for Final layer\n",
    "\n",
    "NN.model.compile(optimizer='adam', loss='BinaryCrossentropy', metrics=['accuracy'])\n",
    "# Have to compile model after specifying layers\n",
    "\n",
    "NN.train(batch = 100000, epoch=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-C7Tc5W8g_e"
   },
   "source": [
    "## Model's Performance Analysis <a id='performance-analysis'/>\n",
    "\n",
    "\n",
    "Runtime, training, confusion matrices, accuracy and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HISQa9FO8g_e"
   },
   "source": [
    "## Model Interpretability & Explainability <a id='model-explanation'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnsadV7M8g_e"
   },
   "source": [
    "### Final Model Proposal <a id='final-model-proposal'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSBc4ETe8g_e"
   },
   "source": [
    "### Examining Feature Importance <a id='examining-feature-importance'/>\n",
    "\n",
    "\n",
    "#### Feature importance for baseline Model\n",
    "\n",
    "!!! TODO: expand on this point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_coef = []\n",
    "feat = zip(baseline.X_train.columns, baseline.model.coef_[0])\n",
    "[feat_coef.append([i,j]) for i,j in feat]\n",
    "feat_coef = pd.DataFrame(feat_coef, columns = ['feature','coef'])\n",
    "top_feat_baseline = feat_coef.loc[abs(feat_coef['coef'])>0].sort_values(by='coef')\n",
    "\n",
    "feat_plot = sns.barplot(data=top_feat_ baseline, x='feature', y='coef', palette = \"ch:s=.25,rot=-.25\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('LR Feature Importance with L1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbAhMB1x8g_e"
   },
   "source": [
    "# Conclusion <a id='conclusion'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uA5o0o7fgOZ"
   },
   "source": [
    "After all of your technical analysis and modeling; what are you proposing to your audience and why?  How should they view your results and what should they consider when moving forward?  Are there other approaches you'd recommend exploring?  This is where you \"bring it all home\" in language they understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oX8fXYczN5D-"
   },
   "source": [
    "### Future Considerations, Model Enhancements and Alternative Modeling Approaches <a id='model-enhancements'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0yVc5DKeFn_"
   },
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Case_Study_6_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
