{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoCXzNvN8g-8"
   },
   "source": [
    "# Case Study 6 - Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBy24RcB8g-9"
   },
   "source": [
    "__Team Members:__ Amber Clark, Andrew Leppla, Jorge Olmos, Paritosh Rai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4O0up-U8g-9"
   },
   "source": [
    "# Content\n",
    "* [Business Understanding](#business-understanding)\n",
    "    - [Scope](#scope)\n",
    "    - [Introduction](#introduction)\n",
    "    - [Methods](#methods)\n",
    "    - [Results](#results)\n",
    "* [Data Evaluation](#data-evaluation)\n",
    "    - [Loading Data](#loading-data) \n",
    "    - [Data Summary](#data-summary)\n",
    "    - [Missing Values](#missing-values)\n",
    "    - [Feature Removal](#feature-removal)\n",
    "    - [Exploratory Data Analysis (EDA)](#eda)\n",
    "* [Model Preparations](#model-preparations)\n",
    "    - [Sampling & Scaling Data](#sampling-scaling-data)\n",
    "    - [Proposed Method](#proposed-metrics)\n",
    "    - [Evaluation Metrics](#evaluation-metrics)\n",
    "    - [Feature Selection](#feature-selection)\n",
    "* [Model Building & Evaluations](#model-building)\n",
    "    - [Sampling Methodology](#sampling-methodology)\n",
    "    - [Model](#model)\n",
    "    - [Performance Analysis](#performance-analysis)\n",
    "* [Model Interpretability & Explainability](#model-explanation)\n",
    "    - [Examining Feature Importance](#examining-feature-importance)\n",
    "* [Conclusion](#conclusion)\n",
    "    - [Final Model Proposal](#final-model-proposal)\n",
    "    - [Future Considerations, Model Enhancements and Alternative Modeling Approaches](#model-enhancements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRedT-FB8g_A"
   },
   "source": [
    "# Business Understanding & Executive Summary <a id='business-understanding'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-4BiuuQOEh4"
   },
   "source": [
    "## Objective:\n",
    "\n",
    "The objective of this case study is to predict the detection of a new subatomic particle with high accuracy from a dataset with 7 million records.  \n",
    "\n",
    "## Introduction:\n",
    "No information regarding the data in the case study was provided; the only stipulation given was to classify a binary variable representing \"the existence of a particle\" using a neural network. In terms of data detection of the binary classifier, 1 represents detection, and 0 represents non-detection. The client has advised that this is a massive amount of data best modeled with Neural Networks, and a high level of accuracy is critical.\n",
    "\n",
    "### Artificial Neural Networks\n",
    "\n",
    "Neural networks are based on brain biology and stimulate the brain's function. Based on neuroscience, neurons are connected by axons to other neurons. This concept is applied in an Artificial Neural Network (ANN). An ANN comprises groups of \"neurons\" called layers. These layers are connected in a network to take inputs from the dataset, fit model weights to the inputs, and eventually produce outputs that can be used to classify a target variable. The layers between the inputs and the target outputs in a neural network are called hidden layers.   \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/olmosjorge28/QTW-SPRING-2022/main/ds7333_case_study_6/Neural_Network_fig.png\" width=400 height=400 />\n",
    "\n",
    "Physiologically, neurons work by firing signals only when a certain signal \"threshold\" is reached. This behavior is mimicked by ANNs. Any signal input below the threshold will not result in an output from the neuron, while any signal at or above the threshold will result in a constant output. Various activation functions are used to mathematically approximate how a neuron works. Activation functions are equations that determine the output of a neural network model. \n",
    "\n",
    "Some of the common activation functions are discussed below:\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/olmosjorge28/QTW-SPRING-2022/main/ds7333_case_study_6/Activation%20Function.png\" width=900 height=900 />\n",
    "\n",
    "\n",
    "Each neuron represents a regression in the neural network and calculates an output. A neural network is an ensemble of many regressors that will take the outputs of previous regressors as inputs. This results in a large ensemble of regression models.\n",
    "\n",
    "\n",
    "## Modeling:\n",
    "\n",
    "### Training and Test Split\n",
    "The data were split into 70% training and 30% test sets.  Cross validation was not considered due to the size of the dataset and the complexity of the model; the compute time would have been excessive without adding much value.  \n",
    "\n",
    "### Key Metrics\n",
    "The key metric used to evaluate the models was accuracy.  Accuracy was appropriate because the target variable was balanced with a 50/50 split between Detection and Non-Detection classes.  Accuracy is a straightforward metric that is intuitive and easy to explain, and the customer also requested that the team focus on accuracy.        \n",
    "\n",
    "### Results\n",
    "\n",
    "A baseline model was built using logistic regression as a simpler and more interpretable option.  This model had an __83.6%__ accuracy between the training and test sets and had several highly important features including 'mass' and 'f6' per the plot below:  \n",
    "\n",
    "Logistic Test Set - Confusion Matrix:\n",
    "\n",
    "|                   | Predicted Not Detected | Predicted Detected |\n",
    "|-------------------|---------------|----------------|\n",
    "| Actual Not Detected  | 879,402         | 170,334              |\n",
    "| Actual Detected | 172,538           | 877,726         |\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/olmosjorge28/QTW-SPRING-2022/main/ds7333_case_study_6/LogReg_Feature_Importance.png\" width=400 height=400 />\n",
    "\n",
    "\n",
    "A neural network (NN) model was built to try to beat this simpler baseline model.  The final neural network model had an accuracy of __88.0%__ between the training and test sets.  This is a significant improvement, especially given how expensive it is to run experiments on a large particle accelerator.  The model took a few minutes longer to train than the simpler logistic regression model, as expected.  This additional training was not excessive and significantly improved the accuracy of the predictions.           \n",
    "\n",
    "__TODO:__ Final NN Test Set - Confusion Matrix:\n",
    "\n",
    "|                   | Predicted Not Detected | Predicted Detected |\n",
    "|-------------------|---------------|----------------|\n",
    "| Actual Not Detected  | 903,103         | 146,633              |\n",
    "| Actual Detected | 104,938           | 945,326        |\n",
    "\n",
    "Comparing this confusion matrix to the one above for the baseline model, predictions were improved for both the Detected and Not Detected classes with the Neural Network model.  The __4.4% improvement in accuracy__ is equivalent to __91,301 more correct predictions__.        \n",
    "\n",
    "\n",
    "## Conclusion\n",
    "The team built a model with relatively high accuracy using a neural network.  The final neural network model should be used to make predictions on whether a new particle is detected or not based on the input data.  There was a high level of agreement between the training and test sets, and care was taken to prevent overfitting and create a generalizable model, so results should be reliable if similar experiments are conducted in the future.  \n",
    "\n",
    "\n",
    "## Future Considerations\n",
    "The team recommends that Shapley values be explored for local feature importance for the neural network model.  In addition, input from domain experts may help with feature engineering, model building, and interpretation.  Additional computation power or parallelization may also be useful to speed up model training.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVtcYu5j8g_B"
   },
   "source": [
    "# Data Evaluation <a id='data-evaluation'>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Owfb6XnGfPKI"
   },
   "source": [
    "Summarize the data being used in the case using appropriate mediums (charts, graphs, tables); address questions such as: Are there missing values? Which variables are needed (which ones are not)? What assumptions or conclusions are you drawing that need to be relayed to your audience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WcvKI3y8g_C"
   },
   "source": [
    "## Loading Data <a id='loading-data'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import warnings\\nwarnings.filterwarnings('ignore')\\nfrom warnings import simplefilter \\nsimplefilter(action='ignore', category=FutureWarning)\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import Image\n",
    "#from IPython.display import clear_output\n",
    "#import sklearn\n",
    "import time\n",
    "#import re\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tabulate import tabulate\n",
    "\n",
    "# data pre-processing\n",
    "from sklearn.impute._base import _BaseImputer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection._split import BaseShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# prediction models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# import warnings filter\n",
    "'''import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from warnings import simplefilter \n",
    "simplefilter(action='ignore', category=FutureWarning)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "class FilePathManager:\n",
    "    def __init__(self, local_dir: str):\n",
    "        self.local_dir = local_dir\n",
    "    \n",
    "    def retrieve_full_path(self):\n",
    "        return os.getcwd()+'/'+self.local_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    #@abstractmethod\n",
    "    def load_data(self, file_name):\n",
    "        pass\n",
    "    \n",
    "    #@abstractmethod\n",
    "    def get_df(self):\n",
    "        pass\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    " \n",
    "class CSVLoader(Loader):\n",
    "    def __init__(self, file_path_manager: FilePathManager):\n",
    "        self.file_path_manager = file_path_manager\n",
    "        \n",
    "    def load_data(self, _prepare_data: Callable[[pd.DataFrame], pd.DataFrame] = None):\n",
    "        self.df = pd.read_csv(self.file_path_manager.retrieve_full_path())\n",
    "        if _prepare_data:\n",
    "            self.df = _prepare_data(self.df)\n",
    "    \n",
    "    def get_df(self):\n",
    "        return self.df;\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df['# label'] = df['# label'].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(FilePathManager('data/all_train.csv'))\n",
    "loader.load_data(clean_data)\n",
    "df = loader.get_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ul_6nw48N5Dy"
   },
   "source": [
    "## Data Summary <a id='data-summary'>\n",
    "    \n",
    "### Data Exploration and Manipulation:\n",
    "    \n",
    "The provided data, although not described in detail, is a large dataset consisting of 28 features and a binary class. The column mass is the only named feature; all others are arbitrarily numbered, and all features are numeric. There are seven million observations. There are no known missing values in the data with the caveat that it is unknown whether zeros could constitute missing data.\n",
    "The only manipulation required for preparing this data for use in a neural network model is to change the target class object type to Boolean to save a small amount of space and to normalize the range of the features, which was performed after the data was split into test/train data set. In addition, the target classes are very well balanced in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aws5HAx98g_E"
   },
   "source": [
    "## Missing Values <a id='missing-values'>\n",
    "There are no missing Values -- elaborate on this point later \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbAmkozvN5Dz"
   },
   "source": [
    "## Exploratory Data Analysis (EDA) <a id='eda'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># label</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>...</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>mass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.346368</td>\n",
       "      <td>0.416306</td>\n",
       "      <td>0.999236</td>\n",
       "      <td>0.475342</td>\n",
       "      <td>0.427493</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>1.989833</td>\n",
       "      <td>0.344530</td>\n",
       "      <td>1.566297</td>\n",
       "      <td>...</td>\n",
       "      <td>4.105282</td>\n",
       "      <td>0.267826</td>\n",
       "      <td>0.378718</td>\n",
       "      <td>1.743123</td>\n",
       "      <td>3.406367</td>\n",
       "      <td>4.350537</td>\n",
       "      <td>-0.352571</td>\n",
       "      <td>1.130032</td>\n",
       "      <td>2.227706</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.708236</td>\n",
       "      <td>-0.319394</td>\n",
       "      <td>-1.241873</td>\n",
       "      <td>-0.887231</td>\n",
       "      <td>-0.871906</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>-0.001047</td>\n",
       "      <td>-1.038225</td>\n",
       "      <td>0.655748</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.178141</td>\n",
       "      <td>-0.877361</td>\n",
       "      <td>-1.483769</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>-1.693781</td>\n",
       "      <td>-0.545062</td>\n",
       "      <td>-0.299118</td>\n",
       "      <td>-0.662942</td>\n",
       "      <td>-0.193019</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.360693</td>\n",
       "      <td>1.794174</td>\n",
       "      <td>0.264738</td>\n",
       "      <td>-0.472273</td>\n",
       "      <td>-0.292344</td>\n",
       "      <td>-1.054221</td>\n",
       "      <td>-1.150495</td>\n",
       "      <td>1.423404</td>\n",
       "      <td>1.270098</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.199511</td>\n",
       "      <td>0.539020</td>\n",
       "      <td>-1.590629</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>-0.543636</td>\n",
       "      <td>-0.937456</td>\n",
       "      <td>-0.300344</td>\n",
       "      <td>-0.523262</td>\n",
       "      <td>-1.506304</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.377914</td>\n",
       "      <td>-0.103932</td>\n",
       "      <td>-0.649434</td>\n",
       "      <td>-2.125015</td>\n",
       "      <td>-1.643797</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>1.011112</td>\n",
       "      <td>-1.040340</td>\n",
       "      <td>-0.541991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463763</td>\n",
       "      <td>-0.006583</td>\n",
       "      <td>1.089122</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>-0.276348</td>\n",
       "      <td>-0.409272</td>\n",
       "      <td>-0.349926</td>\n",
       "      <td>-0.307123</td>\n",
       "      <td>0.529698</td>\n",
       "      <td>1250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.067436</td>\n",
       "      <td>-0.636762</td>\n",
       "      <td>-0.620166</td>\n",
       "      <td>-0.062551</td>\n",
       "      <td>1.588715</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>-0.595304</td>\n",
       "      <td>-1.238987</td>\n",
       "      <td>0.336844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.552837</td>\n",
       "      <td>-1.418494</td>\n",
       "      <td>-0.562982</td>\n",
       "      <td>1.743123</td>\n",
       "      <td>0.881802</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>1.560950</td>\n",
       "      <td>-0.150760</td>\n",
       "      <td>-1.023889</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # label        f0        f1        f2        f3        f4        f5  \\\n",
       "0        1 -0.346368  0.416306  0.999236  0.475342  0.427493 -0.005984   \n",
       "1        1  1.708236 -0.319394 -1.241873 -0.887231 -0.871906 -0.005984   \n",
       "2        0 -0.360693  1.794174  0.264738 -0.472273 -0.292344 -1.054221   \n",
       "3        1 -0.377914 -0.103932 -0.649434 -2.125015 -1.643797 -0.005984   \n",
       "4        0 -0.067436 -0.636762 -0.620166 -0.062551  1.588715 -0.005984   \n",
       "\n",
       "         f6        f7        f8  ...       f18       f19       f20       f21  \\\n",
       "0  1.989833  0.344530  1.566297  ...  4.105282  0.267826  0.378718  1.743123   \n",
       "1 -0.001047 -1.038225  0.655748  ... -1.178141 -0.877361 -1.483769 -0.573682   \n",
       "2 -1.150495  1.423404  1.270098  ... -1.199511  0.539020 -1.590629 -0.573682   \n",
       "3  1.011112 -1.040340 -0.541991  ...  0.463763 -0.006583  1.089122 -0.573682   \n",
       "4 -0.595304 -1.238987  0.336844  ... -0.552837 -1.418494 -0.562982  1.743123   \n",
       "\n",
       "        f22       f23       f24       f25       f26    mass  \n",
       "0  3.406367  4.350537 -0.352571  1.130032  2.227706  1000.0  \n",
       "1 -1.693781 -0.545062 -0.299118 -0.662942 -0.193019   750.0  \n",
       "2 -0.543636 -0.937456 -0.300344 -0.523262 -1.506304   750.0  \n",
       "3 -0.276348 -0.409272 -0.349926 -0.307123  0.529698  1250.0  \n",
       "4  0.881802  0.002516  1.560950 -0.150760 -1.023889   750.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f0</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.016125</td>\n",
       "      <td>1.004417</td>\n",
       "      <td>-1.960549</td>\n",
       "      <td>-0.728821</td>\n",
       "      <td>-0.039303</td>\n",
       "      <td>0.690080</td>\n",
       "      <td>4.378282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.997486</td>\n",
       "      <td>-2.365355</td>\n",
       "      <td>-0.733255</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.734783</td>\n",
       "      <td>2.365287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.000080</td>\n",
       "      <td>-1.732165</td>\n",
       "      <td>-0.865670</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.865946</td>\n",
       "      <td>1.732370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f3</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.010561</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>-9.980274</td>\n",
       "      <td>-0.609229</td>\n",
       "      <td>0.019633</td>\n",
       "      <td>0.679882</td>\n",
       "      <td>4.148023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f4</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>-1.732137</td>\n",
       "      <td>-0.865802</td>\n",
       "      <td>-0.000507</td>\n",
       "      <td>0.865765</td>\n",
       "      <td>1.731978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f5</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>1.000957</td>\n",
       "      <td>-1.054221</td>\n",
       "      <td>-1.054221</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>0.850488</td>\n",
       "      <td>4.482618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f6</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.018160</td>\n",
       "      <td>0.986775</td>\n",
       "      <td>-3.034787</td>\n",
       "      <td>-0.756609</td>\n",
       "      <td>-0.149953</td>\n",
       "      <td>0.768669</td>\n",
       "      <td>3.720345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f7</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.996587</td>\n",
       "      <td>-2.757853</td>\n",
       "      <td>-0.701415</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>0.701319</td>\n",
       "      <td>2.758590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f8</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>1.000007</td>\n",
       "      <td>-1.732359</td>\n",
       "      <td>-0.865654</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.866598</td>\n",
       "      <td>1.731450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f9</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>-0.006870</td>\n",
       "      <td>1.001938</td>\n",
       "      <td>-1.325801</td>\n",
       "      <td>-1.325801</td>\n",
       "      <td>0.754261</td>\n",
       "      <td>0.754261</td>\n",
       "      <td>0.754261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f10</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.017543</td>\n",
       "      <td>0.994151</td>\n",
       "      <td>-2.835563</td>\n",
       "      <td>-0.723727</td>\n",
       "      <td>-0.128573</td>\n",
       "      <td>0.647864</td>\n",
       "      <td>4.639335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f11</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>0.998450</td>\n",
       "      <td>-2.602091</td>\n",
       "      <td>-0.703293</td>\n",
       "      <td>-0.000576</td>\n",
       "      <td>0.704100</td>\n",
       "      <td>2.602294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f12</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>-0.000329</td>\n",
       "      <td>1.000078</td>\n",
       "      <td>-1.732216</td>\n",
       "      <td>-0.866599</td>\n",
       "      <td>-0.001282</td>\n",
       "      <td>0.865832</td>\n",
       "      <td>1.732007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f13</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.999737</td>\n",
       "      <td>-1.161915</td>\n",
       "      <td>-1.161915</td>\n",
       "      <td>0.860649</td>\n",
       "      <td>0.860649</td>\n",
       "      <td>0.860649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f14</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.017246</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>-2.454879</td>\n",
       "      <td>-0.699618</td>\n",
       "      <td>-0.097493</td>\n",
       "      <td>0.634705</td>\n",
       "      <td>5.535799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f15</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.998429</td>\n",
       "      <td>-2.437812</td>\n",
       "      <td>-0.707026</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.708371</td>\n",
       "      <td>2.438369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f16</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>-0.000554</td>\n",
       "      <td>0.999861</td>\n",
       "      <td>-1.732145</td>\n",
       "      <td>-0.866247</td>\n",
       "      <td>-0.001377</td>\n",
       "      <td>0.864942</td>\n",
       "      <td>1.732738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f17</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>1.001006</td>\n",
       "      <td>-0.815440</td>\n",
       "      <td>-0.815440</td>\n",
       "      <td>-0.815440</td>\n",
       "      <td>1.226331</td>\n",
       "      <td>1.226331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f18</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.011648</td>\n",
       "      <td>1.002725</td>\n",
       "      <td>-1.728284</td>\n",
       "      <td>-0.742363</td>\n",
       "      <td>-0.089925</td>\n",
       "      <td>0.642319</td>\n",
       "      <td>5.866367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f19</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>1.000038</td>\n",
       "      <td>-2.281867</td>\n",
       "      <td>-0.720685</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>0.720492</td>\n",
       "      <td>2.282217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f20</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>1.000033</td>\n",
       "      <td>-1.731758</td>\n",
       "      <td>-0.865685</td>\n",
       "      <td>-0.000442</td>\n",
       "      <td>0.865957</td>\n",
       "      <td>1.732740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f21</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>1.000170</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>1.743123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f22</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.012288</td>\n",
       "      <td>1.010477</td>\n",
       "      <td>-3.631608</td>\n",
       "      <td>-0.541794</td>\n",
       "      <td>-0.160276</td>\n",
       "      <td>0.481219</td>\n",
       "      <td>7.293420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f23</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.009778</td>\n",
       "      <td>1.005418</td>\n",
       "      <td>-4.729473</td>\n",
       "      <td>-0.511552</td>\n",
       "      <td>-0.314403</td>\n",
       "      <td>0.163489</td>\n",
       "      <td>9.333287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f24</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.005270</td>\n",
       "      <td>1.009990</td>\n",
       "      <td>-20.622229</td>\n",
       "      <td>-0.354387</td>\n",
       "      <td>-0.326523</td>\n",
       "      <td>-0.233767</td>\n",
       "      <td>14.990636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f25</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>-0.001761</td>\n",
       "      <td>0.984451</td>\n",
       "      <td>-3.452634</td>\n",
       "      <td>-0.692510</td>\n",
       "      <td>-0.357030</td>\n",
       "      <td>0.475313</td>\n",
       "      <td>5.277313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f26</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>0.015331</td>\n",
       "      <td>0.982280</td>\n",
       "      <td>-2.632761</td>\n",
       "      <td>-0.794380</td>\n",
       "      <td>-0.088286</td>\n",
       "      <td>0.761085</td>\n",
       "      <td>4.444690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mass</th>\n",
       "      <td>7000000.0</td>\n",
       "      <td>1000.107387</td>\n",
       "      <td>353.425487</td>\n",
       "      <td>499.999969</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1250.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count         mean         std         min         25%          50%  \\\n",
       "f0    7000000.0     0.016125    1.004417   -1.960549   -0.728821    -0.039303   \n",
       "f1    7000000.0     0.000477    0.997486   -2.365355   -0.733255     0.000852   \n",
       "f2    7000000.0     0.000027    1.000080   -1.732165   -0.865670     0.000320   \n",
       "f3    7000000.0     0.010561    0.995600   -9.980274   -0.609229     0.019633   \n",
       "f4    7000000.0    -0.000105    0.999867   -1.732137   -0.865802    -0.000507   \n",
       "f5    7000000.0     0.002766    1.000957   -1.054221   -1.054221    -0.005984   \n",
       "f6    7000000.0     0.018160    0.986775   -3.034787   -0.756609    -0.149953   \n",
       "f7    7000000.0     0.000025    0.996587   -2.757853   -0.701415    -0.000107   \n",
       "f8    7000000.0     0.000435    1.000007   -1.732359   -0.865654     0.001385   \n",
       "f9    7000000.0    -0.006870    1.001938   -1.325801   -1.325801     0.754261   \n",
       "f10   7000000.0     0.017543    0.994151   -2.835563   -0.723727    -0.128573   \n",
       "f11   7000000.0    -0.000161    0.998450   -2.602091   -0.703293    -0.000576   \n",
       "f12   7000000.0    -0.000329    1.000078   -1.732216   -0.866599    -0.001282   \n",
       "f13   7000000.0     0.001739    0.999737   -1.161915   -1.161915     0.860649   \n",
       "f14   7000000.0     0.017246    0.999465   -2.454879   -0.699618    -0.097493   \n",
       "f15   7000000.0     0.000483    0.998429   -2.437812   -0.707026     0.000298   \n",
       "f16   7000000.0    -0.000554    0.999861   -1.732145   -0.866247    -0.001377   \n",
       "f17   7000000.0     0.004960    1.001006   -0.815440   -0.815440    -0.815440   \n",
       "f18   7000000.0     0.011648    1.002725   -1.728284   -0.742363    -0.089925   \n",
       "f19   7000000.0    -0.000113    1.000038   -2.281867   -0.720685    -0.000067   \n",
       "f20   7000000.0     0.000077    1.000033   -1.731758   -0.865685    -0.000442   \n",
       "f21   7000000.0     0.000291    1.000170   -0.573682   -0.573682    -0.573682   \n",
       "f22   7000000.0     0.012288    1.010477   -3.631608   -0.541794    -0.160276   \n",
       "f23   7000000.0     0.009778    1.005418   -4.729473   -0.511552    -0.314403   \n",
       "f24   7000000.0     0.005270    1.009990  -20.622229   -0.354387    -0.326523   \n",
       "f25   7000000.0    -0.001761    0.984451   -3.452634   -0.692510    -0.357030   \n",
       "f26   7000000.0     0.015331    0.982280   -2.632761   -0.794380    -0.088286   \n",
       "mass  7000000.0  1000.107387  353.425487  499.999969  750.000000  1000.000000   \n",
       "\n",
       "              75%          max  \n",
       "f0       0.690080     4.378282  \n",
       "f1       0.734783     2.365287  \n",
       "f2       0.865946     1.732370  \n",
       "f3       0.679882     4.148023  \n",
       "f4       0.865765     1.731978  \n",
       "f5       0.850488     4.482618  \n",
       "f6       0.768669     3.720345  \n",
       "f7       0.701319     2.758590  \n",
       "f8       0.866598     1.731450  \n",
       "f9       0.754261     0.754261  \n",
       "f10      0.647864     4.639335  \n",
       "f11      0.704100     2.602294  \n",
       "f12      0.865832     1.732007  \n",
       "f13      0.860649     0.860649  \n",
       "f14      0.634705     5.535799  \n",
       "f15      0.708371     2.438369  \n",
       "f16      0.864942     1.732738  \n",
       "f17      1.226331     1.226331  \n",
       "f18      0.642319     5.866367  \n",
       "f19      0.720492     2.282217  \n",
       "f20      0.865957     1.732740  \n",
       "f21     -0.573682     1.743123  \n",
       "f22      0.481219     7.293420  \n",
       "f23      0.163489     9.333287  \n",
       "f24     -0.233767    14.990636  \n",
       "f25      0.475313     5.277313  \n",
       "f26      0.761085     4.444690  \n",
       "mass  1250.000000  1500.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_summary = df.iloc[:, 1:29].describe().T\n",
    "feature_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highly Skewed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f3</th>\n",
       "      <th>f5</th>\n",
       "      <th>f10</th>\n",
       "      <th>f14</th>\n",
       "      <th>f18</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.000000e+06</td>\n",
       "      <td>7.000000e+06</td>\n",
       "      <td>7.000000e+06</td>\n",
       "      <td>7.000000e+06</td>\n",
       "      <td>7.000000e+06</td>\n",
       "      <td>7.000000e+06</td>\n",
       "      <td>7.000000e+06</td>\n",
       "      <td>7.000000e+06</td>\n",
       "      <td>7.000000e+06</td>\n",
       "      <td>7.000000e+06</td>\n",
       "      <td>7.000000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.612528e-02</td>\n",
       "      <td>1.056081e-02</td>\n",
       "      <td>2.765919e-03</td>\n",
       "      <td>1.754267e-02</td>\n",
       "      <td>1.724634e-02</td>\n",
       "      <td>1.164789e-02</td>\n",
       "      <td>1.228774e-02</td>\n",
       "      <td>9.778378e-03</td>\n",
       "      <td>5.269844e-03</td>\n",
       "      <td>-1.760961e-03</td>\n",
       "      <td>1.533136e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.004417e+00</td>\n",
       "      <td>9.956003e-01</td>\n",
       "      <td>1.000957e+00</td>\n",
       "      <td>9.941511e-01</td>\n",
       "      <td>9.994654e-01</td>\n",
       "      <td>1.002725e+00</td>\n",
       "      <td>1.010477e+00</td>\n",
       "      <td>1.005418e+00</td>\n",
       "      <td>1.009990e+00</td>\n",
       "      <td>9.844511e-01</td>\n",
       "      <td>9.822799e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.960549e+00</td>\n",
       "      <td>-9.980274e+00</td>\n",
       "      <td>-1.054221e+00</td>\n",
       "      <td>-2.835563e+00</td>\n",
       "      <td>-2.454879e+00</td>\n",
       "      <td>-1.728284e+00</td>\n",
       "      <td>-3.631608e+00</td>\n",
       "      <td>-4.729473e+00</td>\n",
       "      <td>-2.062223e+01</td>\n",
       "      <td>-3.452634e+00</td>\n",
       "      <td>-2.632761e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.288206e-01</td>\n",
       "      <td>-6.092291e-01</td>\n",
       "      <td>-1.054221e+00</td>\n",
       "      <td>-7.237266e-01</td>\n",
       "      <td>-6.996179e-01</td>\n",
       "      <td>-7.423630e-01</td>\n",
       "      <td>-5.417942e-01</td>\n",
       "      <td>-5.115522e-01</td>\n",
       "      <td>-3.543870e-01</td>\n",
       "      <td>-6.925097e-01</td>\n",
       "      <td>-7.943804e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-3.930319e-02</td>\n",
       "      <td>1.963316e-02</td>\n",
       "      <td>-5.983562e-03</td>\n",
       "      <td>-1.285732e-01</td>\n",
       "      <td>-9.749269e-02</td>\n",
       "      <td>-8.992496e-02</td>\n",
       "      <td>-1.602760e-01</td>\n",
       "      <td>-3.144032e-01</td>\n",
       "      <td>-3.265228e-01</td>\n",
       "      <td>-3.570301e-01</td>\n",
       "      <td>-8.828640e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.900799e-01</td>\n",
       "      <td>6.798818e-01</td>\n",
       "      <td>8.504885e-01</td>\n",
       "      <td>6.478635e-01</td>\n",
       "      <td>6.347052e-01</td>\n",
       "      <td>6.423185e-01</td>\n",
       "      <td>4.812194e-01</td>\n",
       "      <td>1.634892e-01</td>\n",
       "      <td>-2.337671e-01</td>\n",
       "      <td>4.753128e-01</td>\n",
       "      <td>7.610846e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.378282e+00</td>\n",
       "      <td>4.148023e+00</td>\n",
       "      <td>4.482618e+00</td>\n",
       "      <td>4.639335e+00</td>\n",
       "      <td>5.535799e+00</td>\n",
       "      <td>5.866367e+00</td>\n",
       "      <td>7.293420e+00</td>\n",
       "      <td>9.333287e+00</td>\n",
       "      <td>1.499064e+01</td>\n",
       "      <td>5.277313e+00</td>\n",
       "      <td>4.444690e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 f0            f3            f5           f10           f14  \\\n",
       "count  7.000000e+06  7.000000e+06  7.000000e+06  7.000000e+06  7.000000e+06   \n",
       "mean   1.612528e-02  1.056081e-02  2.765919e-03  1.754267e-02  1.724634e-02   \n",
       "std    1.004417e+00  9.956003e-01  1.000957e+00  9.941511e-01  9.994654e-01   \n",
       "min   -1.960549e+00 -9.980274e+00 -1.054221e+00 -2.835563e+00 -2.454879e+00   \n",
       "25%   -7.288206e-01 -6.092291e-01 -1.054221e+00 -7.237266e-01 -6.996179e-01   \n",
       "50%   -3.930319e-02  1.963316e-02 -5.983562e-03 -1.285732e-01 -9.749269e-02   \n",
       "75%    6.900799e-01  6.798818e-01  8.504885e-01  6.478635e-01  6.347052e-01   \n",
       "max    4.378282e+00  4.148023e+00  4.482618e+00  4.639335e+00  5.535799e+00   \n",
       "\n",
       "                f18           f22           f23           f24           f25  \\\n",
       "count  7.000000e+06  7.000000e+06  7.000000e+06  7.000000e+06  7.000000e+06   \n",
       "mean   1.164789e-02  1.228774e-02  9.778378e-03  5.269844e-03 -1.760961e-03   \n",
       "std    1.002725e+00  1.010477e+00  1.005418e+00  1.009990e+00  9.844511e-01   \n",
       "min   -1.728284e+00 -3.631608e+00 -4.729473e+00 -2.062223e+01 -3.452634e+00   \n",
       "25%   -7.423630e-01 -5.417942e-01 -5.115522e-01 -3.543870e-01 -6.925097e-01   \n",
       "50%   -8.992496e-02 -1.602760e-01 -3.144032e-01 -3.265228e-01 -3.570301e-01   \n",
       "75%    6.423185e-01  4.812194e-01  1.634892e-01 -2.337671e-01  4.753128e-01   \n",
       "max    5.866367e+00  7.293420e+00  9.333287e+00  1.499064e+01  5.277313e+00   \n",
       "\n",
       "                f26  \n",
       "count  7.000000e+06  \n",
       "mean   1.533136e-02  \n",
       "std    9.822799e-01  \n",
       "min   -2.632761e+00  \n",
       "25%   -7.943804e-01  \n",
       "50%   -8.828640e-02  \n",
       "75%    7.610846e-01  \n",
       "max    4.444690e+00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_skew = feature_summary.loc[feature_summary['max'] > feature_summary['mean'] + feature_summary['std']*4]\n",
    "left_skew = feature_summary.loc[feature_summary['min'] < feature_summary['mean'] - feature_summary['std']*4]\n",
    "skew = pd.concat([right_skew.T, left_skew.T], axis=0, join='outer')\n",
    "skew.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skewed, no major outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 6, figsize=(14, 5))\n",
    "fig.suptitle('Skewed Features - Boxplots')\n",
    "for i,j in zip(skew.columns, range(11)):\n",
    "    sns.boxplot(ax = axes[int(j/6), j%6], x = df[i])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "### Target Variable Class Distribution\n",
    "\n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/olmosjorge28/QTW-SPRING-2022/main/ds7333_case_study_6/Target_Variable_Class_Distribution.png\" width=400 height=400 />\n",
    "\n",
    "    \n",
    "Also, correlations between features f6, f10, f14, f18, and f26 were observed and also with the target variable. However, all the variables will be included in the model fitting exercise as there is no domain knowledge of the features to assess if some of them can be excluded from the analysis instead of the others.\n",
    "\n",
    "    \n",
    "        \n",
    "<img src=\"https://raw.githubusercontent.com/olmosjorge28/QTW-SPRING-2022/main/ds7333_case_study_6/Correlation_heat_map.png\" width=600 height=600 />\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmuI_mep8g_b"
   },
   "source": [
    "# Model Preparations <a id='model-preparations'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iDJqPa8fUNp"
   },
   "source": [
    "Which methods are you proposing to utilize to solve the problem?  Why is this method appropriate given the business objective? How will you determine if your approach is useful (or how will you differentiate which approach is more useful than another)?  More specifically, what evaluation metrics are most useful given that the problem is a classification one (ex., Accuracy, F1-score, Precision, Recall, AUC, etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnCsXV_c8g_V"
   },
   "source": [
    "## Sampling & Scaling Data <a id='sampling-scaling-data' />\n",
    "\n",
    "\n",
    "Training and test sets were created from the data using the stratified method to maintain the ratio of the binary outcome.  This was done in an abundance of caution, because the classes are almost perfectly balanced. 30% of the data was withheld for the test set, and the defining features were normalized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXqoLTm_8g_c"
   },
   "source": [
    "## Proposed Method <a id='proposed-metrics' />\n",
    "\n",
    "The stakeholders wanted our team to focus on creating a model that would predict the existence of a new particle with high accuracy above all, and the model interpretability was not a priority. With this mind the team decided on using an Artificial Neural Network to achieve a high accuracy model. The model had an input layer of 28 neurons (one for each of the features), and it had 2 hidden layers with 200 neurons and 50 neurons, respectively.  The hidden layers used a ReLu activation functoin, which was chosen for its characteristics that helps estimating non-linear functions. The model had a single neuron output layer with a sigmoid activation function and a BinaryCrossentropy loss function since our target variable is binary. \n",
    "\n",
    "In experimentations our best results were achieved with a batch size of 1000. This gave a large enough sample size to limit unnecessary fluctations, and this gave the model the right balance between variance and bias. Additionally, the batch sizes were small enough to compute in memory, but not so small that it would increase processing time dramatically.  Batch sizes of 10,000 and 100,000 ran faster but resulted in slower learning with lower accuracy.  The team ran 40 epochs with a batch size of 1000 since no further improvement was observed beyond that without risking overfitting.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseImputer:\n",
    "    #@abstractmethod\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    \n",
    "    #@abstractmethod\n",
    "    def transform(self, X):\n",
    "        pass\n",
    "\n",
    "class BaseModel:\n",
    "    #@abstractmethod\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        pass\n",
    "    \n",
    "    #@abstractmethod\n",
    "    def predict(self, X):\n",
    "        passb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modeling:\n",
    "    _X_train_fitted = None\n",
    "    _X_test_fitted = None\n",
    "    _y_train = None\n",
    "    _y_test = None\n",
    "    _y_preds = None\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, \n",
    "                 target_name: str, \n",
    "                 shuffle_splitter: BaseShuffleSplit, \n",
    "                 imputer: BaseImputer, \n",
    "                 model: BaseModel, \n",
    "                 scaler = None):\n",
    "        self._data = data\n",
    "        self._target_name = target_name\n",
    "        self._shuffle_splitter = shuffle_splitter\n",
    "        self._imputer = imputer\n",
    "        self._model = model\n",
    "        self._X, self._y = self._split_data()\n",
    "        self._scaler = scaler\n",
    "        \n",
    "    @property\n",
    "    def X(self):\n",
    "        return self._X\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._y\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "    \n",
    "    @model.setter\n",
    "    def model(self, model):\n",
    "        self._model = model\n",
    "     \n",
    "    @property\n",
    "    def X_train(self):\n",
    "        return self._X_train_fitted\n",
    "    \n",
    "    @property\n",
    "    def X_test(self):\n",
    "        return self._X_test_fitted\n",
    "    \n",
    "    @property\n",
    "    def y_train(self):\n",
    "        return self._y_train\n",
    "    \n",
    "    @property\n",
    "    def y_test(self):\n",
    "        return self._y_test\n",
    "    \n",
    "    @property\n",
    "    def y_preds(self):\n",
    "        return self._y_preds\n",
    "    \n",
    "    def _split_data(self):\n",
    "        X = self._data.copy()\n",
    "        return X.drop([self._target_name], axis=1) , X[self._target_name]\n",
    "    \n",
    "    def _shuffle_split(self):\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        for train_index, test_index in self._shuffle_splitter.split(X,y):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def _fit_imputer(self, train):\n",
    "        if self._imputer is not None:\n",
    "            self._imputer.fit(train)\n",
    "    \n",
    "    def _fit_scaler(self, train):\n",
    "        if self._scaler is not None:\n",
    "            self._scaler.fit(train)\n",
    "    \n",
    "    def _impute_data(self, X: pd.DataFrame):\n",
    "        if self._imputer is not None:\n",
    "            return pd.DataFrame(self._imputer.transform(X), columns = self.X.columns, index = X.index)\n",
    "        return X\n",
    "    \n",
    "    def _scale_data(self, X: pd.DataFrame):\n",
    "        if self._scaler is not None:\n",
    "            X = pd.DataFrame(self._scaler.transform(X), columns = self._X.columns)\n",
    "        return X\n",
    "    \n",
    "    def prepare(self):\n",
    "        X_train, X_test, y_train, y_test = self._shuffle_split()   \n",
    "        self._fit_imputer(X_train)\n",
    "        X_train = self._impute_data(X_train)\n",
    "        X_test = self._impute_data(X_test)\n",
    "        self._fit_scaler(X_train)\n",
    "        self._X_train_fitted = self._scale_data(X_train)\n",
    "        self._X_test_fitted = self._scale_data(X_test)\n",
    "        self._y_train = y_train\n",
    "        self._y_test = y_test\n",
    "        \n",
    "    def prepare_and_train(self):\n",
    "        self.prepare()\n",
    "        return self.train()\n",
    "        \n",
    "    def train(self): \n",
    "        self._model.fit(self.X_train, self.y_train) \n",
    "        self._y_preds = self._model.predict(self.X_train)\n",
    "        \n",
    "        return self.metrics(self.y_train, self.y_preds)\n",
    "        \n",
    "    def test(self):\n",
    "        return self.metrics(self.y_test, self._model.predict(self.X_test))\n",
    "       \n",
    "        \n",
    "    def metrics(self, y_true = None, y_pred = None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModeling(Modeling):\n",
    "    def __init__(self, \n",
    "                 data: pd.DataFrame, \n",
    "                 target_name: str, \n",
    "                 shuffle_splitter: BaseShuffleSplit, \n",
    "                 imputer: BaseImputer, \n",
    "                 model: BaseModel, \n",
    "                 scaler = None,\n",
    "                 beta: int = 1,\n",
    "                 classification: str = 'binary'):\n",
    "        super().__init__(data, target_name, shuffle_splitter, imputer, model, scaler)\n",
    "        self.beta = beta\n",
    "        self.classification = classification\n",
    "    \n",
    "    def metrics(self, y_true = None, y_pred = None):\n",
    "        if y_true is None and y_pred is None:\n",
    "            y_true = self.y_train\n",
    "            y_pred = self.y_preds\n",
    "        return ({'matrix': confusion_matrix(y_true, y_pred), \n",
    "            'accuracy': accuracy_score(y_true, y_pred), \n",
    "            'precision': precision_score(y_true, y_pred, average=self.classification), \n",
    "            'recall': recall_score(y_true, y_pred, average=self.classification),\n",
    "             'f1': f1_score(y_true, y_pred),\n",
    "            'f{}'.format(self.beta) : fbeta_score(y_true, y_pred, average=self.classification, beta=self.beta) } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClassificationModeling(ClassificationModeling):\n",
    "    def __init__(self, \n",
    "             data: pd.DataFrame, \n",
    "             target_name: str, \n",
    "             shuffle_splitter: BaseShuffleSplit, \n",
    "             imputer: BaseImputer, \n",
    "             model: BaseModel, \n",
    "             scaler = None,\n",
    "             beta: int = 1,\n",
    "             classification: str = 'binary', tb_callback = TensorBoard(log_dir=\"logs/\", histogram_freq=1)):\n",
    "        super().__init__(data, target_name, shuffle_splitter, imputer, model, scaler, beta, classification)\n",
    "        self.tb_callback=tb_callback\n",
    "        \n",
    "        \n",
    "    def train(self, epoch, batch):\n",
    "        logDir = \"logs/{epoch}-{batchsize}-{time}\".format(epoch=epoch, batchsize=batch, time=time.time())\n",
    "        self.tb_callback.log_dir = logDir\n",
    "        self._model.fit(self.X_train, self.y_train, batch_size=batch, epochs=epoch, validation_data=(self.X_test, self.y_test), callbacks=[self.tb_callback])\n",
    "        self._y_preds = self._model.predict(self.X_train)\n",
    "        return self.metrics(self.y_train, self.y_preds)\n",
    "    \n",
    "    def metrics(self, y_true = None, y_pred = None):\n",
    "        if y_true is None and y_pred is None:\n",
    "            y_true = self.y_train\n",
    "            y_pred = self.y_preds\n",
    "            \n",
    "        y_pred = pd.Series(y_pred.reshape((y_pred.shape[1], y_pred.shape[0]))[0], index=y_true.index)\n",
    "        y_pred = pd.Series( (y_pred>0.5).astype(int), index=y_true.index)\n",
    "        return super().metrics(y_true,y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HT4eeZsX8g_c"
   },
   "source": [
    "## Evaluation Metrics <a id='evaluation-metrics' />\n",
    "The key metric used to evaluate the models was accuracy.  Accuracy was appropriate because the target variable was balanced with a 50/50 split between Detection and Non-Detection classes.  It is a straightforward metric that is intuitive and easy to explain, and the customer also requested that the team focus on accuracy.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeWgSmQW8g_Z"
   },
   "source": [
    "### Baseline Model\n",
    "\n",
    "For our baseline model the team decided to run a logistic regression model. The model used a 70/30 stratified split, with L1 penalty and saga solver. The logistic model was chosen as this a simple, quick, and interprateable model. This gave the team a benchmark for accuracy to compare the proposed artificial neural network accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = ClassificationModeling(df,'# label',\n",
    "                           StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=12343),\n",
    "                           None,\n",
    "                           LogisticRegression(penalty='l1', solver='saga', random_state=12343),\n",
    "                           StandardScaler(), beta=2)\n",
    "\n",
    "baseline.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = pd.DataFrame()\n",
    "\n",
    "for i in [0.0001, 0.0005, 0.001, .005, 1, 100]:\n",
    "    baseline.model.C = i\n",
    "    baseline_results = baseline_results.append({\"C\": i,\n",
    "                                               \"Train Accuracy\": round( baseline.train()['accuracy'], 4),\n",
    "                                               \"Test Accuracy\": round( baseline.test()['accuracy'], 4)},\n",
    "                                              ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(data=baseline_results, x='C', y='Train Accuracy', color='blue')\n",
    "sns.lineplot(data=baseline_results, x='C', y='Test Accuracy', color='red')\n",
    "plt.title('Tuning C for Logistic Regression with L1')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.xscale('log')\n",
    "plt.axvline(0.001, color='black', ls='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Baseline Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matrix': array([[2052192,  397193],\n",
       "        [ 402982, 2047633]]),\n",
       " 'accuracy': 0.8366989795918367,\n",
       " 'precision': 0.8375373134938846,\n",
       " 'recall': 0.8355588291102437,\n",
       " 'f1': 0.8365469014946764,\n",
       " 'f2': 0.8359537778410662}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.model.C = 0.001\n",
    "baseline.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matrix': array([[879402, 170334],\n",
       "        [172538, 877726]]),\n",
       " 'accuracy': 0.836727619047619,\n",
       " 'precision': 0.8374768620117169,\n",
       " 'recall': 0.8357194000746478,\n",
       " 'f1': 0.8365972080574782,\n",
       " 'f2': 0.8360703021232528}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rADZ1qTz8g_c"
   },
   "source": [
    "## Feature Selection <a id='feature-selection' />\n",
    "\n",
    "All the features were used in the proposed neural network model. The team chose not use regularization since the training and test set evalution metric results aligned, which indicates that the neural network model was not overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuRjMsjg8g_d"
   },
   "source": [
    "# Model Building & Evaluations <a id='model-building'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhUWTUQleFn-"
   },
   "source": [
    "### Final Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrxXXO1jeFn6"
   },
   "source": [
    "The team initially fit a Logistic Regression model to the data set to get a baseline accuracy rate for the prediction. Then, a neural network model was fit to assess the improvement in the accuracy rate.\n",
    "\n",
    "The final model had an input layer of 28 neurons (one for each of the features), and it had 2 hidden layers with 200 neurons and 50 neurons, respectively.  The hidden layers used a ReLu activation functoin, which was chosen for its characteristics that helps estimating non-linear functions. The model had a single neuron output layer with a sigmoid activation function and a BinaryCrossentropy loss function since our target variable is binary. \n",
    "\n",
    "In experimentations our best results were achieved with a batch size of 1000. This gave a large enough sample size to limit unnecessary fluctations, and this gave the model the right balance between variance and bias. Additionally, the batch sizes were small enough to compute in memory, but not so small that it would increase processing time dramatically.  Batch sizes of 10,000 and 100,000 ran faster but resulted in slower learning with lower accuracy.  The team ran 40 epochs with a batch size of 1000 since no further improvement was observed beyond that without risking overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = NNClassificationModeling(df,'# label',\n",
    "                           StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=12343),\n",
    "                           None,\n",
    "                           None,\n",
    "                           StandardScaler(), beta=2)\n",
    "\n",
    "NN.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "49/49 [==============================] - 14s 244ms/step - loss: 0.4290 - accuracy: 0.7961 - val_loss: 0.3508 - val_accuracy: 0.8403\n",
      "Epoch 2/40\n",
      "49/49 [==============================] - 10s 205ms/step - loss: 0.3368 - accuracy: 0.8452 - val_loss: 0.3260 - val_accuracy: 0.8497\n",
      "Epoch 3/40\n",
      "49/49 [==============================] - 9s 179ms/step - loss: 0.3196 - accuracy: 0.8527 - val_loss: 0.3126 - val_accuracy: 0.8557\n",
      "Epoch 4/40\n",
      "49/49 [==============================] - 10s 197ms/step - loss: 0.3072 - accuracy: 0.8577 - val_loss: 0.3013 - val_accuracy: 0.8602\n",
      "Epoch 5/40\n",
      "49/49 [==============================] - 9s 183ms/step - loss: 0.2983 - accuracy: 0.8612 - val_loss: 0.2947 - val_accuracy: 0.8628\n",
      "Epoch 6/40\n",
      "49/49 [==============================] - 10s 195ms/step - loss: 0.2932 - accuracy: 0.8634 - val_loss: 0.2907 - val_accuracy: 0.8647\n",
      "Epoch 7/40\n",
      "49/49 [==============================] - 9s 182ms/step - loss: 0.2896 - accuracy: 0.8652 - val_loss: 0.2875 - val_accuracy: 0.8667\n",
      "Epoch 8/40\n",
      "49/49 [==============================] - 9s 180ms/step - loss: 0.2867 - accuracy: 0.8668 - val_loss: 0.2850 - val_accuracy: 0.8683\n",
      "Epoch 9/40\n",
      "49/49 [==============================] - 9s 181ms/step - loss: 0.2844 - accuracy: 0.8682 - val_loss: 0.2830 - val_accuracy: 0.8694\n",
      "Epoch 10/40\n",
      "49/49 [==============================] - 9s 184ms/step - loss: 0.2826 - accuracy: 0.8695 - val_loss: 0.2814 - val_accuracy: 0.8706\n",
      "Epoch 11/40\n",
      "49/49 [==============================] - 9s 185ms/step - loss: 0.2811 - accuracy: 0.8705 - val_loss: 0.2800 - val_accuracy: 0.8715\n",
      "Epoch 12/40\n",
      "49/49 [==============================] - 9s 180ms/step - loss: 0.2797 - accuracy: 0.8714 - val_loss: 0.2787 - val_accuracy: 0.8723\n",
      "Epoch 13/40\n",
      "49/49 [==============================] - 9s 180ms/step - loss: 0.2785 - accuracy: 0.8723 - val_loss: 0.2776 - val_accuracy: 0.8729\n",
      "Epoch 14/40\n",
      "49/49 [==============================] - 9s 177ms/step - loss: 0.2774 - accuracy: 0.8729 - val_loss: 0.2766 - val_accuracy: 0.8737\n",
      "Epoch 15/40\n",
      "49/49 [==============================] - 9s 179ms/step - loss: 0.2764 - accuracy: 0.8737 - val_loss: 0.2756 - val_accuracy: 0.8745\n",
      "Epoch 16/40\n",
      "49/49 [==============================] - 9s 180ms/step - loss: 0.2754 - accuracy: 0.8743 - val_loss: 0.2748 - val_accuracy: 0.8750\n",
      "Epoch 17/40\n",
      "49/49 [==============================] - 9s 193ms/step - loss: 0.2745 - accuracy: 0.8749 - val_loss: 0.2740 - val_accuracy: 0.8754\n",
      "Epoch 18/40\n",
      "49/49 [==============================] - 9s 181ms/step - loss: 0.2737 - accuracy: 0.8754 - val_loss: 0.2732 - val_accuracy: 0.8760\n",
      "Epoch 19/40\n",
      "49/49 [==============================] - 9s 187ms/step - loss: 0.2730 - accuracy: 0.8758 - val_loss: 0.2725 - val_accuracy: 0.8763\n",
      "Epoch 20/40\n",
      "49/49 [==============================] - 9s 180ms/step - loss: 0.2723 - accuracy: 0.8763 - val_loss: 0.2720 - val_accuracy: 0.8766\n",
      "Epoch 21/40\n",
      "49/49 [==============================] - 9s 184ms/step - loss: 0.2717 - accuracy: 0.8767 - val_loss: 0.2714 - val_accuracy: 0.8770\n",
      "Epoch 22/40\n",
      "49/49 [==============================] - 9s 186ms/step - loss: 0.2711 - accuracy: 0.8770 - val_loss: 0.2711 - val_accuracy: 0.8774\n",
      "Epoch 23/40\n",
      "49/49 [==============================] - 10s 201ms/step - loss: 0.2706 - accuracy: 0.8773 - val_loss: 0.2704 - val_accuracy: 0.8777\n",
      "Epoch 24/40\n",
      "49/49 [==============================] - 10s 202ms/step - loss: 0.2700 - accuracy: 0.8776 - val_loss: 0.2700 - val_accuracy: 0.8780\n",
      "Epoch 25/40\n",
      "49/49 [==============================] - 10s 204ms/step - loss: 0.2696 - accuracy: 0.8779 - val_loss: 0.2696 - val_accuracy: 0.8782\n",
      "Epoch 26/40\n",
      "49/49 [==============================] - 10s 199ms/step - loss: 0.2691 - accuracy: 0.8782 - val_loss: 0.2692 - val_accuracy: 0.8785\n",
      "Epoch 27/40\n",
      "49/49 [==============================] - 9s 188ms/step - loss: 0.2688 - accuracy: 0.8784 - val_loss: 0.2689 - val_accuracy: 0.8785\n",
      "Epoch 28/40\n",
      "49/49 [==============================] - 9s 186ms/step - loss: 0.2684 - accuracy: 0.8785 - val_loss: 0.2685 - val_accuracy: 0.8787\n",
      "Epoch 29/40\n",
      "49/49 [==============================] - 9s 187ms/step - loss: 0.2680 - accuracy: 0.8788 - val_loss: 0.2682 - val_accuracy: 0.8789\n",
      "Epoch 30/40\n",
      "49/49 [==============================] - 9s 189ms/step - loss: 0.2677 - accuracy: 0.8790 - val_loss: 0.2678 - val_accuracy: 0.8792\n",
      "Epoch 31/40\n",
      "49/49 [==============================] - 9s 186ms/step - loss: 0.2674 - accuracy: 0.8792 - val_loss: 0.2677 - val_accuracy: 0.8792\n",
      "Epoch 32/40\n",
      "49/49 [==============================] - 9s 184ms/step - loss: 0.2671 - accuracy: 0.8794 - val_loss: 0.2673 - val_accuracy: 0.8795\n",
      "Epoch 33/40\n",
      "49/49 [==============================] - 9s 182ms/step - loss: 0.2668 - accuracy: 0.8796 - val_loss: 0.2671 - val_accuracy: 0.8796\n",
      "Epoch 34/40\n",
      "49/49 [==============================] - 9s 183ms/step - loss: 0.2666 - accuracy: 0.8797 - val_loss: 0.2669 - val_accuracy: 0.8797\n",
      "Epoch 35/40\n",
      "49/49 [==============================] - 9s 186ms/step - loss: 0.2663 - accuracy: 0.8799 - val_loss: 0.2668 - val_accuracy: 0.8797\n",
      "Epoch 36/40\n",
      "49/49 [==============================] - 9s 181ms/step - loss: 0.2661 - accuracy: 0.8799 - val_loss: 0.2664 - val_accuracy: 0.8800\n",
      "Epoch 37/40\n",
      "49/49 [==============================] - 9s 188ms/step - loss: 0.2658 - accuracy: 0.8801 - val_loss: 0.2664 - val_accuracy: 0.8799\n",
      "Epoch 38/40\n",
      "49/49 [==============================] - 9s 191ms/step - loss: 0.2657 - accuracy: 0.8801 - val_loss: 0.2661 - val_accuracy: 0.8801\n",
      "Epoch 39/40\n",
      "49/49 [==============================] - 9s 180ms/step - loss: 0.2654 - accuracy: 0.8803 - val_loss: 0.2659 - val_accuracy: 0.8802\n",
      "Epoch 40/40\n",
      "49/49 [==============================] - 9s 180ms/step - loss: 0.2653 - accuracy: 0.8804 - val_loss: 0.2657 - val_accuracy: 0.8802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'matrix': array([[2109145,  340240],\n",
       "        [ 244760, 2205855]]),\n",
       " 'accuracy': 0.8806122448979592,\n",
       " 'precision': 0.8663679085030213,\n",
       " 'recall': 0.9001230303413633,\n",
       " 'f1': 0.8829229633098579,\n",
       " 'f2': 0.8931632081648418}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.model = tf.keras.Sequential() # model object\n",
    "\n",
    "NN.model.add( tf.keras.layers.Input( shape=(NN.X_train.shape[1],) ) )\n",
    "# specify data shape for first input layer\n",
    "# columns (features) only, # rows specified by batch size later in fit() \n",
    "\n",
    "NN.model.add( tf.keras.layers.Dense(200, activation = 'relu') )\n",
    "# add these layers sequentially with decreasing # neurons\n",
    "NN.model.add( tf.keras.layers.Dense(50, activation = 'relu') )\n",
    "\n",
    "NN.model.add( tf.keras.layers.Dense(1, activation = 'sigmoid') )\n",
    "# Final layer, Regression Output\n",
    "# For Classification, use activation = 'sigmoid' or 'softmax' for Final layer\n",
    "\n",
    "NN.model.compile(optimizer='adam', loss='BinaryCrossentropy', metrics=['accuracy'])\n",
    "# Have to compile model after specifying layers\n",
    "\n",
    "NN.train(batch = 100000, epoch=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matrix': array([[903103, 146633],\n",
       "        [104938, 945326]]),\n",
       " 'accuracy': 0.8802042857142857,\n",
       " 'precision': 0.8657156541591763,\n",
       " 'recall': 0.9000841693136202,\n",
       " 'f1': 0.8825654472013418,\n",
       " 'f2': 0.8929938796697157}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-C7Tc5W8g_e"
   },
   "source": [
    "## Model's Performance Analysis <a id='performance-analysis'/>\n",
    "\n",
    "A baseline model was built using logistic regression as a simpler and more interpretable option.  This model had an __83.6%__ accuracy between the training and test sets.  Because of the agreement in accuracy between training and test sets, the confusion matrix below is only for the test set (training set confusion matrices are in the appendix for reference). \n",
    "\n",
    "Logistic Test Set - Confusion Matrix:\n",
    "\n",
    "|                   | Predicted Not Detected | Predicted Detected |\n",
    "|-------------------|---------------|----------------|\n",
    "| Actual Not Detected  | 879,402         | 170,334              |\n",
    "| Actual Detected | 172,538           | 877,726         |\n",
    "\n",
    "\n",
    "A neural network (NN) model was built to try to beat this simpler baseline model.  The final neural network model had an accuracy of __88.0%__ between the training and test sets.  This is a significant improvement, especially given how expensive it is to run experiments on a large particle accelerator.  The model took approximately __6 minutes__ to train which was longer than the simpler logistic regression model, as expected.  This additional training was not excessive and significantly improved the accuracy of the predictions.           \n",
    "\n",
    "__TODO:__ Final NN Test Set - Confusion Matrix:\n",
    "\n",
    "|                   | Predicted Not Detected | Predicted Detected |\n",
    "|-------------------|---------------|----------------|\n",
    "| Actual Not Detected  | 903,103         | 146,633              |\n",
    "| Actual Detected | 104,938           | 945,326        |\n",
    "\n",
    "Comparing this confusion matrix to the one above for the baseline model, predictions were improved for both the Detected and Not Detected classes with the Neural Network model.  The __4.4% improvement in accuracy__ is equivalent to __91,301 more correct predictions__.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HISQa9FO8g_e"
   },
   "source": [
    "## Model Interpretability & Explainability <a id='model-explanation'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSBc4ETe8g_e"
   },
   "source": [
    "### Examining Feature Importance <a id='examining-feature-importance'/>\n",
    "\n",
    "\n",
    "#### Feature Importance for Baseline Model\n",
    "\n",
    "The logistic regression baseline model had several highly important features including 'mass' and 'f6' per the plot below.  Domain experts are needed to intrepret these results further.    \n",
    "\n",
    "The neural network model is less interpretable and does not have global feature importance to directly compare to the logistic regression model.  There are opportunities to explore local feature importance which is discussed in the Future Considerations section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat_coef = []\n",
    "feat = zip(baseline.X_train.columns, baseline.model.coef_[0])\n",
    "[feat_coef.append([i,j]) for i,j in feat]\n",
    "feat_coef = pd.DataFrame(feat_coef, columns = ['feature','coef'])\n",
    "top_feat_baseline = feat_coef.loc[abs(feat_coef['coef'])>0].sort_values(by='coef')\n",
    "\n",
    "feat_plot = sns.barplot(data=top_feat_baseline, x='feature', y='coef', palette = \"ch:s=.25,rot=-.25\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('LR Feature Importance with L1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbAhMB1x8g_e"
   },
   "source": [
    "# Conclusion <a id='conclusion'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnsadV7M8g_e"
   },
   "source": [
    "### Final Model Proposal <a id='final-model-proposal'/>\n",
    "The final neural network model should be used to make predictions on whether a new particle is detected or not based on the input data.  There was a high level of agreement between the training and test sets, and care was taken to prevent overfitting and create a generalizable model, so results should be reliable if similar experiments are conducted in the future.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oX8fXYczN5D-"
   },
   "source": [
    "### Future Considerations, Model Enhancements and Alternative Modeling Approaches <a id='model-enhancements'/>\n",
    "\n",
    "The team recommends that Shapley values be explored for local feature importance for the neural network model.  In addition, input from domain experts may help with feature engineering, model building, and interpretation.  Additional computation power or parallelization may also be useful to speed up model training.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0yVc5DKeFn_"
   },
   "source": [
    "## References\n",
    "https://www.tibco.com/reference-center/what-is-a-neural-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "Logistic Train - Confusion Matrix:\n",
    "\n",
    "|                   | Predicted Not Detected | Predicted Detected |\n",
    "|-------------------|---------------|----------------|\n",
    "| Actual Not Detected  | 2,052,192       | 397,193             |\n",
    "| Actual Detected | 402,982            | 2,047,633        |\n",
    "\n",
    "\n",
    "Final NN Train - Confusion Matrix:\n",
    "\n",
    "|                   | Predicted Not Detected | Predicted Detected |\n",
    "|-------------------|---------------|----------------|\n",
    "| Actual Not Detected  | 2,109,145       | 340,240             |\n",
    "| Actual Detected | 244,760            | 2,205,855       |\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Case_Study_6_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
